{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"A Collaborative Deep Learning Framework for Conservation"},{"location":"#welcome-to-pytorch-wildlife","title":"\ud83d\udc4b Welcome to Pytorch-Wildlife","text":"<p>PyTorch-Wildlife is an AI platform designed for the AI for Conservation community to create, modify, and share powerful AI conservation models. It allows users to directly load a variety of models including MegaDetector, DeepFaune, and HerdNet from our ever expanding model zoo for both animal detection and classification. In the future, we will also include models that can be used for applications, including underwater images and bioacoustics. We want to provide a unified and straightforward experience for both practicioners and developers in the AI for conservation field. Your engagement with our work is greatly appreciated, and we eagerly await any feedback you may have.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>\ud83d\udc47 Here is a brief example on how to perform detection and classification on a single image using <code>PyTorch-wildlife</code> <pre><code>import numpy as np\nfrom PytorchWildlife.models import detection as pw_detection\nfrom PytorchWildlife.models import classification as pw_classification\n\nimg = np.random.randn(3, 1280, 1280)\n\n# Detection\ndetection_model = pw_detection.MegaDetectorV6() # Model weights are automatically downloaded.\ndetection_result = detection_model.single_image_detection(img)\n\n#Classification\nclassification_model = pw_classification.AI4GAmazonRainforest() # Model weights are automatically downloaded.\nclassification_results = classification_model.single_image_classification(img)\n</code></pre></p>"},{"location":"#install-pytorch-wildlife","title":"\u2699\ufe0f Install Pytorch-Wildlife","text":"<p><pre><code>pip install PytorchWildlife\n</code></pre> Please refer to our installation guide for more installation information.</p>"},{"location":"#examples","title":"\ud83d\uddbc\ufe0f Examples","text":""},{"location":"#image-detection-using-megadetector","title":"Image detection using <code>MegaDetector</code>","text":"<p> Credits to Universidad de los Andes, Colombia.</p>"},{"location":"#image-classification-with-megadetector-and-ai4gamazonrainforest","title":"Image classification with <code>MegaDetector</code> and <code>AI4GAmazonRainforest</code>","text":"<p> Credits to Universidad de los Andes, Colombia.</p>"},{"location":"#opossum-id-with-megadetector-and-ai4gopossum","title":"Opossum ID with <code>MegaDetector</code> and <code>AI4GOpossum</code>","text":"<p> Credits to the Agency for Regulation and Control of Biosecurity and Quarantine for Gal\u00e1pagos (ABG), Ecuador.</p>"},{"location":"build_mkdocs/","title":"Building the MkDocs Site","text":"<p>To build the MkDocs site locally, follow these steps:</p> <ol> <li> <p>Install MkDocs and Dependencies:    Ensure you have Python installed. Then, install MkDocs and the required dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Build the Site:    Run the following command to build the site:    <pre><code>mkdocs build\n</code></pre>    This will generate the static site in the <code>site/</code> directory.</p> </li> <li> <p>Serve the Site Locally:    To preview the site locally, use:    <pre><code>mkdocs serve\n</code></pre>    This will start a local development server, and you can view the site at <code>http://127.0.0.1:8000/</code>.</p> </li> <li> <p>Custom Documentation Directory:    The site's page files are located in the <code>mkdocs/</code> directory. This is specified in the <code>mkdocs.yml</code> file under the <code>docs_dir</code> key:    <pre><code>docs_dir: mkdocs\n</code></pre>    This is so it doesn't clash with Pytorch Wildlife's preexisting <code>docs/</code> directory (so no breaking changes!)</p> </li> </ol> <p>Ensure that any changes to the documentation are made in the <code>mkdocs/</code> directory.</p> <ol> <li>Exclude the <code>site/</code> Directory:    The <code>site/</code> directory is automatically generated and should not be included in version control. It is already added to <code>.gitignore</code>.</li> </ol>"},{"location":"cite/","title":"Cite us!","text":"<p>We have recently published a summary paper on Pytorch-Wildlife. The paper has been accepted as an oral presentation at the CV4Animals workshop at this CVPR 2024. Please feel free to cite us!</p> <pre><code>@misc{hernandez2024pytorchwildlife,\n      title={Pytorch-Wildlife: A Collaborative Deep Learning Framework for Conservation}, \n      author={Andres Hernandez and Zhongqi Miao and Luisa Vargas and Sara Beery and Rahul Dodhia and Juan Lavista},\n      year={2024},\n      eprint={2405.12930},\n      archivePrefix={arXiv},\n}\n</code></pre> <p>Also, don't forget to cite our original paper for MegaDetector: </p> <pre><code>@misc{beery2019efficient,\n      title={Efficient Pipeline for Camera Trap Image Review},\n      author={Sara Beery and Dan Morris and Siyu Yang},\n      year={2019}\n      eprint={1907.06772},\n      archivePrefix={arXiv},\n}\n</code></pre>"},{"location":"collaborators/","title":"Existing Collaborators","text":"<p># \ud83d\udc65 Existing Collaborators</p> <p>The extensive collaborative efforts of Megadetector have genuinely inspired us, and we deeply value its significant contributions to the community. As we continue to advance with Pytorch-Wildlife, our commitment to delivering technical support to our existing partners on MegaDetector remains the same.</p> <p>Here we list a few of the organizations that have used MegaDetector. We're only listing organizations who have given us permission to refer to them here or have posted publicly about their use of MegaDetector.</p> \ud83d\udc49 Full list of organizations <ul> <li>(Newly Added) TerrO\u00efko (OCAPI platform)</li> <li>Arizona Department of Environmental Quality</li> <li>Blackbird Environmental</li> <li>Camelot</li> <li>Canadian Parks and Wilderness Society (CPAWS) Northern Alberta Chapter</li> <li>Conservation X Labs</li> <li>Czech University of Life Sciences Prague</li> <li>EcoLogic Consultants Ltd.</li> <li>Estaci\u00f3n Biol\u00f3gica de Do\u00f1ana</li> <li>Idaho Department of Fish and Game</li> <li>Island Conservation</li> <li>Myall Lakes Dingo Project</li> <li>Point No Point Treaty Council</li> <li>Ramat Hanadiv Nature Park</li> <li>SPEA (Portuguese Society for the Study of Birds)</li> <li>Synthetaic</li> <li>Taronga Conservation Society</li> <li>The Nature Conservancy in Wyoming</li> <li>TrapTagger</li> <li>Upper Yellowstone Watershed Group</li> <li>Applied Conservation Macro Ecology Lab, University of Victoria</li> <li>Banff National Park Resource Conservation, Parks Canada</li> <li>Blumstein Lab, UCLA</li> <li>Borderlands Research Institute, Sul Ross State University</li> <li>Capitol Reef National Park / Utah Valley University</li> <li>Center for Biodiversity and Conservation, American Museum of Natural History</li> <li>Centre for Ecosystem Science, UNSW Sydney</li> <li>Cross-Cultural Ecology Lab, Macquarie University</li> <li>DC Cat Count, led by the Humane Rescue Alliance</li> <li>Department of Fish and Wildlife Sciences, University of Idaho</li> <li>Department of Wildlife Ecology and Conservation, University of Florida</li> <li>Ecology and Conservation of Amazonian Vertebrates Research Group, Federal University of Amap\u00e1</li> <li>Gola Forest Programme, Royal Society for the Protection of Birds (RSPB)</li> <li>Graeme Shannon's Research Group, Bangor University</li> <li>Hamaarag, The Steinhardt Museum of Natural History, Tel Aviv University</li> <li>Institut des Science de la For\u00eat Temp\u00e9r\u00e9e (ISFORT), Universit\u00e9 du Qu\u00e9bec en Outaouais</li> <li>Lab of Dr. Bilal Habib, the Wildlife Institute of India</li> <li>Mammal Spatial Ecology and Conservation Lab, Washington State University</li> <li>McLoughlin Lab in Population Ecology, University of Saskatchewan</li> <li>National Wildlife Refuge System, Southwest Region, U.S. Fish &amp; Wildlife Service</li> <li>Northern Great Plains Program, Smithsonian</li> <li>Quantitative Ecology Lab, University of Washington</li> <li>Santa Monica Mountains Recreation Area, National Park Service</li> <li>Seattle Urban Carnivore Project, Woodland Park Zoo</li> <li>Serra dos \u00d3rg\u00e3os National Park, ICMBio</li> <li>Snapshot USA, Smithsonian</li> <li>Wildlife Coexistence Lab, University of British Columbia</li> <li>Wildlife Research, Oregon Department of Fish and Wildlife</li> <li>Wildlife Division, Michigan Department of Natural Resources</li> <li>Department of Ecology, TU Berlin</li> <li>Ghost Cat Analytics</li> <li>Protected Areas Unit, Canadian Wildlife Service</li> <li>School of Natural Sciences, University of Tasmania (story)</li> <li>Kenai National Wildlife Refuge, U.S. Fish &amp; Wildlife Service (story)</li> <li>Australian Wildlife Conservancy (blog, blog)</li> <li>Felidae Conservation Fund (WildePod platform) (blog post)</li> <li>Alberta Biodiversity Monitoring Institute (ABMI) (WildTrax platform) (blog post)</li> <li>Shan Shui Conservation Center (blog post) (translated blog post)</li> <li>Irvine Ranch Conservancy (story)</li> <li>Wildlife Protection Solutions (story, story)</li> <li>Road Ecology Center, University of California, Davis (Wildlife Observer Network platform)</li> <li>The Nature Conservancy in California (Animl platform)</li> <li>San Diego Zoo Wildlife Alliance (Animl R package)</li> </ul> <p></p> <p>[!IMPORTANT] If you would like to be added to this list or have any questions regarding MegaDetector and Pytorch-Wildlife, please email us or join us in our Discord channel: </p>"},{"location":"contribute/","title":"\ud83e\udd1d Contribution Guidelines","text":"<p>Thanks for your interest in collaborating on PytorchWildlife! Here you can find the guidelines on how to contribute \ud83c\udf31:</p>"},{"location":"contribute/#how-to-participate","title":"How to participate","text":"<ol> <li>Visit our project board: Microsoft CameraTraps Collaboration </li> <li>Look under the \u201cReady\u201d column for issues open to contributors.  </li> <li>Pick an issue and email us with the subject: PyTorchWildlife collaboration on Issue #  Our e-mails: zhongqimiao@microsoft.com, v-hernandres@microsoft.com, v-druizlopez@microsoft.com, v-ichaconsil@microsoft.com <li>Once we receive your email, we\u2019ll move the issue to \u201cIn progress\u201d and assign you as the collaborator.  </li> <li>Develop your solution and open a pull request targeting the <code>Collaborations</code> branch.  </li> <li>We\u2019ll review, run tests, and merge your PR.  </li> <li>After merging, we\u2019ll mark the task as \u201cFinished\u201d and include your contribution credit in the next release.  </li> <p>Thank you for helping us improve PytorchWildlife!</p> <p>We have adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact us with any additional questions or comments.</p>"},{"location":"contributors/","title":"In construction","text":""},{"location":"core_features/","title":"Core Features","text":""},{"location":"core_features/#core-features","title":"\ud83d\udee0\ufe0f Core Features","text":""},{"location":"core_features/#unified-framework","title":"\ud83c\udf10 Unified Framework:","text":"<p>Pytorch-Wildlife integrates four pivotal elements:</p> <p>\u25aa Machine Learning Models \u25aa Pre-trained Weights \u25aa Datasets \u25aa Utilities</p>"},{"location":"core_features/#our-work","title":"\ud83d\udc77 Our work:","text":"<p>In the provided graph, boxes outlined in red represent elements that will be added and remained fixed, while those in blue will be part of our development.</p>"},{"location":"core_features/#inaugural-model","title":"\ud83d\ude80 Inaugural Model:","text":"<p>We're kickstarting with YOLO as our first available model, complemented by pre-trained weights from <code>MegaDetector</code>. We have <code>MegaDetectorV5</code>, which is the same <code>MegaDetectorV5</code> model from the previous repository, and many different versions of <code>MegaDetectorV6</code> for different usecases.</p>"},{"location":"core_features/#expandable-repository","title":"\ud83d\udcda Expandable Repository:","text":"<p>As we move forward, our platform will welcome new models and pre-trained weights for camera traps and bioacoustic analysis. We're excited to host contributions from global researchers through a dedicated submission platform.</p>"},{"location":"core_features/#versatile-utilities","title":"\ud83e\uddf0 Versatile Utilities:","text":"<p>Our set of utilities spans from visualization tools to task-specific utilities, many inherited from Megadetector.</p>"},{"location":"core_features/#user-interface-flexibility","title":"\ud83d\udcbb User Interface Flexibility:","text":"<p>While we provide a foundational user interface, our platform is designed to inspire. We encourage researchers to craft and share their unique interfaces, and we'll list both existing and new UIs from other collaborators for the community's benefit.</p> <p>Let's shape the future of wildlife research, together! \ud83d\ude4c</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python </li> <li>NVIDIA GPU for CUDA support (Optional, the code and demo also supports cpu calculation).</li> <li><code>conda</code> or <code>mamba</code> for python environment management and specific version of <code>opencv</code>.</li> <li>If you are using CUDA. CudaToolkit 12.1 is required. 4.1 If you are using CUDA and you have PytorchWildlife 1.0.2.14 or lower, CudaToolkit 11.3 is required.</li> </ol>"},{"location":"installation/#create-environment","title":"Create environment","text":"<p>If you have <code>conda</code> or <code>mamba</code> installed, you can create a new environment with the following commands (switch <code>conda</code> to <code>mamba</code> for <code>mamba</code> users): <pre><code>conda create -n pytorch-wildlife python=3.10 -y\nconda activate pytorch-wildlife\n</code></pre> NOTE: For Windows users, please use the Anaconda Prompt if you are using Anaconda. Otherwise, please use PowerShell for the conda environment and the rest of the set up.</p>"},{"location":"installation/#ubuntu","title":"Ubuntu","text":"<p>If you are using a clean install of Ubuntu, additional libraries of OpenCV may need to be installed, please run the following command: <pre><code>sudo apt-get update\nsudo apt-get install -y python3-opencv\n</code></pre></p>"},{"location":"installation/#macos","title":"MacOS","text":"<p>If you are using MacOS, please run the following command to install ffmpeg for video decoding: <pre><code>brew install ffmpeg\n</code></pre></p>"},{"location":"installation/#windows","title":"Windows","text":"<p>Windows installation is a bit more complicated due to operating system differences. Please refer to our Windows installation guide for details.</p>"},{"location":"installation/#cuda-for-windows","title":"CUDA for Windows","text":"<p>If you want to use your CUDA-compatible GPU and you are using Windows. Please run the following commands (CUDA 12.1 is required):</p> <pre><code>pip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#install-through-pip","title":"Install through pip:","text":"<pre><code>pip install PytorchWildlife\n</code></pre>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<ol> <li>Install Docker on your OS. Here are the guidelines for Windows, Ubuntu and Mac.</li> <li>Pull the docker image from our DockerHub. <pre><code>docker pull andreshdz/pytorchwildlife:1.0.2.3\n</code></pre></li> <li>Run the gradio demo after pulling the image <pre><code>docker run -p 80:80 andreshdz/pytorchwildlife:1.0.2.3 python demo/gradio_demo.py\n</code></pre></li> <li>If you want to run any code using the docker image,  please use <code>docker run andreshdz/pytorchwildlife:1.0.2.3</code> followed by the command that you want to execute.</li> </ol>"},{"location":"installation/#running-the-demo","title":"Running the Demo","text":"<p>Here is a brief example on how to perform detection and classification on a single image using <code>PyTorch-wildlife</code>:</p> <pre><code>import numpy as np\nfrom PytorchWildlife.models import detection as pw_detection\nfrom PytorchWildlife.models import classification as pw_classification\n\nimg = np.random.randn(3, 1280, 1280)\n\n# Detection\ndetection_model = pw_detection.MegaDetectorV6() # Model weights are automatically downloaded.\ndetection_result = detection_model.single_image_detection(img)\n\n#Classification\nclassification_model = pw_classification.AI4GAmazonRainforest() # Model weights are automatically downloaded.\nclassification_results = classification_model.single_image_classification(img)\n</code></pre> <p>If you want to use our Gradio demo for a user-friendly interface. Please run the following code inside the current repo. You can also find Jupyter Notebooks with an image and video tutorial:</p> <p><pre><code>git clone https://github.com/microsoft/CameraTraps.git\ncd CameraTraps\ncd demo\n# For the image demo\npython image_demo.py\n# For the video demo\npython video_demo.py\n# For the gradio app\npython gradio_demo.py\n</code></pre> The <code>gradio_demo.py</code> will launch a Gradio interface where you can: - Perform Single Image Detection: Upload an image and set a confidence threshold to get detections. - Perform Batch Image Detection: Upload a zip file containing multiple images to get detections in a JSON format. - Perform Video Detection: Upload a video and get a processed video with detected animals. </p> <p>As a showcase platform, the gradio demo offers a hands-on experience with all the available features. However, it's important to note that this interface is primarily for demonstration purposes. While it is fully equipped to run all the features effectively, it may not be optimized for scenarios involving excessive data loads. We advise users to be mindful of this limitation when experimenting with large datasets.</p> <p>Some browsers may not render processed videos due to unsupported codec. If that happens, please either use a newer version of browser or run the following for a <code>conda</code> version of <code>opencv</code> and choose <code>avc1</code> in the Video encoder drop down menu in the webapp (this might not work for MacOS):</p> <pre><code>pip uninstall opencv-python\nconda install -c conda-forge opencv\n</code></pre> <p></p> <p>NOTE: Windows may encounter some errors with large file uploads making Batch Image Detection and Video Detection unable to process. It is a Gradio issue. Newer versions of Gradio in the future may fix this problem.</p>"},{"location":"installation/#using-jupyter-notebooks","title":"Using Jupyter Notebooks","text":"<p>Juptyer helps to progressively understand what each code block does. We have provided a set of demo files that can be read using Jupyter. If you have the Anaconda Navigator installed, you should have the option to run Jupyter. To make sure that the PytorchWildlife environment is recognized by Jupyter, please run the following code while your <code>pytorch-wildlife</code> environment is active: <pre><code>conda install ipykernel\npython -m ipykernel install --user --name pytorch-wildlife --display-name \"Python (PytorchWildlife)\"\n</code></pre> Once you execute the commands, you should be able to choose the <code>Python (PytorchWildlife)</code> kernel to start running the code!</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#model-licensing","title":"Model licensing","text":"<p>The Pytorch-Wildlife package is under MIT, however some of the models in the model zoo are not. For example, MegaDetectorV5, which is trained using the Ultralytics package, a package under AGPL-3.0, and is not for closed-source commercial uses if they are using updated 'ultralytics' packages. </p> <p>There may be a confusion because YOLOv5 was initially released before the establishment of the AGPL-3.0 license. According to the official Ultralytics-Yolov5 package, it is under AGPL-3.0 now, and the maintainers have discussed how their licensing policy has evolved over time in their issues section. </p> <p>We want to make Pytorch-Wildlife a platform where different models with different licenses can be hosted and want to enable different use cases. To reduce user confusions, in our model zoo section, we list all existing and planned future models in our model zoo, their corresponding license, and release schedules. </p> <p>In addition, since the Pytorch-Wildlife package is under MIT, all the utility functions, including data pre-/post-processing functions and model fine-tuning functions in this packages are under MIT as well.</p> <pre><code>MIT License\n\nCopyright (c) [2023] [Microsoft]\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"megadetector/","title":"\ud83d\udc3e Pytorch-Wildlife and MegaDetector","text":"<p>[!TIP] MegaDetector now resides in Pytorch-Wildlife as part of the model zoo.</p> <p>At the core of our mission is the desire to create a harmonious space where conservation scientists from all over the globe can unite. Where they're able to share, grow, use datasets and deep learning architectures for wildlife conservation. We've been inspired by the potential and capabilities of Megadetector, and we deeply value its contributions to the community. As we forge ahead with Pytorch-Wildlife, under which Megadetector now resides, please know that we remain committed to supporting, maintaining, and developing Megadetector, ensuring its continued relevance, expansion, and utility.</p>"},{"location":"megadetector/#megadetectorv6-smaller-faster-better","title":"MegaDetectorV6: SMALLER, FASTER, BETTER!","text":"<p>We have officially released our 6th version of MegaDetector, MegaDetectorV6! In the next generation of MegaDetector, we are focusing on computational efficiency, performance, modernizing of model architectures, and licensing. We have trained multiple new models using different model architectures that are optimized for performance and low-budget devices, including Yolo-v9, Yolo-v10, and RT-Detr for maximum user flexibility. For example, the MegaDetectorV6-Ultralytics-YoloV10-Compact (MDV6-yolov10-c) model only have 2% of the parameters of the previous MegaDetectorV5 and still exhibits comparable performance on our validation datasets. </p> <p>To test the newest version of MegaDetector with all the existing functionalities, you can use our Hugging Face interface or simply load the model with Pytorch-Wildlife. The weights will be automatically downloaded:  <pre><code>from PytorchWildlife.models import detection as pw_detection\ndetection_model = pw_detection.MegaDetectorV6()\n</code></pre></p> <p>We will also continuously fine-tune our V6 models on newly collected public and private data to further improve the generalization performance.</p> <p>[!TIP] All versions of MegaDetector and corresponding performance can be found in the model zoo.</p> <p>From now on, we encourage our users to use MegaDetectorV6 as their default animal detection model and choose whichever model that fits the project needs. To reduce potential confusion, we have also standardized the model names into MDV6-Compact and MDV6-Extra for two model sizes using the same architecture. Learn how to use MegaDetectorV6 in our image demo and our demo data installtion guideline.</p>"},{"location":"megadetector/#megadetectorv5-and-archive-repos","title":"MegaDetectorV5 and Archive Repos","text":"<p>For those interested in accessing the previous MegaDetector repository, which utilizes the same <code>MegaDetectorV5</code> model weights and was primarily developed by Dan Morris during his time at Microsoft, please visit the archive branch , or you can visit this forked repository that Dan Morris is currently actively maintaining.</p> <p>[!TIP] If you have any questions regarding MegaDetector and Pytorch-Wildlife, please email us or join us in our discord channel: </p>"},{"location":"base/overview/","title":"PytorchWildlife Base Module","text":"<p>The <code>PytorchWildlife</code> base module is the core component of this repository, designed to facilitate wildlife detection and classification tasks using PyTorch. It provides utilities for data processing, model implementation, and post-processing. It is also what is currently packaged in our Python package.</p>"},{"location":"base/overview/#overview","title":"Overview","text":"<p>The module is structured into the following submodules:</p> <ul> <li><code>data</code>: Contains utilities for handling datasets and applying transformations.</li> <li><code>models</code>: Includes implementations for classification and detection models.</li> <li><code>utils</code>: Provides miscellaneous utilities for post-processing and other tasks.</li> </ul>"},{"location":"base/overview/#submodules","title":"Submodules","text":""},{"location":"base/overview/#data","title":"<code>data</code>","text":"<ul> <li><code>datasets.py</code>: Defines dataset classes for loading and preprocessing data.</li> <li><code>transforms.py</code>: Implements data augmentation and transformation utilities.</li> </ul>"},{"location":"base/overview/#models","title":"<code>models</code>","text":"<ul> <li><code>classification/</code>: Contains classification model architectures.</li> <li><code>detection/</code>: Includes detection model architectures.</li> </ul>"},{"location":"base/overview/#utils","title":"<code>utils</code>","text":"<ul> <li><code>misc.py</code>: Provides helper functions for miscellaneous tasks.</li> <li><code>post_process.py</code>: Implements post-processing utilities for model outputs.</li> </ul>"},{"location":"base/overview/#getting-started","title":"Getting Started","text":"<p>To use the <code>PytorchWildlife</code> module, import the required submodules as follows:</p> <pre><code>from PytorchWildlife.data import datasets, transforms\nfrom PytorchWildlife.models import classification, detection\nfrom PytorchWildlife.utils import misc, post_process\n</code></pre> <p>Refer to the specific submodule documentation for detailed usage instructions.</p>"},{"location":"base/data/datasets/","title":"Datasets Module","text":""},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ClassificationImageFolder","title":"<code>ClassificationImageFolder</code>","text":"<p>               Bases: <code>ImageFolder</code></p> <p>A PyTorch Dataset for loading images from a specified directory. Each item in the dataset is a tuple containing the image data,  the image's path, and the original size of the image.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>class ClassificationImageFolder(ImageFolder):\n    \"\"\"\n    A PyTorch Dataset for loading images from a specified directory.\n    Each item in the dataset is a tuple containing the image data, \n    the image's path, and the original size of the image.\n    \"\"\"\n\n    def __init__(self, image_dir, transform=None):\n        \"\"\"\n        Initializes the dataset.\n\n        Parameters:\n            image_dir (str): Path to the directory containing the images.\n            transform (callable, optional): Optional transform to be applied on the image.\n        \"\"\"\n        super(ClassificationImageFolder, self).__init__(image_dir, transform)\n\n    def __getitem__(self, idx) -&gt; tuple:\n        \"\"\"\n        Retrieves an image from the dataset.\n\n        Parameters:\n            idx (int): Index of the image to retrieve.\n\n        Returns:\n            tuple: Contains the image data, the image's path, and its original size.\n        \"\"\"\n        # Get image filename and path\n        img_path = self.images[idx]\n\n        # Load and convert image to RGB\n        img = Image.open(img_path).convert(\"RGB\")\n\n        # Apply transformation if specified\n        if self.transform:\n            img = self.transform(img)\n\n        return img, img_path\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ClassificationImageFolder.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves an image from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the image to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Contains the image data, the image's path, and its original size.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __getitem__(self, idx) -&gt; tuple:\n    \"\"\"\n    Retrieves an image from the dataset.\n\n    Parameters:\n        idx (int): Index of the image to retrieve.\n\n    Returns:\n        tuple: Contains the image data, the image's path, and its original size.\n    \"\"\"\n    # Get image filename and path\n    img_path = self.images[idx]\n\n    # Load and convert image to RGB\n    img = Image.open(img_path).convert(\"RGB\")\n\n    # Apply transformation if specified\n    if self.transform:\n        img = self.transform(img)\n\n    return img, img_path\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ClassificationImageFolder.__init__","title":"<code>__init__(image_dir, transform=None)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied on the image.</p> <code>None</code> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __init__(self, image_dir, transform=None):\n    \"\"\"\n    Initializes the dataset.\n\n    Parameters:\n        image_dir (str): Path to the directory containing the images.\n        transform (callable, optional): Optional transform to be applied on the image.\n    \"\"\"\n    super(ClassificationImageFolder, self).__init__(image_dir, transform)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.DetectionCrops","title":"<code>DetectionCrops</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>class DetectionCrops(Dataset):\n\n    def __init__(self, detection_results, transform=None, path_head=None, animal_cls_id=0):\n\n        self.detection_results = detection_results\n        self.transform = transform\n        self.path_head = path_head\n        self.animal_cls_id = animal_cls_id # This determines which detection class id represents animals.\n        self.img_ids = []\n        self.xyxys = []\n\n        self.load_detection_results()\n\n    def load_detection_results(self):\n        for det in self.detection_results:\n            for xyxy, det_id in zip(det[\"detections\"].xyxy, det[\"detections\"].class_id):\n                # Only run recognition on animal detections\n                if det_id == self.animal_cls_id:\n                    self.img_ids.append(det[\"img_id\"])\n                    self.xyxys.append(xyxy)\n\n    def __getitem__(self, idx) -&gt; tuple:\n        \"\"\"\n        Retrieves an image from the dataset.\n\n        Parameters:\n            idx (int): Index of the image to retrieve.\n\n        Returns:\n            tuple: Contains the image data and the image's path.\n        \"\"\"\n\n        # Get image path and corresponding bbox xyxy for cropping\n        img_id = self.img_ids[idx]\n        xyxy = self.xyxys[idx]\n\n        img_path = os.path.join(self.path_head, img_id) if self.path_head else img_id\n\n        # Load and crop image with supervision\n        img = sv.crop_image(np.array(Image.open(img_path).convert(\"RGB\")),\n                            xyxy=xyxy)\n\n        # Apply transformation if specified\n        if self.transform:\n            img = self.transform(Image.fromarray(img))\n\n        return img, img_path\n\n    def __len__(self) -&gt; int:\n        return len(self.img_ids)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.DetectionCrops.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves an image from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the image to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Contains the image data and the image's path.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __getitem__(self, idx) -&gt; tuple:\n    \"\"\"\n    Retrieves an image from the dataset.\n\n    Parameters:\n        idx (int): Index of the image to retrieve.\n\n    Returns:\n        tuple: Contains the image data and the image's path.\n    \"\"\"\n\n    # Get image path and corresponding bbox xyxy for cropping\n    img_id = self.img_ids[idx]\n    xyxy = self.xyxys[idx]\n\n    img_path = os.path.join(self.path_head, img_id) if self.path_head else img_id\n\n    # Load and crop image with supervision\n    img = sv.crop_image(np.array(Image.open(img_path).convert(\"RGB\")),\n                        xyxy=xyxy)\n\n    # Apply transformation if specified\n    if self.transform:\n        img = self.transform(Image.fromarray(img))\n\n    return img, img_path\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.DetectionImageFolder","title":"<code>DetectionImageFolder</code>","text":"<p>               Bases: <code>ImageFolder</code></p> <p>A PyTorch Dataset for loading images from a specified directory. Each item in the dataset is a tuple containing the image data,  the image's path, and the original size of the image.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>class DetectionImageFolder(ImageFolder):\n    \"\"\"\n    A PyTorch Dataset for loading images from a specified directory.\n    Each item in the dataset is a tuple containing the image data, \n    the image's path, and the original size of the image.\n    \"\"\"\n\n    def __init__(self, image_dir, transform=None):\n        \"\"\"\n        Initializes the dataset.\n\n        Parameters:\n            image_dir (str): Path to the directory containing the images.\n            transform (callable, optional): Optional transform to be applied on the image.\n        \"\"\"\n        super(DetectionImageFolder, self).__init__(image_dir, transform)\n\n    def __getitem__(self, idx) -&gt; tuple:\n        \"\"\"\n        Retrieves an image from the dataset.\n\n        Parameters:\n            idx (int): Index of the image to retrieve.\n\n        Returns:\n            tuple: Contains the image data, the image's path, and its original size.\n        \"\"\"\n        # Get image filename and path\n        img_path = self.images[idx]\n\n        # Load and convert image to RGB\n        img = Image.open(img_path).convert(\"RGB\")\n        img_size_ori = img.size[::-1]\n\n        # Apply transformation if specified\n        if self.transform:\n            img = self.transform(img)\n\n        return img, img_path, torch.tensor(img_size_ori)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.DetectionImageFolder.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves an image from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the image to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Contains the image data, the image's path, and its original size.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __getitem__(self, idx) -&gt; tuple:\n    \"\"\"\n    Retrieves an image from the dataset.\n\n    Parameters:\n        idx (int): Index of the image to retrieve.\n\n    Returns:\n        tuple: Contains the image data, the image's path, and its original size.\n    \"\"\"\n    # Get image filename and path\n    img_path = self.images[idx]\n\n    # Load and convert image to RGB\n    img = Image.open(img_path).convert(\"RGB\")\n    img_size_ori = img.size[::-1]\n\n    # Apply transformation if specified\n    if self.transform:\n        img = self.transform(img)\n\n    return img, img_path, torch.tensor(img_size_ori)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.DetectionImageFolder.__init__","title":"<code>__init__(image_dir, transform=None)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied on the image.</p> <code>None</code> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __init__(self, image_dir, transform=None):\n    \"\"\"\n    Initializes the dataset.\n\n    Parameters:\n        image_dir (str): Path to the directory containing the images.\n        transform (callable, optional): Optional transform to be applied on the image.\n    \"\"\"\n    super(DetectionImageFolder, self).__init__(image_dir, transform)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ImageFolder","title":"<code>ImageFolder</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for loading images from a specified directory. Each item in the dataset is a tuple containing the image data,  the image's path, and the original size of the image.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>class ImageFolder(Dataset):\n    \"\"\"\n    A PyTorch Dataset for loading images from a specified directory.\n    Each item in the dataset is a tuple containing the image data, \n    the image's path, and the original size of the image.\n    \"\"\"\n\n    def __init__(self, image_dir, transform=None):\n        \"\"\"\n        Initializes the dataset.\n\n        Parameters:\n            image_dir (str): Path to the directory containing the images.\n            transform (callable, optional): Optional transform to be applied on the image.\n        \"\"\"\n        super(ImageFolder, self).__init__()\n        self.image_dir = image_dir\n        self.transform = transform\n        self.images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(image_dir) for f in filenames if is_image_file(f)] # dp: directory path, dn: directory name, f: filename\n\n    def __getitem__(self, idx) -&gt; tuple:\n        \"\"\"\n        Retrieves an image from the dataset.\n\n        Parameters:\n            idx (int): Index of the image to retrieve.\n\n        Returns:\n            tuple: Contains the image data, the image's path, and its original size.\n        \"\"\"\n        pass\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the total number of images in the dataset.\n\n        Returns:\n            int: Total number of images.\n        \"\"\"\n        return len(self.images)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ImageFolder.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Retrieves an image from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the image to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Contains the image data, the image's path, and its original size.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __getitem__(self, idx) -&gt; tuple:\n    \"\"\"\n    Retrieves an image from the dataset.\n\n    Parameters:\n        idx (int): Index of the image to retrieve.\n\n    Returns:\n        tuple: Contains the image data, the image's path, and its original size.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ImageFolder.__init__","title":"<code>__init__(image_dir, transform=None)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>transform</code> <code>callable</code> <p>Optional transform to be applied on the image.</p> <code>None</code> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __init__(self, image_dir, transform=None):\n    \"\"\"\n    Initializes the dataset.\n\n    Parameters:\n        image_dir (str): Path to the directory containing the images.\n        transform (callable, optional): Optional transform to be applied on the image.\n    \"\"\"\n    super(ImageFolder, self).__init__()\n    self.image_dir = image_dir\n    self.transform = transform\n    self.images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(image_dir) for f in filenames if is_image_file(f)] # dp: directory path, dn: directory name, f: filename\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.ImageFolder.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of images in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of images.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the total number of images in the dataset.\n\n    Returns:\n        int: Total number of images.\n    \"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.has_file_allowed_extension","title":"<code>has_file_allowed_extension(filename, extensions)</code>","text":"<p>Checks if a file is an allowed extension.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def has_file_allowed_extension(filename: str, extensions: tuple) -&gt; bool:  \n    \"\"\"Checks if a file is an allowed extension.\"\"\"  \n    return filename.lower().endswith(extensions if isinstance(extensions, str) else tuple(extensions))\n</code></pre>"},{"location":"base/data/datasets/#PytorchWildlife.data.datasets.is_image_file","title":"<code>is_image_file(filename)</code>","text":"<p>Checks if a file is an allowed image extension.</p> Source code in <code>PytorchWildlife/data/datasets.py</code> <pre><code>def is_image_file(filename: str) -&gt; bool:  \n    \"\"\"Checks if a file is an allowed image extension.\"\"\"  \n    return has_file_allowed_extension(filename, IMG_EXTENSIONS) \n</code></pre>"},{"location":"base/data/transforms/","title":"Transforms Module","text":""},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.Classification_Inference_Transform","title":"<code>Classification_Inference_Transform</code>","text":"<p>A transformation class to preprocess images for classification inference. This includes resizing, normalization, and conversion to a tensor.</p> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>class Classification_Inference_Transform:\n    \"\"\"\n    A transformation class to preprocess images for classification inference.\n    This includes resizing, normalization, and conversion to a tensor.\n    \"\"\"\n    # Normalization constants\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    def __init__(self, target_size=224, **kwargs):\n        \"\"\"\n        Initializes the transform.\n\n        Args:\n            target_size (int): Desired size for the height and width after resizing.\n        \"\"\"\n        # Define the sequence of transformations\n        self.trans = transforms.Compose([\n            # transforms.Resize((target_size, target_size)),\n            transforms.Resize((target_size, target_size), **kwargs),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n    def __call__(self, img) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the transformation on the provided image.\n\n        Args:\n            img (PIL.Image.Image): Input image in PIL format.\n\n        Returns:\n            torch.Tensor: Transformed image.\n        \"\"\"\n        img = self.trans(img)\n        return img\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.Classification_Inference_Transform.__call__","title":"<code>__call__(img)</code>","text":"<p>Applies the transformation on the provided image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image</code> <p>Input image in PIL format.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Transformed image.</p> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>def __call__(self, img) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the transformation on the provided image.\n\n    Args:\n        img (PIL.Image.Image): Input image in PIL format.\n\n    Returns:\n        torch.Tensor: Transformed image.\n    \"\"\"\n    img = self.trans(img)\n    return img\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.Classification_Inference_Transform.__init__","title":"<code>__init__(target_size=224, **kwargs)</code>","text":"<p>Initializes the transform.</p> <p>Parameters:</p> Name Type Description Default <code>target_size</code> <code>int</code> <p>Desired size for the height and width after resizing.</p> <code>224</code> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>def __init__(self, target_size=224, **kwargs):\n    \"\"\"\n    Initializes the transform.\n\n    Args:\n        target_size (int): Desired size for the height and width after resizing.\n    \"\"\"\n    # Define the sequence of transformations\n    self.trans = transforms.Compose([\n        # transforms.Resize((target_size, target_size)),\n        transforms.Resize((target_size, target_size), **kwargs),\n        transforms.ToTensor(),\n        transforms.Normalize(self.mean, self.std)\n    ])\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.MegaDetector_v5_Transform","title":"<code>MegaDetector_v5_Transform</code>","text":"<p>A transformation class to preprocess images for the MegaDetector v5 model. This includes resizing, transposing, and normalization operations. This is a required transformation for the YoloV5 model.</p> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>class MegaDetector_v5_Transform:\n    \"\"\"\n    A transformation class to preprocess images for the MegaDetector v5 model.\n    This includes resizing, transposing, and normalization operations.\n    This is a required transformation for the YoloV5 model.\n\n    \"\"\"\n\n    def __init__(self, target_size=1280, stride=32):\n        \"\"\"\n        Initializes the transform.\n\n        Args:\n            target_size (int): Desired size for the image's longest side after resizing.\n            stride (int): Stride value for resizing.\n        \"\"\"\n        self.target_size = target_size\n        self.stride = stride\n\n    def __call__(self, np_img) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the transformation on the provided image.\n\n        Args:\n            np_img (np.ndarray): Input image as a numpy array or PIL Image.\n\n        Returns:\n            torch.Tensor: Transformed image.\n        \"\"\"\n        # Convert the image to a PyTorch tensor and normalize it\n        if isinstance(np_img, np.ndarray):\n            np_img = np_img.transpose((2, 0, 1))\n            np_img = np.ascontiguousarray(np_img)\n            np_img = torch.from_numpy(np_img).float()\n            np_img /= 255.0\n\n        # Resize and pad the image using a customized letterbox function. \n        img = letterbox(np_img, new_shape=self.target_size, stride=self.stride, auto=False)\n\n        return img\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.MegaDetector_v5_Transform.__call__","title":"<code>__call__(np_img)</code>","text":"<p>Applies the transformation on the provided image.</p> <p>Parameters:</p> Name Type Description Default <code>np_img</code> <code>ndarray</code> <p>Input image as a numpy array or PIL Image.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Transformed image.</p> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>def __call__(self, np_img) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the transformation on the provided image.\n\n    Args:\n        np_img (np.ndarray): Input image as a numpy array or PIL Image.\n\n    Returns:\n        torch.Tensor: Transformed image.\n    \"\"\"\n    # Convert the image to a PyTorch tensor and normalize it\n    if isinstance(np_img, np.ndarray):\n        np_img = np_img.transpose((2, 0, 1))\n        np_img = np.ascontiguousarray(np_img)\n        np_img = torch.from_numpy(np_img).float()\n        np_img /= 255.0\n\n    # Resize and pad the image using a customized letterbox function. \n    img = letterbox(np_img, new_shape=self.target_size, stride=self.stride, auto=False)\n\n    return img\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.MegaDetector_v5_Transform.__init__","title":"<code>__init__(target_size=1280, stride=32)</code>","text":"<p>Initializes the transform.</p> <p>Parameters:</p> Name Type Description Default <code>target_size</code> <code>int</code> <p>Desired size for the image's longest side after resizing.</p> <code>1280</code> <code>stride</code> <code>int</code> <p>Stride value for resizing.</p> <code>32</code> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>def __init__(self, target_size=1280, stride=32):\n    \"\"\"\n    Initializes the transform.\n\n    Args:\n        target_size (int): Desired size for the image's longest side after resizing.\n        stride (int): Stride value for resizing.\n    \"\"\"\n    self.target_size = target_size\n    self.stride = stride\n</code></pre>"},{"location":"base/data/transforms/#PytorchWildlife.data.transforms.letterbox","title":"<code>letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True, stride=32)</code>","text":"<p>Resize and pad an image to a desired shape while keeping the aspect ratio unchanged.</p> <p>This function is commonly used in object detection tasks to prepare images for models like YOLOv5.  It resizes the image to fit into the new shape with the correct aspect ratio and then pads the rest.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image or Tensor</code> <p>The input image. It can be a PIL image or a PyTorch tensor.</p> required <code>new_shape</code> <code>tuple</code> <p>The target size of the image, in the form (height, width). Defaults to (640, 640).</p> <code>(640, 640)</code> <code>color</code> <code>tuple</code> <p>The color used for padding. Defaults to (114, 114, 114).</p> <code>(114, 114, 114)</code> <code>auto</code> <code>bool</code> <p>Adjust padding to ensure the padded image dimensions are a multiple of the stride. Defaults to False.</p> <code>False</code> <code>scaleFill</code> <code>bool</code> <p>If True, scales the image to fill the new shape, ignoring the aspect ratio. Defaults to False.</p> <code>False</code> <code>scaleup</code> <code>bool</code> <p>Allow the function to scale up the image. Defaults to True.</p> <code>True</code> <code>stride</code> <code>int</code> <p>The stride used in the model. The padding is adjusted to be a multiple of this stride. Defaults to 32.</p> <code>32</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The transformed image with padding applied.</p> Source code in <code>PytorchWildlife/data/transforms.py</code> <pre><code>def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True, stride=32) -&gt; torch.Tensor:\n    \"\"\"\n    Resize and pad an image to a desired shape while keeping the aspect ratio unchanged.\n\n    This function is commonly used in object detection tasks to prepare images for models like YOLOv5. \n    It resizes the image to fit into the new shape with the correct aspect ratio and then pads the rest.\n\n    Args:\n        im (PIL.Image.Image or torch.Tensor): The input image. It can be a PIL image or a PyTorch tensor.\n        new_shape (tuple, optional): The target size of the image, in the form (height, width). Defaults to (640, 640).\n        color (tuple, optional): The color used for padding. Defaults to (114, 114, 114).\n        auto (bool, optional): Adjust padding to ensure the padded image dimensions are a multiple of the stride. Defaults to False.\n        scaleFill (bool, optional): If True, scales the image to fill the new shape, ignoring the aspect ratio. Defaults to False.\n        scaleup (bool, optional): Allow the function to scale up the image. Defaults to True.\n        stride (int, optional): The stride used in the model. The padding is adjusted to be a multiple of this stride. Defaults to 32.\n\n    Returns:\n        torch.Tensor: The transformed image with padding applied.\n    \"\"\"\n\n    # Convert PIL Image to Torch Tensor\n\n    if isinstance(im, Image.Image):\n        im = T.ToTensor()(im)\n\n    # Original shape\n    shape = im.shape[1:]  # shape = [height, width]\n\n    # New shape\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old) and compute padding\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:\n        r = min(r, 1.0)\n\n    new_unpad = (int(round(shape[1] * r)), int(round(shape[0] * r)))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n\n    if auto:\n        dw, dh = dw % stride, dh % stride\n    elif scaleFill:\n        dw, dh = 0, 0\n        new_unpad = new_shape\n        r = new_shape[1] / shape[1], new_shape[0] / shape[0]\n\n    dw /= 2\n    dh /= 2\n\n    # Resize image\n    if shape[::-1] != new_unpad:\n        resize_transform = T.Resize(new_unpad[::-1], interpolation=T.InterpolationMode.BILINEAR,\n                                    antialias=False)\n        im = resize_transform(im)\n\n    # Pad image\n    padding = (int(round(dw - 0.1)), int(round(dw + 0.1)), int(round(dh + 0.1)), int(round(dh - 0.1)))\n    im = F.pad(im*255.0, padding, value=114)/255.0\n\n    return im\n</code></pre>"},{"location":"base/models/classification/base_classifier/","title":"Base Classifier","text":""},{"location":"base/models/classification/base_classifier/#PytorchWildlife.models.classification.base_classifier.BaseClassifierInference","title":"<code>BaseClassifierInference</code>","text":"<p>               Bases: <code>Module</code></p> <p>Inference module for the PlainResNet Classifier.</p> Source code in <code>PytorchWildlife/models/classification/base_classifier.py</code> <pre><code>class BaseClassifierInference(nn.Module):\n    \"\"\"\n    Inference module for the PlainResNet Classifier.\n    \"\"\"\n    def __init__(self):\n        super(BaseClassifierInference, self).__init__()\n        pass\n\n    def results_generation(self):\n        pass\n\n    def forward(self):\n        pass\n\n    def single_image_classification(self):\n        pass\n\n    def batch_image_classification(self):\n        pass\n</code></pre>"},{"location":"base/models/classification/resnet_base/amazon/","title":"Amazon","text":""},{"location":"base/models/classification/resnet_base/amazon/#PytorchWildlife.models.classification.resnet_base.amazon.AI4GAmazonRainforest","title":"<code>AI4GAmazonRainforest</code>","text":"<p>               Bases: <code>PlainResNetInference</code></p> <p>Amazon Ranforest Animal Classifier that inherits from PlainResNetInference. This classifier is specialized for recognizing 36 different animals in the Amazon Rainforest.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/amazon.py</code> <pre><code>class AI4GAmazonRainforest(PlainResNetInference):\n    \"\"\"\n    Amazon Ranforest Animal Classifier that inherits from PlainResNetInference.\n    This classifier is specialized for recognizing 36 different animals in the Amazon Rainforest.\n    \"\"\"\n\n    # Image size for the Opossum classifier\n    IMAGE_SIZE = 224\n\n    # Class names for prediction\n    CLASS_NAMES = {\n        0: 'Dasyprocta',\n        1: 'Bos',\n        2: 'Pecari',\n        3: 'Mazama',\n        4: 'Cuniculus',\n        5: 'Leptotila',\n        6: 'Human',\n        7: 'Aramides',\n        8: 'Tinamus',\n        9: 'Eira',\n        10: 'Crax',\n        11: 'Procyon',\n        12: 'Capra',\n        13: 'Dasypus',\n        14: 'Sciurus',\n        15: 'Crypturellus',\n        16: 'Tamandua',\n        17: 'Proechimys',\n        18: 'Leopardus',\n        19: 'Equus',\n        20: 'Columbina',\n        21: 'Nyctidromus',\n        22: 'Ortalis',\n        23: 'Emballonura',\n        24: 'Odontophorus',\n        25: 'Geotrygon',\n        26: 'Metachirus',\n        27: 'Catharus',\n        28: 'Cerdocyon',\n        29: 'Momotus',\n        30: 'Tapirus',\n        31: 'Canis',\n        32: 'Furnarius',\n        33: 'Didelphis',\n        34: 'Sylvilagus',\n        35: 'Unknown'\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True, version=\"v2\"):\n        \"\"\"\n        Initialize the Amazon animal Classifier.\n\n        Args:\n            weights (str, optional): Path to the model weights. Defaults to None.\n            device (str, optional): Device for model inference. Defaults to \"cpu\".\n            pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n            version (str, optional): Version of the model to load. Default is 'v2'.\n        \"\"\"\n\n        # If pretrained, use the provided URL to fetch the weights\n        if pretrained:\n            if version == 'v1':\n                url = \"https://zenodo.org/records/10042023/files/AI4GAmazonClassification_v0.0.0.ckpt?download=1\"\n            elif version == 'v2':\n                url = \"https://zenodo.org/records/14252214/files/AI4GAmazonDeforestationv2?download=1\"\n        else:\n            url = None\n\n        super(AI4GAmazonRainforest, self).__init__(weights=weights, device=device,\n                                                   num_cls=36, num_layers=50, url=url)\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Generate results for classification.\n\n        Args:\n            logits (torch.Tensor): Output tensor from the model.\n            img_ids (str): Image identifier.\n            id_strip (str): stiping string for better image id saving.       \n\n        Returns:\n            dict: Dictionary containing image ID, prediction, and confidence score.\n        \"\"\"\n\n        probs = torch.softmax(logits, dim=1)\n        preds = probs.argmax(dim=1)\n        confs = probs.max(dim=1)[0]\n        confidences = probs[0].tolist()\n        result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n        results = []\n        for pred, img_id, conf in zip(preds, img_ids, confs):\n            r = {\"img_id\": str(img_id).strip(id_strip)}\n            r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n            r[\"class_id\"] = pred.item()\n            r[\"confidence\"] = conf.item()\n            r[\"all_confidences\"] = result\n            results.append(r)\n\n        return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/amazon/#PytorchWildlife.models.classification.resnet_base.amazon.AI4GAmazonRainforest.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True, version='v2')</code>","text":"<p>Initialize the Amazon animal Classifier.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained weights. Defaults to True.</p> <code>True</code> <code>version</code> <code>str</code> <p>Version of the model to load. Default is 'v2'.</p> <code>'v2'</code> Source code in <code>PytorchWildlife/models/classification/resnet_base/amazon.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True, version=\"v2\"):\n    \"\"\"\n    Initialize the Amazon animal Classifier.\n\n    Args:\n        weights (str, optional): Path to the model weights. Defaults to None.\n        device (str, optional): Device for model inference. Defaults to \"cpu\".\n        pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n        version (str, optional): Version of the model to load. Default is 'v2'.\n    \"\"\"\n\n    # If pretrained, use the provided URL to fetch the weights\n    if pretrained:\n        if version == 'v1':\n            url = \"https://zenodo.org/records/10042023/files/AI4GAmazonClassification_v0.0.0.ckpt?download=1\"\n        elif version == 'v2':\n            url = \"https://zenodo.org/records/14252214/files/AI4GAmazonDeforestationv2?download=1\"\n    else:\n        url = None\n\n    super(AI4GAmazonRainforest, self).__init__(weights=weights, device=device,\n                                               num_cls=36, num_layers=50, url=url)\n</code></pre>"},{"location":"base/models/classification/resnet_base/amazon/#PytorchWildlife.models.classification.resnet_base.amazon.AI4GAmazonRainforest.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Generate results for classification.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output tensor from the model.</p> required <code>img_ids</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>stiping string for better image id saving.       </p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>Dictionary containing image ID, prediction, and confidence score.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/amazon.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Generate results for classification.\n\n    Args:\n        logits (torch.Tensor): Output tensor from the model.\n        img_ids (str): Image identifier.\n        id_strip (str): stiping string for better image id saving.       \n\n    Returns:\n        dict: Dictionary containing image ID, prediction, and confidence score.\n    \"\"\"\n\n    probs = torch.softmax(logits, dim=1)\n    preds = probs.argmax(dim=1)\n    confs = probs.max(dim=1)[0]\n    confidences = probs[0].tolist()\n    result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n    results = []\n    for pred, img_id, conf in zip(preds, img_ids, confs):\n        r = {\"img_id\": str(img_id).strip(id_strip)}\n        r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n        r[\"class_id\"] = pred.item()\n        r[\"confidence\"] = conf.item()\n        r[\"all_confidences\"] = result\n        results.append(r)\n\n    return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/","title":"ResNet Base","text":""},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetClassifier","title":"<code>PlainResNetClassifier</code>","text":"<p>               Bases: <code>Module</code></p> <p>Basic ResNet Classifier that uses a custom ResNet backbone.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>class PlainResNetClassifier(nn.Module):\n    \"\"\"\n    Basic ResNet Classifier that uses a custom ResNet backbone.\n    \"\"\"\n    name = \"PlainResNetClassifier\"\n\n    def __init__(self, num_cls=1, num_layers=50):\n        super(PlainResNetClassifier, self).__init__()\n        self.num_cls = num_cls\n        self.num_layers = num_layers\n        self.feature = None\n        self.classifier = None\n        self.criterion_cls = None\n        # Initialize the network and weights\n        self.setup_net()\n\n    def setup_net(self):\n        \"\"\"\n        Set up the ResNet classifier according to the specified number of layers.\n        \"\"\"\n        kwargs = {}\n\n        if self.num_layers == 18:\n            block = BasicBlock\n            layers = [2, 2, 2, 2]\n            # ... [Missing weight URL definition for ResNet18]\n        elif self.num_layers == 50:\n            block = Bottleneck\n            layers = [3, 4, 6, 3]\n            # ... [Missing weight URL definition for ResNet50]\n        else:\n            raise Exception(\"ResNet Type not supported.\")\n\n        self.feature = ResNetBackbone(block, layers, **kwargs)\n        self.classifier = nn.Linear(512 * block.expansion, self.num_cls)\n\n    def setup_criteria(self):\n        \"\"\"\n        Setup the criterion for classification.\n        \"\"\"\n        self.criterion_cls = nn.CrossEntropyLoss()\n\n    def feat_init(self):\n        \"\"\"\n        Initialize the features using pretrained weights.\n        \"\"\"\n        init_weights = self.pretrained_weights.get_state_dict(progress=True)\n        init_weights = OrderedDict({k.replace(\"module.\", \"\").replace(\"feature.\", \"\"): init_weights[k]\n                                    for k in init_weights})\n        self.feature.load_state_dict(init_weights, strict=False)\n        # Print missing and unused keys for debugging purposes\n        load_keys = set(init_weights.keys())\n        self_keys = set(self.feature.state_dict().keys())\n        missing_keys = self_keys - load_keys\n        unused_keys = load_keys - self_keys\n        print(\"missing keys:\", sorted(list(missing_keys)))\n        print(\"unused_keys:\", sorted(list(unused_keys)))\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetClassifier.feat_init","title":"<code>feat_init()</code>","text":"<p>Initialize the features using pretrained weights.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>def feat_init(self):\n    \"\"\"\n    Initialize the features using pretrained weights.\n    \"\"\"\n    init_weights = self.pretrained_weights.get_state_dict(progress=True)\n    init_weights = OrderedDict({k.replace(\"module.\", \"\").replace(\"feature.\", \"\"): init_weights[k]\n                                for k in init_weights})\n    self.feature.load_state_dict(init_weights, strict=False)\n    # Print missing and unused keys for debugging purposes\n    load_keys = set(init_weights.keys())\n    self_keys = set(self.feature.state_dict().keys())\n    missing_keys = self_keys - load_keys\n    unused_keys = load_keys - self_keys\n    print(\"missing keys:\", sorted(list(missing_keys)))\n    print(\"unused_keys:\", sorted(list(unused_keys)))\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetClassifier.setup_criteria","title":"<code>setup_criteria()</code>","text":"<p>Setup the criterion for classification.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>def setup_criteria(self):\n    \"\"\"\n    Setup the criterion for classification.\n    \"\"\"\n    self.criterion_cls = nn.CrossEntropyLoss()\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetClassifier.setup_net","title":"<code>setup_net()</code>","text":"<p>Set up the ResNet classifier according to the specified number of layers.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>def setup_net(self):\n    \"\"\"\n    Set up the ResNet classifier according to the specified number of layers.\n    \"\"\"\n    kwargs = {}\n\n    if self.num_layers == 18:\n        block = BasicBlock\n        layers = [2, 2, 2, 2]\n        # ... [Missing weight URL definition for ResNet18]\n    elif self.num_layers == 50:\n        block = Bottleneck\n        layers = [3, 4, 6, 3]\n        # ... [Missing weight URL definition for ResNet50]\n    else:\n        raise Exception(\"ResNet Type not supported.\")\n\n    self.feature = ResNetBackbone(block, layers, **kwargs)\n    self.classifier = nn.Linear(512 * block.expansion, self.num_cls)\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetInference","title":"<code>PlainResNetInference</code>","text":"<p>               Bases: <code>BaseClassifierInference</code></p> <p>Inference module for the PlainResNet Classifier.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>class PlainResNetInference(BaseClassifierInference):\n    \"\"\"\n    Inference module for the PlainResNet Classifier.\n    \"\"\"\n    IMAGE_SIZE = None\n    def __init__(self, num_cls=36, num_layers=50, weights=None, device=\"cpu\", url=None, transform=None):\n        super(PlainResNetInference, self).__init__()\n        self.device = device\n        self.net = PlainResNetClassifier(num_cls=num_cls, num_layers=num_layers)\n        if weights:\n            clf_weights = torch.load(weights, map_location=torch.device(self.device))\n        elif url:\n            clf_weights = load_state_dict_from_url(url, map_location=torch.device(self.device))\n        else:\n            raise Exception(\"Need weights for inference.\")\n        self.load_state_dict(clf_weights[\"state_dict\"], strict=True)\n        self.eval()\n        self.net.to(self.device)\n\n        if transform:\n            self.transform = transform\n        else:\n            self.transform = pw_trans.Classification_Inference_Transform(target_size=self.IMAGE_SIZE)\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Process logits to produce final results. \n\n        Args:\n            logits (torch.Tensor): Logits from the network.\n            img_ids (list[str]): List of image paths.\n            id_strip (str): Stripping string for better image ID saving.       \n\n        Returns:\n            list[dict]: List of dictionaries containing the results.\n        \"\"\"\n        pass\n\n    def forward(self, img):\n        feats = self.net.feature(img)\n        logits = self.net.classifier(feats)\n        return logits\n\n    def single_image_classification(self, img, img_id=None, id_strip=None):\n        if type(img) == str:\n            img = Image.open(img)\n        else:\n            img = Image.fromarray(img)\n        img = self.transform(img)\n        logits = self.forward(img.unsqueeze(0).to(self.device))\n        return self.results_generation(logits.cpu(), [img_id], id_strip=id_strip)[0]\n\n    def batch_image_classification(self, data_path=None, det_results=None, id_strip=None):\n        \"\"\"\n        Process a batch of images for classification.\n        \"\"\"\n\n        if data_path:\n            dataset = pw_data.ImageFolder(\n                data_path,\n                transform=self.transform,\n                path_head='.'\n            )\n        elif det_results:\n            dataset = pw_data.DetectionCrops(\n                det_results,\n                transform=self.transform,\n                path_head='.'\n            )\n        else:\n            raise Exception(\"Need data for inference.\")\n\n        dataloader = DataLoader(dataset, batch_size=32, shuffle=False, \n                                pin_memory=True, num_workers=4, drop_last=False)\n        total_logits = []\n        total_paths = []\n\n        with tqdm(total=len(dataloader)) as pbar: \n            for batch in dataloader:\n                imgs, paths = batch\n                imgs = imgs.to(self.device)\n                total_logits.append(self.forward(imgs))\n                total_paths.append(paths)\n                pbar.update(1)\n\n        total_logits = torch.cat(total_logits, dim=0).cpu()\n        total_paths = np.concatenate(total_paths, axis=0)\n\n        return self.results_generation(total_logits, total_paths, id_strip=id_strip)\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetInference.batch_image_classification","title":"<code>batch_image_classification(data_path=None, det_results=None, id_strip=None)</code>","text":"<p>Process a batch of images for classification.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>def batch_image_classification(self, data_path=None, det_results=None, id_strip=None):\n    \"\"\"\n    Process a batch of images for classification.\n    \"\"\"\n\n    if data_path:\n        dataset = pw_data.ImageFolder(\n            data_path,\n            transform=self.transform,\n            path_head='.'\n        )\n    elif det_results:\n        dataset = pw_data.DetectionCrops(\n            det_results,\n            transform=self.transform,\n            path_head='.'\n        )\n    else:\n        raise Exception(\"Need data for inference.\")\n\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, \n                            pin_memory=True, num_workers=4, drop_last=False)\n    total_logits = []\n    total_paths = []\n\n    with tqdm(total=len(dataloader)) as pbar: \n        for batch in dataloader:\n            imgs, paths = batch\n            imgs = imgs.to(self.device)\n            total_logits.append(self.forward(imgs))\n            total_paths.append(paths)\n            pbar.update(1)\n\n    total_logits = torch.cat(total_logits, dim=0).cpu()\n    total_paths = np.concatenate(total_paths, axis=0)\n\n    return self.results_generation(total_logits, total_paths, id_strip=id_strip)\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.PlainResNetInference.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Process logits to produce final results. </p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits from the network.</p> required <code>img_ids</code> <code>list[str]</code> <p>List of image paths.</p> required <code>id_strip</code> <code>str</code> <p>Stripping string for better image ID saving.       </p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of dictionaries containing the results.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Process logits to produce final results. \n\n    Args:\n        logits (torch.Tensor): Logits from the network.\n        img_ids (list[str]): List of image paths.\n        id_strip (str): Stripping string for better image ID saving.       \n\n    Returns:\n        list[dict]: List of dictionaries containing the results.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/classification/resnet_base/base_classifier/#PytorchWildlife.models.classification.resnet_base.base_classifier.ResNetBackbone","title":"<code>ResNetBackbone</code>","text":"<p>               Bases: <code>ResNet</code></p> <p>Custom ResNet Backbone that extracts features from input images.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/base_classifier.py</code> <pre><code>class ResNetBackbone(ResNet):\n    \"\"\"\n    Custom ResNet Backbone that extracts features from input images.\n    \"\"\"\n    def _forward_impl(self, x):\n        # Following the ResNet structure to extract features\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        return x\n</code></pre>"},{"location":"base/models/classification/resnet_base/custom_weights/","title":"Custom Weights","text":""},{"location":"base/models/classification/resnet_base/custom_weights/#PytorchWildlife.models.classification.resnet_base.custom_weights.CustomWeights","title":"<code>CustomWeights</code>","text":"<p>               Bases: <code>PlainResNetInference</code></p> <p>Custom Weight Classifier that inherits from PlainResNetInference. This classifier can load any model that was based on the PytorchWildlife finetuning tool.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/custom_weights.py</code> <pre><code>class CustomWeights(PlainResNetInference):\n    \"\"\"\n    Custom Weight Classifier that inherits from PlainResNetInference.\n    This classifier can load any model that was based on the PytorchWildlife finetuning tool.\n    \"\"\"\n\n    # Image size for the classifier\n    IMAGE_SIZE = 224\n\n\n    def __init__(self, weights=None, class_names=None, device=\"cpu\"):\n        \"\"\"\n        Initialize the CustomWeights Classifier.\n\n        Args:\n            weights (str, optional): Path to the model weights. Defaults to None.\n            class_names (list[str]): List of class names for the classifier.\n            device (str, optional): Device for model inference. Defaults to \"cpu\".\n        \"\"\"\n        self.CLASS_NAMES = class_names\n        self.num_cls = len(self.CLASS_NAMES)\n        super(CustomWeights, self).__init__(weights=weights, device=device,\n                                                   num_cls=self.num_cls, num_layers=50, url=None)\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Generate results for classification.\n\n        Args:\n            logits (torch.Tensor): Output tensor from the model.\n            img_ids (list[str]): List of image identifiers.\n            id_strip (str): Stripping string for better image ID saving.\n\n        Returns:\n            list[dict]: List of dictionaries containing image ID, prediction, and confidence score.\n        \"\"\"\n\n        probs = torch.softmax(logits, dim=1)\n        preds = probs.argmax(dim=1)\n        confs = probs.max(dim=1)[0]\n        confidences = probs[0].tolist()\n        result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n        results = []\n        for pred, img_id, conf in zip(preds, img_ids, confs):\n            r = {\"img_id\": str(img_id).strip(id_strip)}\n            r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n            r[\"class_id\"] = pred.item()\n            r[\"confidence\"] = conf.item()\n            r[\"all_confidences\"] = result\n            results.append(r)\n\n        return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/custom_weights/#PytorchWildlife.models.classification.resnet_base.custom_weights.CustomWeights.__init__","title":"<code>__init__(weights=None, class_names=None, device='cpu')</code>","text":"<p>Initialize the CustomWeights Classifier.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>List of class names for the classifier.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> Source code in <code>PytorchWildlife/models/classification/resnet_base/custom_weights.py</code> <pre><code>def __init__(self, weights=None, class_names=None, device=\"cpu\"):\n    \"\"\"\n    Initialize the CustomWeights Classifier.\n\n    Args:\n        weights (str, optional): Path to the model weights. Defaults to None.\n        class_names (list[str]): List of class names for the classifier.\n        device (str, optional): Device for model inference. Defaults to \"cpu\".\n    \"\"\"\n    self.CLASS_NAMES = class_names\n    self.num_cls = len(self.CLASS_NAMES)\n    super(CustomWeights, self).__init__(weights=weights, device=device,\n                                               num_cls=self.num_cls, num_layers=50, url=None)\n</code></pre>"},{"location":"base/models/classification/resnet_base/custom_weights/#PytorchWildlife.models.classification.resnet_base.custom_weights.CustomWeights.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Generate results for classification.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output tensor from the model.</p> required <code>img_ids</code> <code>list[str]</code> <p>List of image identifiers.</p> required <code>id_strip</code> <code>str</code> <p>Stripping string for better image ID saving.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of dictionaries containing image ID, prediction, and confidence score.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/custom_weights.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Generate results for classification.\n\n    Args:\n        logits (torch.Tensor): Output tensor from the model.\n        img_ids (list[str]): List of image identifiers.\n        id_strip (str): Stripping string for better image ID saving.\n\n    Returns:\n        list[dict]: List of dictionaries containing image ID, prediction, and confidence score.\n    \"\"\"\n\n    probs = torch.softmax(logits, dim=1)\n    preds = probs.argmax(dim=1)\n    confs = probs.max(dim=1)[0]\n    confidences = probs[0].tolist()\n    result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n    results = []\n    for pred, img_id, conf in zip(preds, img_ids, confs):\n        r = {\"img_id\": str(img_id).strip(id_strip)}\n        r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n        r[\"class_id\"] = pred.item()\n        r[\"confidence\"] = conf.item()\n        r[\"all_confidences\"] = result\n        results.append(r)\n\n    return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/opossum/","title":"Opossum","text":""},{"location":"base/models/classification/resnet_base/opossum/#PytorchWildlife.models.classification.resnet_base.opossum.AI4GOpossum","title":"<code>AI4GOpossum</code>","text":"<p>               Bases: <code>PlainResNetInference</code></p> <p>Opossum Classifier that inherits from PlainResNetInference. This classifier is specialized for distinguishing between Opossums and Non-opossums.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/opossum.py</code> <pre><code>class AI4GOpossum(PlainResNetInference):\n    \"\"\"\n    Opossum Classifier that inherits from PlainResNetInference.\n    This classifier is specialized for distinguishing between Opossums and Non-opossums.\n    \"\"\"\n\n    # Image size for the Opossum classifier\n    IMAGE_SIZE = 224\n\n    # Class names for prediction\n    CLASS_NAMES = {\n        0: \"Non-opossum\",\n        1: \"Opossum\"\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True):\n        \"\"\"\n        Initialize the Opossum Classifier.\n\n        Args:\n            weights (str, optional): Path to the model weights. Defaults to None.\n            device (str, optional): Device for model inference. Defaults to \"cpu\".\n            pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n        \"\"\"\n\n        # If pretrained, use the provided URL to fetch the weights\n        if pretrained:\n            url = \"https://zenodo.org/records/10023414/files/OpossumClassification_v0.0.0.ckpt?download=1\"\n        else:\n            url = None\n\n        super(AI4GOpossum, self).__init__(weights=weights, device=device,\n                                          num_cls=1, num_layers=50, url=url)\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Generate results for classification.\n\n        Args:\n            logits (torch.Tensor): Output tensor from the model.\n            img_ids (list): List of image identifier.\n            id_strip (str): stiping string for better image id saving.       \n\n        Returns:\n            dict: Dictionary containing image ID, prediction, and confidence score.\n        \"\"\"\n\n        probs = torch.sigmoid(logits)\n        preds = (probs &gt; 0.5).squeeze(1).numpy().astype(int)\n\n        results = []\n        for pred, img_id, prob in zip(preds, img_ids, probs):\n            r = {\"img_id\": str(img_id).strip(id_strip)}\n            r[\"prediction\"] = self.CLASS_NAMES[pred]\n            r[\"class_id\"] = pred\n            r[\"confidence\"] = prob.item() if pred == 1 else (1 - prob.item())\n            results.append(r)\n\n        return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/opossum/#PytorchWildlife.models.classification.resnet_base.opossum.AI4GOpossum.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True)</code>","text":"<p>Initialize the Opossum Classifier.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained weights. Defaults to True.</p> <code>True</code> Source code in <code>PytorchWildlife/models/classification/resnet_base/opossum.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True):\n    \"\"\"\n    Initialize the Opossum Classifier.\n\n    Args:\n        weights (str, optional): Path to the model weights. Defaults to None.\n        device (str, optional): Device for model inference. Defaults to \"cpu\".\n        pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n    \"\"\"\n\n    # If pretrained, use the provided URL to fetch the weights\n    if pretrained:\n        url = \"https://zenodo.org/records/10023414/files/OpossumClassification_v0.0.0.ckpt?download=1\"\n    else:\n        url = None\n\n    super(AI4GOpossum, self).__init__(weights=weights, device=device,\n                                      num_cls=1, num_layers=50, url=url)\n</code></pre>"},{"location":"base/models/classification/resnet_base/opossum/#PytorchWildlife.models.classification.resnet_base.opossum.AI4GOpossum.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Generate results for classification.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output tensor from the model.</p> required <code>img_ids</code> <code>list</code> <p>List of image identifier.</p> required <code>id_strip</code> <code>str</code> <p>stiping string for better image id saving.       </p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>Dictionary containing image ID, prediction, and confidence score.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/opossum.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Generate results for classification.\n\n    Args:\n        logits (torch.Tensor): Output tensor from the model.\n        img_ids (list): List of image identifier.\n        id_strip (str): stiping string for better image id saving.       \n\n    Returns:\n        dict: Dictionary containing image ID, prediction, and confidence score.\n    \"\"\"\n\n    probs = torch.sigmoid(logits)\n    preds = (probs &gt; 0.5).squeeze(1).numpy().astype(int)\n\n    results = []\n    for pred, img_id, prob in zip(preds, img_ids, probs):\n        r = {\"img_id\": str(img_id).strip(id_strip)}\n        r[\"prediction\"] = self.CLASS_NAMES[pred]\n        r[\"class_id\"] = pred\n        r[\"confidence\"] = prob.item() if pred == 1 else (1 - prob.item())\n        results.append(r)\n\n    return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/serengeti/","title":"Serengeti","text":""},{"location":"base/models/classification/resnet_base/serengeti/#PytorchWildlife.models.classification.resnet_base.serengeti.AI4GSnapshotSerengeti","title":"<code>AI4GSnapshotSerengeti</code>","text":"<p>               Bases: <code>PlainResNetInference</code></p> <p>Snapshot Serengeti Animal Classifier that inherits from PlainResNetInference. This classifier is specialized for recognizing 9 different animals and has 1 'other' class.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/serengeti.py</code> <pre><code>class AI4GSnapshotSerengeti(PlainResNetInference):\n    \"\"\"\n    Snapshot Serengeti Animal Classifier that inherits from PlainResNetInference.\n    This classifier is specialized for recognizing 9 different animals and has 1 'other' class.\n    \"\"\"\n\n    # Image size for the Opossum classifier\n    IMAGE_SIZE = 224\n\n    # Class names for prediction\n    CLASS_NAMES = {\n        0: 'wildebeest',\n        1: 'guineafowl',\n        2: 'zebra',\n        3: 'buffalo',\n        4: 'gazellethomsons',\n        5: 'gazellegrants',\n        6: 'warthog',\n        7: 'impala',\n        8: 'hyenaspotted',\n        9: 'other'\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True):\n        \"\"\"\n        Initialize the Amazon animal Classifier.\n\n        Args:\n            weights (str, optional): Path to the model weights. Defaults to None.\n            device (str, optional): Device for model inference. Defaults to \"cpu\".\n            pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n        \"\"\"\n\n        # If pretrained, use the provided URL to fetch the weights\n        if pretrained:\n            url = \"https://zenodo.org/records/10456813/files/AI4GSnapshotSerengeti.ckpt?download=1\"\n        else:\n            url = None\n\n        super(AI4GSnapshotSerengeti, self).__init__(weights=weights, device=device,\n                                                   num_cls=10, num_layers=18, url=url)\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Generate results for classification.\n\n        Args:\n            logits (torch.Tensor): Output tensor from the model.\n            img_ids (str): Image identifier.\n            id_strip (str): stiping string for better image id saving.       \n\n        Returns:\n            dict: Dictionary containing image ID, prediction, and confidence score.\n        \"\"\"\n\n        probs = torch.softmax(logits, dim=1)\n        preds = probs.argmax(dim=1)\n        confs = probs.max(dim=1)[0]\n        confidences = probs[0].tolist()\n        result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n        results = []\n        for pred, img_id, conf in zip(preds, img_ids, confs):\n            r = {\"img_id\": str(img_id).strip(id_strip)}\n            r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n            r[\"class_id\"] = pred.item()\n            r[\"confidence\"] = conf.item()\n            r[\"all_confidences\"] = result\n            results.append(r)\n\n        return results\n</code></pre>"},{"location":"base/models/classification/resnet_base/serengeti/#PytorchWildlife.models.classification.resnet_base.serengeti.AI4GSnapshotSerengeti.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True)</code>","text":"<p>Initialize the Amazon animal Classifier.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained weights. Defaults to True.</p> <code>True</code> Source code in <code>PytorchWildlife/models/classification/resnet_base/serengeti.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True):\n    \"\"\"\n    Initialize the Amazon animal Classifier.\n\n    Args:\n        weights (str, optional): Path to the model weights. Defaults to None.\n        device (str, optional): Device for model inference. Defaults to \"cpu\".\n        pretrained (bool, optional): Whether to use pretrained weights. Defaults to True.\n    \"\"\"\n\n    # If pretrained, use the provided URL to fetch the weights\n    if pretrained:\n        url = \"https://zenodo.org/records/10456813/files/AI4GSnapshotSerengeti.ckpt?download=1\"\n    else:\n        url = None\n\n    super(AI4GSnapshotSerengeti, self).__init__(weights=weights, device=device,\n                                               num_cls=10, num_layers=18, url=url)\n</code></pre>"},{"location":"base/models/classification/resnet_base/serengeti/#PytorchWildlife.models.classification.resnet_base.serengeti.AI4GSnapshotSerengeti.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Generate results for classification.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output tensor from the model.</p> required <code>img_ids</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>stiping string for better image id saving.       </p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>Dictionary containing image ID, prediction, and confidence score.</p> Source code in <code>PytorchWildlife/models/classification/resnet_base/serengeti.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Generate results for classification.\n\n    Args:\n        logits (torch.Tensor): Output tensor from the model.\n        img_ids (str): Image identifier.\n        id_strip (str): stiping string for better image id saving.       \n\n    Returns:\n        dict: Dictionary containing image ID, prediction, and confidence score.\n    \"\"\"\n\n    probs = torch.softmax(logits, dim=1)\n    preds = probs.argmax(dim=1)\n    confs = probs.max(dim=1)[0]\n    confidences = probs[0].tolist()\n    result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n    results = []\n    for pred, img_id, conf in zip(preds, img_ids, confs):\n        r = {\"img_id\": str(img_id).strip(id_strip)}\n        r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n        r[\"class_id\"] = pred.item()\n        r[\"confidence\"] = conf.item()\n        r[\"all_confidences\"] = result\n        results.append(r)\n\n    return results\n</code></pre>"},{"location":"base/models/classification/timm_base/DFNE/","title":"DFNE","text":"<p>               Bases: <code>TIMM_BaseClassifierInference</code></p> <p>Base detector class for dinov2 classifier. This class provides utility methods for loading the model, performing single and batch image classifications, and  formatting results. Make sure the appropriate file for the model weights has been  downloaded to the \"models\" folder before running DFNE.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/DFNE.py</code> <pre><code>class DFNE(TIMM_BaseClassifierInference):\n    \"\"\"\n    Base detector class for dinov2 classifier. This class provides utility methods\n    for loading the model, performing single and batch image classifications, and \n    formatting results. Make sure the appropriate file for the model weights has been \n    downloaded to the \"models\" folder before running DFNE.\n    \"\"\"\n    BACKBONE = \"vit_large_patch14_dinov2.lvd142m\"\n    MODEL_NAME = \"dfne_weights_v1_0.pth\"\n    IMAGE_SIZE = 182\n    CLASS_NAMES = {\n                0: \"American Marten\",\n                1: \"Bird sp.\",\n                2: \"Black Bear\",\n                3: \"Bobcat\",\n                4: \"Coyote\",\n                5: \"Domestic Cat\",\n                6: \"Domestic Cow\",\n                7: \"Domestic Dog\",\n                8: \"Fisher\",\n                9: \"Gray Fox\",\n                10: \"Gray Squirrel\",\n                11: \"Human\",\n                12: \"Moose\",\n                13: \"Mouse sp.\",\n                14: \"Opossum\",\n                15: \"Raccoon\",\n                16: \"Red Fox\",\n                17: \"Red Squirrel\",\n                18: \"Skunk\",\n                19: \"Snowshoe Hare\",\n                20: \"White-tailed Deer\",\n                21: \"Wild Boar\",\n                22: \"Wild Turkey\",\n                23: \"no-species\"\n            }\n\n    def __init__(self, weights=None, device=\"cpu\", transform=None):\n        url = 'https://prod-is-usgs-sb-prod-publish.s3.amazonaws.com/67ae17fcd34e3f09c0e0f002/dfne_weights_v1_0.pth'\n        super(DFNE, self).__init__(weights=weights, device=device, url=url, transform=transform, weights_key='model_state_dict')    \n</code></pre>"},{"location":"base/models/classification/timm_base/Deepfaune/","title":"Deepfaune","text":"<p>This is a Pytorch-Wildlife loader for the Deepfaune classifier. The original Deepfaune model is available at: https://www.deepfaune.cnrs.fr/en/ Licence: CC BY-SA 4.0 Copyright CNRS 2024 simon.chamaille@cefe.cnrs.fr; vincent.miele@univ-lyon1.fr</p>"},{"location":"base/models/classification/timm_base/Deepfaune/#PytorchWildlife.models.classification.timm_base.Deepfaune.DeepfauneClassifier","title":"<code>DeepfauneClassifier</code>","text":"<p>               Bases: <code>TIMM_BaseClassifierInference</code></p> <p>Base detector class for dinov2 classifier. This class provides utility methods for loading the model, performing single and batch image classifications, and  formatting results. Make sure the appropriate file for the model weights has been  downloaded to the \"models\" folder before running DFNE.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/Deepfaune.py</code> <pre><code>class DeepfauneClassifier(TIMM_BaseClassifierInference):\n    \"\"\"\n    Base detector class for dinov2 classifier. This class provides utility methods\n    for loading the model, performing single and batch image classifications, and \n    formatting results. Make sure the appropriate file for the model weights has been \n    downloaded to the \"models\" folder before running DFNE.\n    \"\"\"\n    BACKBONE = \"vit_large_patch14_dinov2.lvd142m\"\n    MODEL_NAME = \"deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt\"\n    IMAGE_SIZE = 182\n    CLASS_NAMES={\n        'fr': ['bison', 'blaireau', 'bouquetin', 'castor', 'cerf', 'chamois', 'chat', 'chevre', 'chevreuil', 'chien', 'daim', 'ecureuil', 'elan', 'equide', 'genette', 'glouton', 'herisson', 'lagomorphe', 'loup', 'loutre', 'lynx', 'marmotte', 'micromammifere', 'mouflon', 'mouton', 'mustelide', 'oiseau', 'ours', 'ragondin', 'raton laveur', 'renard', 'renne', 'sanglier', 'vache'],\n        'en': ['bison', 'badger', 'ibex', 'beaver', 'red deer', 'chamois', 'cat', 'goat', 'roe deer', 'dog', 'fallow deer', 'squirrel', 'moose', 'equid', 'genet', 'wolverine', 'hedgehog', 'lagomorph', 'wolf', 'otter', 'lynx', 'marmot', 'micromammal', 'mouflon', 'sheep', 'mustelid', 'bird', 'bear', 'nutria', 'raccoon', 'fox', 'reindeer', 'wild boar', 'cow'],\n        'it': ['bisonte', 'tasso', 'stambecco', 'castoro', 'cervo', 'camoscio', 'gatto', 'capra', 'capriolo', 'cane', 'daino', 'scoiattolo', 'alce', 'equide', 'genetta', 'ghiottone', 'riccio', 'lagomorfo', 'lupo', 'lontra', 'lince', 'marmotta', 'micromammifero', 'muflone', 'pecora', 'mustelide', 'uccello', 'orso', 'nutria', 'procione', 'volpe', 'renna', 'cinghiale', 'mucca'],\n        'de': ['Bison', 'Dachs', 'Steinbock', 'Biber', 'Rothirsch', 'G\u00e4mse', 'Katze', 'Ziege', 'Rehwild', 'Hund', 'Damwild', 'Eichh\u00f6rnchen', 'Elch', 'Equide', 'Ginsterkatze', 'Vielfra\u00df', 'Igel', 'Lagomorpha', 'Wolf', 'Otter', 'Luchs', 'Murmeltier', 'Kleins\u00e4uger', 'Mufflon', 'Schaf', 'Marder', 'Vogel', 'B\u00e4r', 'Nutria', 'Waschb\u00e4r', 'Fuchs', 'Rentier', 'Wildschwein', 'Kuh'],\n    }\n\n\n    def __init__(self, weights=None, device=\"cpu\", transform=None, class_name_lang='en'):\n        url = 'https://pbil.univ-lyon1.fr/software/download/deepfaune/v1.3/deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt'\n        self.CLASS_NAMES = {i: c for i, c in enumerate(self.CLASS_NAMES[class_name_lang])}\n        if transform is None:\n            transform = pw_trans.Classification_Inference_Transform(target_size=self.IMAGE_SIZE, \n                                                                    interpolation=InterpolationMode.BICUBIC, \n                                                                    max_size=None,\n                                                                    antialias=None)\n        super(DeepfauneClassifier, self).__init__(weights=weights, device=device, url=url, transform=transform,\n                                                  weights_key='state_dict', weights_prefix='base_model.')\n</code></pre>"},{"location":"base/models/classification/timm_base/base_classifier/","title":"Timm Base","text":"<p>model class for loading the DFNE classifier.</p>"},{"location":"base/models/classification/timm_base/base_classifier/#PytorchWildlife.models.classification.timm_base.base_classifier.TIMM_BaseClassifierInference","title":"<code>TIMM_BaseClassifierInference</code>","text":"<p>               Bases: <code>BaseClassifierInference</code></p> <p>Base detector class for dinov2 classifier. This class provides utility methods for loading the model, performing single and batch image classifications, and  formatting results. Make sure the appropriate file for the model weights has been  downloaded to the \"models\" folder before running DFNE.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/base_classifier.py</code> <pre><code>class TIMM_BaseClassifierInference(BaseClassifierInference):\n    \"\"\"\n    Base detector class for dinov2 classifier. This class provides utility methods\n    for loading the model, performing single and batch image classifications, and \n    formatting results. Make sure the appropriate file for the model weights has been \n    downloaded to the \"models\" folder before running DFNE.\n    \"\"\"\n\n    BACKBONE = None\n    MODEL_NAME = None\n    IMAGE_SIZE = None\n\n    def __init__(self, weights=None, device=\"cpu\", url=None, transform=None,\n                 weights_key='model_state_dict', weights_prefix=''):\n        \"\"\"\n        Initialize the model.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n            weights_key (str, optional): \n                Key to fetch the model weights. Defaults to None.\n            weights_prefix (str, optional): \n                prefix of model weight keys. Defaults to None.\n        \"\"\"\n        super(TIMM_BaseClassifierInference, self).__init__()\n        self.device = device\n\n        if transform:\n            self.transform = transform\n        else:\n            self.transform = pw_trans.Classification_Inference_Transform(target_size=self.IMAGE_SIZE)\n\n        self._load_model(weights, url, weights_key, weights_prefix)\n\n    def _load_model(self, weights=None, url=None, weights_key='model_state_dict', weights_prefix=''):\n        \"\"\"\n        Load TIMM based model weights\n\n        Args:\n        weights (str, optional): \n            Path to the model weights. (defaults to None)\n        url (str, optional): \n            url to the model weights. (defaults to None)\n        \"\"\"\n\n        self.predictor = timm.create_model(\n            self.BACKBONE, \n            pretrained = False, \n            num_classes = len(self.CLASS_NAMES),\n            dynamic_img_size = True\n        )\n\n        if url:\n            if not os.path.exists(os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)):\n                os.makedirs(os.path.join(torch.hub.get_dir(), \"checkpoints\"), exist_ok=True)\n                weights = wget.download(url, out=os.path.join(torch.hub.get_dir(), \"checkpoints\"))\n            else:\n                weights = os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)\n        elif weights is None:\n            raise Exception(\"Need weights for inference.\")\n\n        checkpoint = torch.load(\n            f = weights,\n            map_location = self.device,\n            weights_only = False\n        )[weights_key]\n\n        checkpoint = OrderedDict({k.replace(\"{}\".format(weights_prefix), \"\"): checkpoint[k]\n                                    for k in checkpoint})\n\n        self.predictor.load_state_dict(checkpoint)\n        print(\"Model loaded from {}\".format(os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)))\n\n        self.predictor.to(self.device)\n        self.eval()\n\n    def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Generate results for classification.\n\n        Args:\n            logits (torch.Tensor): Output tensor from the model.\n            img_ids (list[str]): List of image identifiers.\n            id_strip (str): Stripping string for better image ID saving.\n\n        Returns:\n            list[dict]: List of dictionaries containing image ID, prediction, and confidence score.\n        \"\"\"\n\n        probs = torch.softmax(logits, dim=1)\n        preds = probs.argmax(dim=1)\n        confs = probs.max(dim=1)[0]\n        confidences = probs[0].tolist()\n        result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n        results = []\n        for pred, img_id, conf in zip(preds, img_ids, confs):\n            r = {\"img_id\": str(img_id).strip(id_strip)}\n            r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n            r[\"class_id\"] = pred.item()\n            r[\"confidence\"] = conf.item()\n            r[\"all_confidences\"] = result\n            results.append(r)\n\n        return results\n\n    def single_image_classification(self, img, img_id=None, id_strip=None):\n        \"\"\"\n        Perform classification on a single image.\n\n        Args:\n            img (str or ndarray): \n                Image path or ndarray of images.\n            img_id (str, optional): \n                Image path or identifier.\n            id_strip (str, optional):\n                Whether to strip stings in id. Defaults to None.\n\n        Returns:\n            (dict): Classification results.\n        \"\"\"\n        if type(img) == str:\n            img = Image.open(img).convert(\"RGB\")\n        else:\n            img = Image.fromarray(img)\n        img = self.transform(img)\n\n        logits = self.predictor(img.unsqueeze(0).to(self.device))\n        return self.results_generation(logits.cpu(), [img_id], id_strip=id_strip)[0]\n\n    def batch_image_classification(self, data_path=None, det_results=None, id_strip=None,\n                                   batch_size=32, num_workers=0, **kwargs):\n        \"\"\"\n        Perform classification on a batch of images.\n\n        Args:\n            data_path (str): \n                Path containing all images for inference. Defaults to None. \n            det_results (dict):\n                Dirct outputs from detectors. Defaults to None.\n            id_strip (str, optional):\n                Whether to strip stings in id. Defaults to None.\n            batch_size (int, optional):\n                Batch size for inference. Defaults to 32.\n            num_workers (int, optional):\n                Number of workers for dataloader. Defaults to 0.\n\n        Returns:\n            (dict): Classification results.\n        \"\"\"\n\n        if data_path:\n            dataset = pw_data.ImageFolder(\n                data_path,\n                transform=self.transform,\n                path_head='.'\n            )\n        elif det_results:\n            dataset = pw_data.DetectionCrops(\n                det_results,\n                transform=self.transform,\n                path_head='.'\n            )\n        else:\n            raise Exception(\"Need data for inference.\")\n\n        dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n                                shuffle=False, pin_memory=True, drop_last=False, **kwargs)\n\n        total_logits = []\n        total_paths = []\n\n        with tqdm(total=len(dataloader)) as pbar: \n            for batch in dataloader:\n                imgs, paths = batch\n                imgs = imgs.to(self.device)\n                total_logits.append(self.predictor(imgs))\n                total_paths.append(paths)\n                pbar.update(1)\n\n        total_logits = torch.cat(total_logits, dim=0).cpu()\n        total_paths = np.concatenate(total_paths, axis=0)\n\n        return self.results_generation(total_logits, total_paths, id_strip=id_strip)\n</code></pre>"},{"location":"base/models/classification/timm_base/base_classifier/#PytorchWildlife.models.classification.timm_base.base_classifier.TIMM_BaseClassifierInference.__init__","title":"<code>__init__(weights=None, device='cpu', url=None, transform=None, weights_key='model_state_dict', weights_prefix='')</code>","text":"<p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> <code>weights_key</code> <code>str</code> <p>Key to fetch the model weights. Defaults to None.</p> <code>'model_state_dict'</code> <code>weights_prefix</code> <code>str</code> <p>prefix of model weight keys. Defaults to None.</p> <code>''</code> Source code in <code>PytorchWildlife/models/classification/timm_base/base_classifier.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None, transform=None,\n             weights_key='model_state_dict', weights_prefix=''):\n    \"\"\"\n    Initialize the model.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n        weights_key (str, optional): \n            Key to fetch the model weights. Defaults to None.\n        weights_prefix (str, optional): \n            prefix of model weight keys. Defaults to None.\n    \"\"\"\n    super(TIMM_BaseClassifierInference, self).__init__()\n    self.device = device\n\n    if transform:\n        self.transform = transform\n    else:\n        self.transform = pw_trans.Classification_Inference_Transform(target_size=self.IMAGE_SIZE)\n\n    self._load_model(weights, url, weights_key, weights_prefix)\n</code></pre>"},{"location":"base/models/classification/timm_base/base_classifier/#PytorchWildlife.models.classification.timm_base.base_classifier.TIMM_BaseClassifierInference.batch_image_classification","title":"<code>batch_image_classification(data_path=None, det_results=None, id_strip=None, batch_size=32, num_workers=0, **kwargs)</code>","text":"<p>Perform classification on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path containing all images for inference. Defaults to None. </p> <code>None</code> <code>det_results</code> <code>dict</code> <p>Dirct outputs from detectors. Defaults to None.</p> <code>None</code> <code>id_strip</code> <code>str</code> <p>Whether to strip stings in id. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 32.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of workers for dataloader. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Classification results.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/base_classifier.py</code> <pre><code>def batch_image_classification(self, data_path=None, det_results=None, id_strip=None,\n                               batch_size=32, num_workers=0, **kwargs):\n    \"\"\"\n    Perform classification on a batch of images.\n\n    Args:\n        data_path (str): \n            Path containing all images for inference. Defaults to None. \n        det_results (dict):\n            Dirct outputs from detectors. Defaults to None.\n        id_strip (str, optional):\n            Whether to strip stings in id. Defaults to None.\n        batch_size (int, optional):\n            Batch size for inference. Defaults to 32.\n        num_workers (int, optional):\n            Number of workers for dataloader. Defaults to 0.\n\n    Returns:\n        (dict): Classification results.\n    \"\"\"\n\n    if data_path:\n        dataset = pw_data.ImageFolder(\n            data_path,\n            transform=self.transform,\n            path_head='.'\n        )\n    elif det_results:\n        dataset = pw_data.DetectionCrops(\n            det_results,\n            transform=self.transform,\n            path_head='.'\n        )\n    else:\n        raise Exception(\"Need data for inference.\")\n\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers,\n                            shuffle=False, pin_memory=True, drop_last=False, **kwargs)\n\n    total_logits = []\n    total_paths = []\n\n    with tqdm(total=len(dataloader)) as pbar: \n        for batch in dataloader:\n            imgs, paths = batch\n            imgs = imgs.to(self.device)\n            total_logits.append(self.predictor(imgs))\n            total_paths.append(paths)\n            pbar.update(1)\n\n    total_logits = torch.cat(total_logits, dim=0).cpu()\n    total_paths = np.concatenate(total_paths, axis=0)\n\n    return self.results_generation(total_logits, total_paths, id_strip=id_strip)\n</code></pre>"},{"location":"base/models/classification/timm_base/base_classifier/#PytorchWildlife.models.classification.timm_base.base_classifier.TIMM_BaseClassifierInference.results_generation","title":"<code>results_generation(logits, img_ids, id_strip=None)</code>","text":"<p>Generate results for classification.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output tensor from the model.</p> required <code>img_ids</code> <code>list[str]</code> <p>List of image identifiers.</p> required <code>id_strip</code> <code>str</code> <p>Stripping string for better image ID saving.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of dictionaries containing image ID, prediction, and confidence score.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/base_classifier.py</code> <pre><code>def results_generation(self, logits: torch.Tensor, img_ids: list[str], id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Generate results for classification.\n\n    Args:\n        logits (torch.Tensor): Output tensor from the model.\n        img_ids (list[str]): List of image identifiers.\n        id_strip (str): Stripping string for better image ID saving.\n\n    Returns:\n        list[dict]: List of dictionaries containing image ID, prediction, and confidence score.\n    \"\"\"\n\n    probs = torch.softmax(logits, dim=1)\n    preds = probs.argmax(dim=1)\n    confs = probs.max(dim=1)[0]\n    confidences = probs[0].tolist()\n    result = [[self.CLASS_NAMES[i], confidence] for i, confidence in enumerate(confidences)]\n\n    results = []\n    for pred, img_id, conf in zip(preds, img_ids, confs):\n        r = {\"img_id\": str(img_id).strip(id_strip)}\n        r[\"prediction\"] = self.CLASS_NAMES[pred.item()]\n        r[\"class_id\"] = pred.item()\n        r[\"confidence\"] = conf.item()\n        r[\"all_confidences\"] = result\n        results.append(r)\n\n    return results\n</code></pre>"},{"location":"base/models/classification/timm_base/base_classifier/#PytorchWildlife.models.classification.timm_base.base_classifier.TIMM_BaseClassifierInference.single_image_classification","title":"<code>single_image_classification(img, img_id=None, id_strip=None)</code>","text":"<p>Perform classification on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image path or ndarray of images.</p> required <code>img_id</code> <code>str</code> <p>Image path or identifier.</p> <code>None</code> <code>id_strip</code> <code>str</code> <p>Whether to strip stings in id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Classification results.</p> Source code in <code>PytorchWildlife/models/classification/timm_base/base_classifier.py</code> <pre><code>def single_image_classification(self, img, img_id=None, id_strip=None):\n    \"\"\"\n    Perform classification on a single image.\n\n    Args:\n        img (str or ndarray): \n            Image path or ndarray of images.\n        img_id (str, optional): \n            Image path or identifier.\n        id_strip (str, optional):\n            Whether to strip stings in id. Defaults to None.\n\n    Returns:\n        (dict): Classification results.\n    \"\"\"\n    if type(img) == str:\n        img = Image.open(img).convert(\"RGB\")\n    else:\n        img = Image.fromarray(img)\n    img = self.transform(img)\n\n    logits = self.predictor(img.unsqueeze(0).to(self.device))\n    return self.results_generation(logits.cpu(), [img_id], id_strip=id_strip)[0]\n</code></pre>"},{"location":"base/models/detection/base_detector/","title":"Base Detector","text":"<p>Base detector class.</p>"},{"location":"base/models/detection/base_detector/#PytorchWildlife.models.detection.base_detector.BaseDetector","title":"<code>BaseDetector</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base detector class. This class provides utility methods for loading the model, generating results, and performing single and batch image detections.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>class BaseDetector(nn.Module):\n    \"\"\"\n    Base detector class. This class provides utility methods for\n    loading the model, generating results, and performing single and batch image detections.\n    \"\"\"\n\n    # Placeholder class-level attributes to be defined in derived classes\n    IMAGE_SIZE = None\n    STRIDE = None\n    CLASS_NAMES = None\n    TRANSFORM = None\n\n    def __init__(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Initialize the base detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        \"\"\"\n        super(BaseDetector, self).__init__()\n        self.device = device\n\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n        pass\n\n    def results_generation(self, preds, img_id: str, id_strip: str = None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (numpy.ndarray): Model predictions.\n            img_id (str): Image identifier.\n            id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        pass\n\n    def single_image_detection(self, img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None) -&gt; dict:\n        \"\"\"\n        Perform detection on a single image.\n\n        Args:\n            img (str or ndarray): \n                Image path or ndarray of images.\n            img_size (tuple): \n                Original image size.\n            img_path (str): \n                Image path or identifier.\n            conf_thres (float, optional): \n                Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): \n                Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            dict: Detection results.\n        \"\"\"\n        pass\n\n    def batch_image_detection(self, dataloader, conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Perform detection on a batch of images.\n\n        Args:\n            dataloader (DataLoader): DataLoader containing image batches.\n            conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            list[dict]: List of detection results for all images.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"base/models/detection/base_detector/#PytorchWildlife.models.detection.base_detector.BaseDetector.__init__","title":"<code>__init__(weights=None, device='cpu', url=None)</code>","text":"<p>Initialize the base detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None):\n    \"\"\"\n    Initialize the base detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n    \"\"\"\n    super(BaseDetector, self).__init__()\n    self.device = device\n</code></pre>"},{"location":"base/models/detection/base_detector/#PytorchWildlife.models.detection.base_detector.BaseDetector.batch_image_detection","title":"<code>batch_image_detection(dataloader, conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing image batches.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of detection results for all images.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def batch_image_detection(self, dataloader, conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Perform detection on a batch of images.\n\n    Args:\n        dataloader (DataLoader): DataLoader containing image batches.\n        conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        list[dict]: List of detection results for all images.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/base_detector/#PytorchWildlife.models.detection.base_detector.BaseDetector.results_generation","title":"<code>results_generation(preds, img_id, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>ndarray</code> <p>Model predictions.</p> required <code>img_id</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def results_generation(self, preds, img_id: str, id_strip: str = None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (numpy.ndarray): Model predictions.\n        img_id (str): Image identifier.\n        id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/base_detector/#PytorchWildlife.models.detection.base_detector.BaseDetector.single_image_detection","title":"<code>single_image_detection(img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image path or ndarray of images.</p> required <code>img_size</code> <code>tuple</code> <p>Original image size.</p> <code>None</code> <code>img_path</code> <code>str</code> <p>Image path or identifier.</p> <code>None</code> <code>conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Detection results.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def single_image_detection(self, img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None) -&gt; dict:\n    \"\"\"\n    Perform detection on a single image.\n\n    Args:\n        img (str or ndarray): \n            Image path or ndarray of images.\n        img_size (tuple): \n            Original image size.\n        img_path (str): \n            Image path or identifier.\n        conf_thres (float, optional): \n            Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): \n            Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        dict: Detection results.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/herdnet/","title":"HerdNet","text":""},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.BaseDetector","title":"<code>BaseDetector</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base detector class. This class provides utility methods for loading the model, generating results, and performing single and batch image detections.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>class BaseDetector(nn.Module):\n    \"\"\"\n    Base detector class. This class provides utility methods for\n    loading the model, generating results, and performing single and batch image detections.\n    \"\"\"\n\n    # Placeholder class-level attributes to be defined in derived classes\n    IMAGE_SIZE = None\n    STRIDE = None\n    CLASS_NAMES = None\n    TRANSFORM = None\n\n    def __init__(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Initialize the base detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        \"\"\"\n        super(BaseDetector, self).__init__()\n        self.device = device\n\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n        pass\n\n    def results_generation(self, preds, img_id: str, id_strip: str = None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (numpy.ndarray): Model predictions.\n            img_id (str): Image identifier.\n            id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        pass\n\n    def single_image_detection(self, img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None) -&gt; dict:\n        \"\"\"\n        Perform detection on a single image.\n\n        Args:\n            img (str or ndarray): \n                Image path or ndarray of images.\n            img_size (tuple): \n                Original image size.\n            img_path (str): \n                Image path or identifier.\n            conf_thres (float, optional): \n                Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): \n                Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            dict: Detection results.\n        \"\"\"\n        pass\n\n    def batch_image_detection(self, dataloader, conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Perform detection on a batch of images.\n\n        Args:\n            dataloader (DataLoader): DataLoader containing image batches.\n            conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            list[dict]: List of detection results for all images.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.BaseDetector.__init__","title":"<code>__init__(weights=None, device='cpu', url=None)</code>","text":"<p>Initialize the base detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None):\n    \"\"\"\n    Initialize the base detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n    \"\"\"\n    super(BaseDetector, self).__init__()\n    self.device = device\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.BaseDetector.batch_image_detection","title":"<code>batch_image_detection(dataloader, conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>DataLoader containing image batches.</p> required <code>conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of detection results for all images.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def batch_image_detection(self, dataloader, conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Perform detection on a batch of images.\n\n    Args:\n        dataloader (DataLoader): DataLoader containing image batches.\n        conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        list[dict]: List of detection results for all images.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.BaseDetector.results_generation","title":"<code>results_generation(preds, img_id, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>ndarray</code> <p>Model predictions.</p> required <code>img_id</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def results_generation(self, preds, img_id: str, id_strip: str = None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (numpy.ndarray): Model predictions.\n        img_id (str): Image identifier.\n        id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.BaseDetector.single_image_detection","title":"<code>single_image_detection(img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image path or ndarray of images.</p> required <code>img_size</code> <code>tuple</code> <p>Original image size.</p> <code>None</code> <code>img_path</code> <code>str</code> <p>Image path or identifier.</p> <code>None</code> <code>conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Detection results.</p> Source code in <code>PytorchWildlife/models/detection/base_detector.py</code> <pre><code>def single_image_detection(self, img, img_size=None, img_path=None, conf_thres=0.2, id_strip=None) -&gt; dict:\n    \"\"\"\n    Perform detection on a single image.\n\n    Args:\n        img (str or ndarray): \n            Image path or ndarray of images.\n        img_size (tuple): \n            Original image size.\n        img_path (str): \n            Image path or identifier.\n        conf_thres (float, optional): \n            Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): \n            Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        dict: Detection results.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet","title":"<code>HerdNet</code>","text":"<p>               Bases: <code>BaseDetector</code></p> <p>HerdNet detector class. This class provides utility methods for loading the model, generating results, and performing single and batch image detections.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>class HerdNet(BaseDetector):\n    \"\"\"\n    HerdNet detector class. This class provides utility methods for\n    loading the model, generating results, and performing single and batch image detections.\n    \"\"\"\n\n    def __init__(self, weights=None, device=\"cpu\", version='general' ,url=\"https://zenodo.org/records/13899852/files/20220413_HerdNet_General_dataset_2022.pth?download=1\", transform=None):\n        \"\"\"\n        Initialize the HerdNet detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            version (str, optional):\n                Version name based on what dataset the model is trained on. It should be either 'general' or 'ennedi'. Defaults to 'general'.\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n            transform (torchvision.transforms.Compose, optional):\n                Image transformation for inference. Defaults to None.\n        \"\"\"\n        super(HerdNet, self).__init__(weights=weights, device=device, url=url)\n        # Assert that the dataset is either 'general' or 'ennedi'\n        version = version.lower()\n        assert version in ['general', 'ennedi'], \"Dataset should be either 'general' or 'ennedi'\"\n        if version == 'ennedi':\n            url = \"https://zenodo.org/records/13914287/files/20220329_HerdNet_Ennedi_dataset_2023.pth?download=1\"\n        self._load_model(weights, device, url)\n\n        self.stitcher = HerdNetStitcher( # This module enables patch-based inference\n            model = self.model,\n            size = (512,512),\n            overlap = 160,\n            down_ratio = 2,\n            up = True, \n            reduction = 'mean',\n            device_name = device\n            )\n\n        self.lmds_kwargs: dict = {'kernel_size': (3, 3), 'adapt_ts': 0.2, 'neg_ts': 0.1}\n        self.lmds = HerdNetLMDS(up=False, **self.lmds_kwargs) # Local Maxima Detection Strategy\n\n        if not transform:\n            self.transforms = transforms.Compose([\n                ResizeIfSmaller(512),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=self.img_mean, std=self.img_std)  \n                ]) \n        else:\n            self.transforms = transform\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load the HerdNet model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n        if weights:\n            checkpoint = torch.load(weights, map_location=torch.device(device))\n        elif url:\n            filename = url.split('/')[-1][:-11] # Splitting the URL to get the filename and removing the '?download=1' part\n            if not os.path.exists(os.path.join(torch.hub.get_dir(), \"checkpoints\", filename)):\n                os.makedirs(os.path.join(torch.hub.get_dir(), \"checkpoints\"), exist_ok=True)\n                weights = wget.download(url, out=os.path.join(torch.hub.get_dir(), \"checkpoints\"))\n            else:\n                weights = os.path.join(torch.hub.get_dir(), \"checkpoints\", filename)\n            checkpoint = torch.load(weights, map_location=torch.device(device))\n        else:\n            raise Exception(\"Need weights for inference.\")\n\n        # Load the class names and other metadata from the checkpoint\n        self.CLASS_NAMES = checkpoint[\"classes\"]\n        self.num_classes = len(self.CLASS_NAMES) + 1\n        self.img_mean = checkpoint['mean']\n        self.img_std = checkpoint['std']\n\n        # Load the model architecture\n        self.model = HerdNetArch(num_classes=self.num_classes, pretrained=False)\n\n        # Load checkpoint into model\n        state_dict = checkpoint['model_state_dict']  \n        # Remove 'model.' prefix from the state_dict keys if the key starts with 'model.'\n        new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items() if k.startswith('model.')}\n        # Load the new state_dict \n        self.model.load_state_dict(new_state_dict, strict=True)\n\n        print(f\"Model loaded from {weights}\")\n\n    def results_generation(self, preds: np.ndarray, img: np.ndarray = None, img_id: str = None, id_strip: str = None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (numpy.ndarray): Model predictions.\n            img (numpy.ndarray, optional): Image for inference. Defaults to None.\n            img_id (str, optional): Image identifier. Defaults to None.\n            id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        assert img is not None or img_id is not None, \"Either img or img_id should be provided.\"\n        if img_id is not None:\n            img_id = str(img_id).strip(id_strip) if id_strip else str(img_id)\n            results = {\"img_id\": img_id}\n        elif img is not None:\n            results = {\"img\": img}\n\n        results[\"detections\"] = sv.Detections(\n            xyxy=preds[:, :4],\n            confidence=preds[:, 4],\n            class_id=preds[:, 5].astype(int)\n        )\n        results[\"labels\"] = [\n            f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"\n            for confidence, class_id in zip(results[\"detections\"].confidence, results[\"detections\"].class_id)\n        ]\n        return results\n\n    def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, clf_conf_thres=0.2, id_strip=None) -&gt; dict:\n        \"\"\"\n        Perform detection on a single image.\n\n        Args:\n            img (str or np.ndarray): \n                Image for inference.\n            img_path (str, optional): \n                Path to the image. Defaults to None.\n            det_conf_thres (float, optional):\n                Confidence threshold for detections. Defaults to 0.2.\n            clf_conf_thres (float, optional):\n                Confidence threshold for classification. Defaults to 0.2.\n            id_strip (str, optional): \n                Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            dict: Detection results for the image.\n        \"\"\"\n        if isinstance(img, str):  \n            img_path = img_path or img  \n            img = np.array(Image.open(img_path).convert(\"RGB\"))  \n        if self.transforms:  \n            img_tensor = self.transforms(img)\n\n        preds = self.stitcher(img_tensor)  \n        heatmap, clsmap = preds[:,:1,:,:], preds[:,1:,:,:]  \n        counts, locs, labels, scores, dscores = self.lmds((heatmap, clsmap))\n        preds_array = self.process_lmds_results(counts, locs, labels, scores, dscores, det_conf_thres, clf_conf_thres)\n        if img_path:\n            results_dict = self.results_generation(preds_array, img_id=img_path, id_strip=id_strip)\n        else:\n            results_dict = self.results_generation(preds_array, img=img)\n        return results_dict\n\n    def batch_image_detection(self, data_path: str, det_conf_thres: float = 0.2, clf_conf_thres: float = 0.2, batch_size: int = 1, id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Perform detection on a batch of images.\n\n        Args:\n            data_path (str): Path containing all images for inference.\n            det_conf_thres (float, optional): Confidence threshold for detections. Defaults to 0.2.\n            clf_conf_thres (float, optional): Confidence threshold for classification. Defaults to 0.2.\n            batch_size (int, optional): Batch size for inference. Defaults to 1.\n            id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            list[dict]: List of detection results for all images.\n        \"\"\"\n        dataset = pw_data.DetectionImageFolder(\n            data_path,\n            transform=self.transforms\n        )\n        # Creating a Dataloader for batching and parallel processing of the images\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                            pin_memory=True, num_workers=0, drop_last=False) # TODO: discuss. why is num_workers 0?\n\n        results = []\n\n        with tqdm(total=len(loader)) as pbar:\n            for batch_index, (imgs, paths, sizes) in enumerate(loader):\n                imgs = imgs.to(self.device)\n                predictions = self.stitcher(imgs[0]).detach().cpu()\n                heatmap, clsmap = predictions[:,:1,:,:], predictions[:,1:,:,:]\n                counts, locs, labels, scores, dscores = self.lmds((heatmap, clsmap))\n                preds_array = self.process_lmds_results(counts, locs, labels, scores, dscores, det_conf_thres, clf_conf_thres) \n                results_dict = self.results_generation(preds_array, img_id=paths[0], id_strip=id_strip)\n                pbar.update(1)\n                sizes = sizes.numpy()\n                normalized_coords = [[x1 / sizes[0][0], y1 / sizes[0][1], x2 / sizes[0][0], y2 / sizes[0][1]] for x1, y1, x2, y2 in preds_array[:, :4]] # TODO: Check if this is correct due to xy swapping \n                results_dict['normalized_coords'] = normalized_coords\n                results.append(results_dict)\n        return results\n\n    def process_lmds_results(self, counts: list, locs: list, labels: list, scores: list, dscores: list, det_conf_thres: float = 0.2, clf_conf_thres: float = 0.2) -&gt; np.ndarray:\n        \"\"\"\n        Process the results from the Local Maxima Detection Strategy.\n\n        Args:\n            counts (list): Number of detections for each species.\n            locs (list): Locations of the detections.\n            labels (list): Labels of the detections.\n            scores (list): Scores of the detections.\n            dscores (list): Detection scores.\n            det_conf_thres (float, optional): Confidence threshold for detections. Defaults to 0.2.\n            clf_conf_thres (float, optional): Confidence threshold for classification. Defaults to 0.2.\n\n        Returns:\n            numpy.ndarray: Processed detection results.\n        \"\"\"\n        # Flatten the lists since we know its a single image \n        counts = counts[0]  \n        locs = locs[0]  \n        labels = labels[0]  \n        scores = scores[0]\n        dscores = dscores[0]  \n\n        # Calculate the total number of detections  \n        total_detections = sum(counts)  \n\n        # Pre-allocate based on total possible detections  \n        preds_array = np.empty((total_detections, 6)) #xyxy, confidence, class_id format\n        detection_idx = 0\n        valid_detections_idx = 0 # Index for valid detections after applying the confidence threshold\n        # Loop through each species  \n        for specie_idx in range(len(counts)):  \n            count = counts[specie_idx]  \n            if count == 0:  \n                continue  \n\n            # Get the detections for this species  \n            species_locs = np.array(locs[detection_idx : detection_idx + count])\n            species_locs[:, [0, 1]] = species_locs[:, [1, 0]] # Swap x and y in species_locs\n            species_scores = np.array(scores[detection_idx : detection_idx + count])\n            species_dscores = np.array(dscores[detection_idx : detection_idx + count])\n            species_labels = np.array(labels[detection_idx : detection_idx + count])\n\n            # Apply the confidence threshold\n            valid_detections_by_clf_score = species_scores &gt; clf_conf_thres\n            valid_detections_by_det_score = species_dscores &gt; det_conf_thres\n            valid_detections = np.logical_and(valid_detections_by_clf_score, valid_detections_by_det_score)\n            valid_detections_count = np.sum(valid_detections)\n            valid_detections_idx += valid_detections_count\n            # Fill the preds_array with the valid detections\n            if valid_detections_count &gt; 0:\n                preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, :2] = species_locs[valid_detections] - 1\n                preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 2:4] = species_locs[valid_detections] + 1\n                preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 4] = species_scores[valid_detections]\n                preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 5] = species_labels[valid_detections]\n\n            detection_idx += count # Move to the next species \n\n        preds_array = preds_array[:valid_detections_idx] # Remove the empty rows\n\n        return preds_array\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            input (torch.Tensor): \n                Input tensor for the model.\n\n        Returns:\n            torch.Tensor: Model output.\n        \"\"\"\n        # Call the forward method of the model in evaluation mode\n        self.model.eval()\n        return self.model(input)\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.__init__","title":"<code>__init__(weights=None, device='cpu', version='general', url='https://zenodo.org/records/13899852/files/20220413_HerdNet_General_dataset_2022.pth?download=1', transform=None)</code>","text":"<p>Initialize the HerdNet detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>version</code> <code>str</code> <p>Version name based on what dataset the model is trained on. It should be either 'general' or 'ennedi'. Defaults to 'general'.</p> <code>'general'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>'https://zenodo.org/records/13899852/files/20220413_HerdNet_General_dataset_2022.pth?download=1'</code> <code>transform</code> <code>Compose</code> <p>Image transformation for inference. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", version='general' ,url=\"https://zenodo.org/records/13899852/files/20220413_HerdNet_General_dataset_2022.pth?download=1\", transform=None):\n    \"\"\"\n    Initialize the HerdNet detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        version (str, optional):\n            Version name based on what dataset the model is trained on. It should be either 'general' or 'ennedi'. Defaults to 'general'.\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n        transform (torchvision.transforms.Compose, optional):\n            Image transformation for inference. Defaults to None.\n    \"\"\"\n    super(HerdNet, self).__init__(weights=weights, device=device, url=url)\n    # Assert that the dataset is either 'general' or 'ennedi'\n    version = version.lower()\n    assert version in ['general', 'ennedi'], \"Dataset should be either 'general' or 'ennedi'\"\n    if version == 'ennedi':\n        url = \"https://zenodo.org/records/13914287/files/20220329_HerdNet_Ennedi_dataset_2023.pth?download=1\"\n    self._load_model(weights, device, url)\n\n    self.stitcher = HerdNetStitcher( # This module enables patch-based inference\n        model = self.model,\n        size = (512,512),\n        overlap = 160,\n        down_ratio = 2,\n        up = True, \n        reduction = 'mean',\n        device_name = device\n        )\n\n    self.lmds_kwargs: dict = {'kernel_size': (3, 3), 'adapt_ts': 0.2, 'neg_ts': 0.1}\n    self.lmds = HerdNetLMDS(up=False, **self.lmds_kwargs) # Local Maxima Detection Strategy\n\n    if not transform:\n        self.transforms = transforms.Compose([\n            ResizeIfSmaller(512),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=self.img_mean, std=self.img_std)  \n            ]) \n    else:\n        self.transforms = transform\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.batch_image_detection","title":"<code>batch_image_detection(data_path, det_conf_thres=0.2, clf_conf_thres=0.2, batch_size=1, id_strip=None)</code>","text":"<p>Perform detection on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path containing all images for inference.</p> required <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.2.</p> <code>0.2</code> <code>clf_conf_thres</code> <code>float</code> <p>Confidence threshold for classification. Defaults to 0.2.</p> <code>0.2</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 1.</p> <code>1</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of detection results for all images.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def batch_image_detection(self, data_path: str, det_conf_thres: float = 0.2, clf_conf_thres: float = 0.2, batch_size: int = 1, id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Perform detection on a batch of images.\n\n    Args:\n        data_path (str): Path containing all images for inference.\n        det_conf_thres (float, optional): Confidence threshold for detections. Defaults to 0.2.\n        clf_conf_thres (float, optional): Confidence threshold for classification. Defaults to 0.2.\n        batch_size (int, optional): Batch size for inference. Defaults to 1.\n        id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        list[dict]: List of detection results for all images.\n    \"\"\"\n    dataset = pw_data.DetectionImageFolder(\n        data_path,\n        transform=self.transforms\n    )\n    # Creating a Dataloader for batching and parallel processing of the images\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                        pin_memory=True, num_workers=0, drop_last=False) # TODO: discuss. why is num_workers 0?\n\n    results = []\n\n    with tqdm(total=len(loader)) as pbar:\n        for batch_index, (imgs, paths, sizes) in enumerate(loader):\n            imgs = imgs.to(self.device)\n            predictions = self.stitcher(imgs[0]).detach().cpu()\n            heatmap, clsmap = predictions[:,:1,:,:], predictions[:,1:,:,:]\n            counts, locs, labels, scores, dscores = self.lmds((heatmap, clsmap))\n            preds_array = self.process_lmds_results(counts, locs, labels, scores, dscores, det_conf_thres, clf_conf_thres) \n            results_dict = self.results_generation(preds_array, img_id=paths[0], id_strip=id_strip)\n            pbar.update(1)\n            sizes = sizes.numpy()\n            normalized_coords = [[x1 / sizes[0][0], y1 / sizes[0][1], x2 / sizes[0][0], y2 / sizes[0][1]] for x1, y1, x2, y2 in preds_array[:, :4]] # TODO: Check if this is correct due to xy swapping \n            results_dict['normalized_coords'] = normalized_coords\n            results.append(results_dict)\n    return results\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor for the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Model output.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        input (torch.Tensor): \n            Input tensor for the model.\n\n    Returns:\n        torch.Tensor: Model output.\n    \"\"\"\n    # Call the forward method of the model in evaluation mode\n    self.model.eval()\n    return self.model(input)\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.process_lmds_results","title":"<code>process_lmds_results(counts, locs, labels, scores, dscores, det_conf_thres=0.2, clf_conf_thres=0.2)</code>","text":"<p>Process the results from the Local Maxima Detection Strategy.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>list</code> <p>Number of detections for each species.</p> required <code>locs</code> <code>list</code> <p>Locations of the detections.</p> required <code>labels</code> <code>list</code> <p>Labels of the detections.</p> required <code>scores</code> <code>list</code> <p>Scores of the detections.</p> required <code>dscores</code> <code>list</code> <p>Detection scores.</p> required <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.2.</p> <code>0.2</code> <code>clf_conf_thres</code> <code>float</code> <p>Confidence threshold for classification. Defaults to 0.2.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed detection results.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def process_lmds_results(self, counts: list, locs: list, labels: list, scores: list, dscores: list, det_conf_thres: float = 0.2, clf_conf_thres: float = 0.2) -&gt; np.ndarray:\n    \"\"\"\n    Process the results from the Local Maxima Detection Strategy.\n\n    Args:\n        counts (list): Number of detections for each species.\n        locs (list): Locations of the detections.\n        labels (list): Labels of the detections.\n        scores (list): Scores of the detections.\n        dscores (list): Detection scores.\n        det_conf_thres (float, optional): Confidence threshold for detections. Defaults to 0.2.\n        clf_conf_thres (float, optional): Confidence threshold for classification. Defaults to 0.2.\n\n    Returns:\n        numpy.ndarray: Processed detection results.\n    \"\"\"\n    # Flatten the lists since we know its a single image \n    counts = counts[0]  \n    locs = locs[0]  \n    labels = labels[0]  \n    scores = scores[0]\n    dscores = dscores[0]  \n\n    # Calculate the total number of detections  \n    total_detections = sum(counts)  \n\n    # Pre-allocate based on total possible detections  \n    preds_array = np.empty((total_detections, 6)) #xyxy, confidence, class_id format\n    detection_idx = 0\n    valid_detections_idx = 0 # Index for valid detections after applying the confidence threshold\n    # Loop through each species  \n    for specie_idx in range(len(counts)):  \n        count = counts[specie_idx]  \n        if count == 0:  \n            continue  \n\n        # Get the detections for this species  \n        species_locs = np.array(locs[detection_idx : detection_idx + count])\n        species_locs[:, [0, 1]] = species_locs[:, [1, 0]] # Swap x and y in species_locs\n        species_scores = np.array(scores[detection_idx : detection_idx + count])\n        species_dscores = np.array(dscores[detection_idx : detection_idx + count])\n        species_labels = np.array(labels[detection_idx : detection_idx + count])\n\n        # Apply the confidence threshold\n        valid_detections_by_clf_score = species_scores &gt; clf_conf_thres\n        valid_detections_by_det_score = species_dscores &gt; det_conf_thres\n        valid_detections = np.logical_and(valid_detections_by_clf_score, valid_detections_by_det_score)\n        valid_detections_count = np.sum(valid_detections)\n        valid_detections_idx += valid_detections_count\n        # Fill the preds_array with the valid detections\n        if valid_detections_count &gt; 0:\n            preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, :2] = species_locs[valid_detections] - 1\n            preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 2:4] = species_locs[valid_detections] + 1\n            preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 4] = species_scores[valid_detections]\n            preds_array[valid_detections_idx - valid_detections_count : valid_detections_idx, 5] = species_labels[valid_detections]\n\n        detection_idx += count # Move to the next species \n\n    preds_array = preds_array[:valid_detections_idx] # Remove the empty rows\n\n    return preds_array\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.results_generation","title":"<code>results_generation(preds, img=None, img_id=None, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>ndarray</code> <p>Model predictions.</p> required <code>img</code> <code>ndarray</code> <p>Image for inference. Defaults to None.</p> <code>None</code> <code>img_id</code> <code>str</code> <p>Image identifier. Defaults to None.</p> <code>None</code> <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def results_generation(self, preds: np.ndarray, img: np.ndarray = None, img_id: str = None, id_strip: str = None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (numpy.ndarray): Model predictions.\n        img (numpy.ndarray, optional): Image for inference. Defaults to None.\n        img_id (str, optional): Image identifier. Defaults to None.\n        id_strip (str, optional): Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    assert img is not None or img_id is not None, \"Either img or img_id should be provided.\"\n    if img_id is not None:\n        img_id = str(img_id).strip(id_strip) if id_strip else str(img_id)\n        results = {\"img_id\": img_id}\n    elif img is not None:\n        results = {\"img\": img}\n\n    results[\"detections\"] = sv.Detections(\n        xyxy=preds[:, :4],\n        confidence=preds[:, 4],\n        class_id=preds[:, 5].astype(int)\n    )\n    results[\"labels\"] = [\n        f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"\n        for confidence, class_id in zip(results[\"detections\"].confidence, results[\"detections\"].class_id)\n    ]\n    return results\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNet.single_image_detection","title":"<code>single_image_detection(img, img_path=None, det_conf_thres=0.2, clf_conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image for inference.</p> required <code>img_path</code> <code>str</code> <p>Path to the image. Defaults to None.</p> <code>None</code> <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.2.</p> <code>0.2</code> <code>clf_conf_thres</code> <code>float</code> <p>Confidence threshold for classification. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Detection results for the image.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/herdnet.py</code> <pre><code>def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, clf_conf_thres=0.2, id_strip=None) -&gt; dict:\n    \"\"\"\n    Perform detection on a single image.\n\n    Args:\n        img (str or np.ndarray): \n            Image for inference.\n        img_path (str, optional): \n            Path to the image. Defaults to None.\n        det_conf_thres (float, optional):\n            Confidence threshold for detections. Defaults to 0.2.\n        clf_conf_thres (float, optional):\n            Confidence threshold for classification. Defaults to 0.2.\n        id_strip (str, optional): \n            Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        dict: Detection results for the image.\n    \"\"\"\n    if isinstance(img, str):  \n        img_path = img_path or img  \n        img = np.array(Image.open(img_path).convert(\"RGB\"))  \n    if self.transforms:  \n        img_tensor = self.transforms(img)\n\n    preds = self.stitcher(img_tensor)  \n    heatmap, clsmap = preds[:,:1,:,:], preds[:,1:,:,:]  \n    counts, locs, labels, scores, dscores = self.lmds((heatmap, clsmap))\n    preds_array = self.process_lmds_results(counts, locs, labels, scores, dscores, det_conf_thres, clf_conf_thres)\n    if img_path:\n        results_dict = self.results_generation(preds_array, img_id=img_path, id_strip=id_strip)\n    else:\n        results_dict = self.results_generation(preds_array, img=img)\n    return results_dict\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetArch","title":"<code>HerdNetArch</code>","text":"<p>               Bases: <code>Module</code></p> <p>HerdNet architecture</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>class HerdNet(nn.Module):\n    ''' HerdNet architecture '''\n\n    def __init__(\n        self,\n        num_layers: int = 34,\n        num_classes: int = 2,\n        pretrained: bool = True, \n        down_ratio: Optional[int] = 2, \n        head_conv: int = 64\n        ):\n        '''\n        Args:\n            num_layers (int, optional): number of layers of DLA. Defaults to 34.\n            num_classes (int, optional): number of output classes, background included. \n                Defaults to 2.\n            pretrained (bool, optional): set False to disable pretrained DLA encoder parameters\n                from ImageNet. Defaults to True.\n            down_ratio (int, optional): downsample ratio. Possible values are 1, 2, 4, 8, or 16. \n                Set to 1 to get output of the same size as input (i.e. no downsample).\n                Defaults to 2.\n            head_conv (int, optional): number of supplementary convolutional layers at the end \n                of decoder. Defaults to 64.\n        '''\n\n        super(HerdNet, self).__init__()\n\n        assert down_ratio in [1, 2, 4, 8, 16], \\\n            f'Downsample ratio possible values are 1, 2, 4, 8 or 16, got {down_ratio}'\n\n        base_name = 'dla{}'.format(num_layers)\n\n        self.down_ratio = down_ratio\n        self.num_classes = num_classes\n        self.head_conv = head_conv\n\n        self.first_level = int(np.log2(down_ratio))\n\n        # backbone\n        base = dla_modules.__dict__[base_name](pretrained=pretrained, return_levels=True)\n        setattr(self, 'base_0', base)\n        setattr(self, 'channels_0', base.channels)\n\n        channels = self.channels_0\n\n        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n        self.dla_up = dla_modules.DLAUp(channels[self.first_level:], scales=scales)\n        # self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])\n\n        # bottleneck conv\n        self.bottleneck_conv = nn.Conv2d(\n            channels[-1], channels[-1], \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n        )\n\n        # localization head\n        self.loc_head = nn.Sequential(\n            nn.Conv2d(channels[self.first_level], head_conv,\n            kernel_size=3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                head_conv, 1, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                ),\n            nn.Sigmoid()\n            )\n\n        self.loc_head[-2].bias.data.fill_(0.00)\n\n        # classification head\n        self.cls_head = nn.Sequential(\n            nn.Conv2d(channels[-1], head_conv,\n            kernel_size=3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                head_conv, self.num_classes, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                )\n            )\n\n        self.cls_head[-1].bias.data.fill_(0.00)\n\n    def forward(self, input: torch.Tensor):\n\n        encode = self.base_0(input)    \n        bottleneck = self.bottleneck_conv(encode[-1])\n        encode[-1] = bottleneck\n\n        decode_hm = self.dla_up(encode[self.first_level:])\n        # decode_cls = self.cls_dla_up(encode[-3:])\n\n        heatmap = self.loc_head(decode_hm)\n        clsmap = self.cls_head(bottleneck)\n        # clsmap = self.cls_head(decode_cls)\n\n        return heatmap, clsmap\n\n    def freeze(self, layers: list) -&gt; None:\n        ''' Freeze all layers mentioned in the input list '''\n        for layer in layers:\n            self._freeze_layer(layer)\n\n    def _freeze_layer(self, layer_name: str) -&gt; None:\n        for param in getattr(self, layer_name).parameters():\n            param.requires_grad = False\n\n    def reshape_classes(self, num_classes: int) -&gt; None:\n        ''' Reshape architecture according to a new number of classes.\n\n        Arg:\n            num_classes (int): new number of classes\n        '''\n\n        self.cls_head[-1] = nn.Conv2d(\n                self.head_conv, num_classes, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                )\n\n        self.cls_head[-1].bias.data.fill_(0.00)\n\n        self.num_classes = num_classes\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetArch.__init__","title":"<code>__init__(num_layers=34, num_classes=2, pretrained=True, down_ratio=2, head_conv=64)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>number of layers of DLA. Defaults to 34.</p> <code>34</code> <code>num_classes</code> <code>int</code> <p>number of output classes, background included.  Defaults to 2.</p> <code>2</code> <code>pretrained</code> <code>bool</code> <p>set False to disable pretrained DLA encoder parameters from ImageNet. Defaults to True.</p> <code>True</code> <code>down_ratio</code> <code>int</code> <p>downsample ratio. Possible values are 1, 2, 4, 8, or 16.  Set to 1 to get output of the same size as input (i.e. no downsample). Defaults to 2.</p> <code>2</code> <code>head_conv</code> <code>int</code> <p>number of supplementary convolutional layers at the end  of decoder. Defaults to 64.</p> <code>64</code> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def __init__(\n    self,\n    num_layers: int = 34,\n    num_classes: int = 2,\n    pretrained: bool = True, \n    down_ratio: Optional[int] = 2, \n    head_conv: int = 64\n    ):\n    '''\n    Args:\n        num_layers (int, optional): number of layers of DLA. Defaults to 34.\n        num_classes (int, optional): number of output classes, background included. \n            Defaults to 2.\n        pretrained (bool, optional): set False to disable pretrained DLA encoder parameters\n            from ImageNet. Defaults to True.\n        down_ratio (int, optional): downsample ratio. Possible values are 1, 2, 4, 8, or 16. \n            Set to 1 to get output of the same size as input (i.e. no downsample).\n            Defaults to 2.\n        head_conv (int, optional): number of supplementary convolutional layers at the end \n            of decoder. Defaults to 64.\n    '''\n\n    super(HerdNet, self).__init__()\n\n    assert down_ratio in [1, 2, 4, 8, 16], \\\n        f'Downsample ratio possible values are 1, 2, 4, 8 or 16, got {down_ratio}'\n\n    base_name = 'dla{}'.format(num_layers)\n\n    self.down_ratio = down_ratio\n    self.num_classes = num_classes\n    self.head_conv = head_conv\n\n    self.first_level = int(np.log2(down_ratio))\n\n    # backbone\n    base = dla_modules.__dict__[base_name](pretrained=pretrained, return_levels=True)\n    setattr(self, 'base_0', base)\n    setattr(self, 'channels_0', base.channels)\n\n    channels = self.channels_0\n\n    scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n    self.dla_up = dla_modules.DLAUp(channels[self.first_level:], scales=scales)\n    # self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])\n\n    # bottleneck conv\n    self.bottleneck_conv = nn.Conv2d(\n        channels[-1], channels[-1], \n        kernel_size=1, stride=1, \n        padding=0, bias=True\n    )\n\n    # localization head\n    self.loc_head = nn.Sequential(\n        nn.Conv2d(channels[self.first_level], head_conv,\n        kernel_size=3, padding=1, bias=True),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(\n            head_conv, 1, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            ),\n        nn.Sigmoid()\n        )\n\n    self.loc_head[-2].bias.data.fill_(0.00)\n\n    # classification head\n    self.cls_head = nn.Sequential(\n        nn.Conv2d(channels[-1], head_conv,\n        kernel_size=3, padding=1, bias=True),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(\n            head_conv, self.num_classes, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            )\n        )\n\n    self.cls_head[-1].bias.data.fill_(0.00)\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetArch.freeze","title":"<code>freeze(layers)</code>","text":"<p>Freeze all layers mentioned in the input list</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def freeze(self, layers: list) -&gt; None:\n    ''' Freeze all layers mentioned in the input list '''\n    for layer in layers:\n        self._freeze_layer(layer)\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetArch.reshape_classes","title":"<code>reshape_classes(num_classes)</code>","text":"<p>Reshape architecture according to a new number of classes.</p> Arg <p>num_classes (int): new number of classes</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def reshape_classes(self, num_classes: int) -&gt; None:\n    ''' Reshape architecture according to a new number of classes.\n\n    Arg:\n        num_classes (int): new number of classes\n    '''\n\n    self.cls_head[-1] = nn.Conv2d(\n            self.head_conv, num_classes, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            )\n\n    self.cls_head[-1].bias.data.fill_(0.00)\n\n    self.num_classes = num_classes\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetLMDS","title":"<code>HerdNetLMDS</code>","text":"<p>               Bases: <code>LMDS</code></p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>class HerdNetLMDS(LMDS):\n\n    def __init__(\n        self, \n        up: bool = True, \n        kernel_size: tuple = (3,3), \n        adapt_ts: float = 0.3, \n        neg_ts: float = 0.1\n        ) -&gt; None:\n        '''\n        Args:\n            up (bool, optional): set to False to disable class maps upsampling.\n                Defaults to True.\n            kernel_size (tuple, optional): size of the kernel used to select local\n                maxima. Defaults to (3,3) (as in the paper).\n            adapt_ts (float, optional): adaptive threshold to select final points\n                from candidates. Defaults to 0.3.\n            neg_ts (float, optional): negative sample threshold used to define if \n                an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n        '''\n\n        super().__init__(kernel_size=kernel_size, adapt_ts=adapt_ts, neg_ts=neg_ts)\n\n        self.up = up\n\n    def __call__(self, outputs: List[torch.Tensor]) -&gt; Tuple[list, list, list, list, list]:\n        \"\"\"\n        Args:\n            outputs (List[torch.Tensor]): Outputs of HerdNet, i.e., 2 tensors:\n                - heatmap: [B,1,H,W],\n                - class map: [B,C,H/16,W/16].\n\n        Returns:\n            Tuple[list, list, list, list, list]:\n                Counts, locations, labels, class scores, and detection scores per batch.\n        \"\"\"\n\n        heatmap, clsmap = outputs\n\n        # upsample class map\n        if self.up:\n            scale_factor = 16\n            clsmap = F.interpolate(clsmap, scale_factor=scale_factor, mode='nearest')\n\n        # softmax\n        cls_scores = torch.softmax(clsmap, dim=1)[:,1:,:,:]\n\n        # cat to heatmap\n        outmaps = torch.cat([heatmap, cls_scores], dim=1)\n\n        # LMDS\n        batch_size, channels = outmaps.shape[:2]\n\n        b_counts, b_labels, b_scores, b_locs, b_dscores = [], [], [], [], []\n        for b in range(batch_size):\n\n            _, locs, _ = self._lmds(heatmap[b][0])\n\n            cls_idx = torch.argmax(clsmap[b,1:,:,:], dim=0)\n            classes = torch.add(cls_idx, 1)\n\n            h_idx = torch.Tensor([l[0] for l in locs]).long()\n            w_idx = torch.Tensor([l[1] for l in locs]).long()\n            labels = classes[h_idx, w_idx].long().tolist()\n\n            chan_idx = cls_idx[h_idx, w_idx].long().tolist()\n            scores = cls_scores[b, chan_idx, h_idx, w_idx].float().tolist()\n\n            dscores = heatmap[b, 0, h_idx, w_idx].float().tolist()\n\n            counts = [labels.count(i) for i in range(1, channels)]\n\n            b_labels.append(labels)\n            b_scores.append(scores)\n            b_locs.append(locs)\n            b_counts.append(counts)\n            b_dscores.append(dscores)\n\n        return b_counts, b_locs, b_labels, b_scores, b_dscores\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetLMDS.__call__","title":"<code>__call__(outputs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>List[Tensor]</code> <p>Outputs of HerdNet, i.e., 2 tensors: - heatmap: [B,1,H,W], - class map: [B,C,H/16,W/16].</p> required <p>Returns:</p> Type Description <code>Tuple[list, list, list, list, list]</code> <p>Tuple[list, list, list, list, list]: Counts, locations, labels, class scores, and detection scores per batch.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __call__(self, outputs: List[torch.Tensor]) -&gt; Tuple[list, list, list, list, list]:\n    \"\"\"\n    Args:\n        outputs (List[torch.Tensor]): Outputs of HerdNet, i.e., 2 tensors:\n            - heatmap: [B,1,H,W],\n            - class map: [B,C,H/16,W/16].\n\n    Returns:\n        Tuple[list, list, list, list, list]:\n            Counts, locations, labels, class scores, and detection scores per batch.\n    \"\"\"\n\n    heatmap, clsmap = outputs\n\n    # upsample class map\n    if self.up:\n        scale_factor = 16\n        clsmap = F.interpolate(clsmap, scale_factor=scale_factor, mode='nearest')\n\n    # softmax\n    cls_scores = torch.softmax(clsmap, dim=1)[:,1:,:,:]\n\n    # cat to heatmap\n    outmaps = torch.cat([heatmap, cls_scores], dim=1)\n\n    # LMDS\n    batch_size, channels = outmaps.shape[:2]\n\n    b_counts, b_labels, b_scores, b_locs, b_dscores = [], [], [], [], []\n    for b in range(batch_size):\n\n        _, locs, _ = self._lmds(heatmap[b][0])\n\n        cls_idx = torch.argmax(clsmap[b,1:,:,:], dim=0)\n        classes = torch.add(cls_idx, 1)\n\n        h_idx = torch.Tensor([l[0] for l in locs]).long()\n        w_idx = torch.Tensor([l[1] for l in locs]).long()\n        labels = classes[h_idx, w_idx].long().tolist()\n\n        chan_idx = cls_idx[h_idx, w_idx].long().tolist()\n        scores = cls_scores[b, chan_idx, h_idx, w_idx].float().tolist()\n\n        dscores = heatmap[b, 0, h_idx, w_idx].float().tolist()\n\n        counts = [labels.count(i) for i in range(1, channels)]\n\n        b_labels.append(labels)\n        b_scores.append(scores)\n        b_locs.append(locs)\n        b_counts.append(counts)\n        b_dscores.append(dscores)\n\n    return b_counts, b_locs, b_labels, b_scores, b_dscores\n</code></pre>"},{"location":"base/models/detection/herdnet/#PytorchWildlife.models.detection.herdnet.HerdNetLMDS.__init__","title":"<code>__init__(up=True, kernel_size=(3, 3), adapt_ts=0.3, neg_ts=0.1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>up</code> <code>bool</code> <p>set to False to disable class maps upsampling. Defaults to True.</p> <code>True</code> <code>kernel_size</code> <code>tuple</code> <p>size of the kernel used to select local maxima. Defaults to (3,3) (as in the paper).</p> <code>(3, 3)</code> <code>adapt_ts</code> <code>float</code> <p>adaptive threshold to select final points from candidates. Defaults to 0.3.</p> <code>0.3</code> <code>neg_ts</code> <code>float</code> <p>negative sample threshold used to define if  an image is a negative sample or not. Defaults to 0.1 (as in the paper).</p> <code>0.1</code> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __init__(\n    self, \n    up: bool = True, \n    kernel_size: tuple = (3,3), \n    adapt_ts: float = 0.3, \n    neg_ts: float = 0.1\n    ) -&gt; None:\n    '''\n    Args:\n        up (bool, optional): set to False to disable class maps upsampling.\n            Defaults to True.\n        kernel_size (tuple, optional): size of the kernel used to select local\n            maxima. Defaults to (3,3) (as in the paper).\n        adapt_ts (float, optional): adaptive threshold to select final points\n            from candidates. Defaults to 0.3.\n        neg_ts (float, optional): negative sample threshold used to define if \n            an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n    '''\n\n    super().__init__(kernel_size=kernel_size, adapt_ts=adapt_ts, neg_ts=neg_ts)\n\n    self.up = up\n</code></pre>"},{"location":"base/models/detection/herdnet/dla/","title":"DLA","text":""},{"location":"base/models/detection/herdnet/dla/#PytorchWildlife.models.detection.herdnet.dla.conv3x3","title":"<code>conv3x3(in_planes, out_planes, stride=1)</code>","text":"<p>3x3 convolution with padding</p> Source code in <code>PytorchWildlife/models/detection/herdnet/dla.py</code> <pre><code>def conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n</code></pre>"},{"location":"base/models/detection/herdnet/model/","title":"Model","text":""},{"location":"base/models/detection/herdnet/model/#PytorchWildlife.models.detection.herdnet.model.HerdNet","title":"<code>HerdNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>HerdNet architecture</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>class HerdNet(nn.Module):\n    ''' HerdNet architecture '''\n\n    def __init__(\n        self,\n        num_layers: int = 34,\n        num_classes: int = 2,\n        pretrained: bool = True, \n        down_ratio: Optional[int] = 2, \n        head_conv: int = 64\n        ):\n        '''\n        Args:\n            num_layers (int, optional): number of layers of DLA. Defaults to 34.\n            num_classes (int, optional): number of output classes, background included. \n                Defaults to 2.\n            pretrained (bool, optional): set False to disable pretrained DLA encoder parameters\n                from ImageNet. Defaults to True.\n            down_ratio (int, optional): downsample ratio. Possible values are 1, 2, 4, 8, or 16. \n                Set to 1 to get output of the same size as input (i.e. no downsample).\n                Defaults to 2.\n            head_conv (int, optional): number of supplementary convolutional layers at the end \n                of decoder. Defaults to 64.\n        '''\n\n        super(HerdNet, self).__init__()\n\n        assert down_ratio in [1, 2, 4, 8, 16], \\\n            f'Downsample ratio possible values are 1, 2, 4, 8 or 16, got {down_ratio}'\n\n        base_name = 'dla{}'.format(num_layers)\n\n        self.down_ratio = down_ratio\n        self.num_classes = num_classes\n        self.head_conv = head_conv\n\n        self.first_level = int(np.log2(down_ratio))\n\n        # backbone\n        base = dla_modules.__dict__[base_name](pretrained=pretrained, return_levels=True)\n        setattr(self, 'base_0', base)\n        setattr(self, 'channels_0', base.channels)\n\n        channels = self.channels_0\n\n        scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n        self.dla_up = dla_modules.DLAUp(channels[self.first_level:], scales=scales)\n        # self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])\n\n        # bottleneck conv\n        self.bottleneck_conv = nn.Conv2d(\n            channels[-1], channels[-1], \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n        )\n\n        # localization head\n        self.loc_head = nn.Sequential(\n            nn.Conv2d(channels[self.first_level], head_conv,\n            kernel_size=3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                head_conv, 1, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                ),\n            nn.Sigmoid()\n            )\n\n        self.loc_head[-2].bias.data.fill_(0.00)\n\n        # classification head\n        self.cls_head = nn.Sequential(\n            nn.Conv2d(channels[-1], head_conv,\n            kernel_size=3, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                head_conv, self.num_classes, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                )\n            )\n\n        self.cls_head[-1].bias.data.fill_(0.00)\n\n    def forward(self, input: torch.Tensor):\n\n        encode = self.base_0(input)    \n        bottleneck = self.bottleneck_conv(encode[-1])\n        encode[-1] = bottleneck\n\n        decode_hm = self.dla_up(encode[self.first_level:])\n        # decode_cls = self.cls_dla_up(encode[-3:])\n\n        heatmap = self.loc_head(decode_hm)\n        clsmap = self.cls_head(bottleneck)\n        # clsmap = self.cls_head(decode_cls)\n\n        return heatmap, clsmap\n\n    def freeze(self, layers: list) -&gt; None:\n        ''' Freeze all layers mentioned in the input list '''\n        for layer in layers:\n            self._freeze_layer(layer)\n\n    def _freeze_layer(self, layer_name: str) -&gt; None:\n        for param in getattr(self, layer_name).parameters():\n            param.requires_grad = False\n\n    def reshape_classes(self, num_classes: int) -&gt; None:\n        ''' Reshape architecture according to a new number of classes.\n\n        Arg:\n            num_classes (int): new number of classes\n        '''\n\n        self.cls_head[-1] = nn.Conv2d(\n                self.head_conv, num_classes, \n                kernel_size=1, stride=1, \n                padding=0, bias=True\n                )\n\n        self.cls_head[-1].bias.data.fill_(0.00)\n\n        self.num_classes = num_classes\n</code></pre>"},{"location":"base/models/detection/herdnet/model/#PytorchWildlife.models.detection.herdnet.model.HerdNet.__init__","title":"<code>__init__(num_layers=34, num_classes=2, pretrained=True, down_ratio=2, head_conv=64)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>number of layers of DLA. Defaults to 34.</p> <code>34</code> <code>num_classes</code> <code>int</code> <p>number of output classes, background included.  Defaults to 2.</p> <code>2</code> <code>pretrained</code> <code>bool</code> <p>set False to disable pretrained DLA encoder parameters from ImageNet. Defaults to True.</p> <code>True</code> <code>down_ratio</code> <code>int</code> <p>downsample ratio. Possible values are 1, 2, 4, 8, or 16.  Set to 1 to get output of the same size as input (i.e. no downsample). Defaults to 2.</p> <code>2</code> <code>head_conv</code> <code>int</code> <p>number of supplementary convolutional layers at the end  of decoder. Defaults to 64.</p> <code>64</code> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def __init__(\n    self,\n    num_layers: int = 34,\n    num_classes: int = 2,\n    pretrained: bool = True, \n    down_ratio: Optional[int] = 2, \n    head_conv: int = 64\n    ):\n    '''\n    Args:\n        num_layers (int, optional): number of layers of DLA. Defaults to 34.\n        num_classes (int, optional): number of output classes, background included. \n            Defaults to 2.\n        pretrained (bool, optional): set False to disable pretrained DLA encoder parameters\n            from ImageNet. Defaults to True.\n        down_ratio (int, optional): downsample ratio. Possible values are 1, 2, 4, 8, or 16. \n            Set to 1 to get output of the same size as input (i.e. no downsample).\n            Defaults to 2.\n        head_conv (int, optional): number of supplementary convolutional layers at the end \n            of decoder. Defaults to 64.\n    '''\n\n    super(HerdNet, self).__init__()\n\n    assert down_ratio in [1, 2, 4, 8, 16], \\\n        f'Downsample ratio possible values are 1, 2, 4, 8 or 16, got {down_ratio}'\n\n    base_name = 'dla{}'.format(num_layers)\n\n    self.down_ratio = down_ratio\n    self.num_classes = num_classes\n    self.head_conv = head_conv\n\n    self.first_level = int(np.log2(down_ratio))\n\n    # backbone\n    base = dla_modules.__dict__[base_name](pretrained=pretrained, return_levels=True)\n    setattr(self, 'base_0', base)\n    setattr(self, 'channels_0', base.channels)\n\n    channels = self.channels_0\n\n    scales = [2 ** i for i in range(len(channels[self.first_level:]))]\n    self.dla_up = dla_modules.DLAUp(channels[self.first_level:], scales=scales)\n    # self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])\n\n    # bottleneck conv\n    self.bottleneck_conv = nn.Conv2d(\n        channels[-1], channels[-1], \n        kernel_size=1, stride=1, \n        padding=0, bias=True\n    )\n\n    # localization head\n    self.loc_head = nn.Sequential(\n        nn.Conv2d(channels[self.first_level], head_conv,\n        kernel_size=3, padding=1, bias=True),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(\n            head_conv, 1, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            ),\n        nn.Sigmoid()\n        )\n\n    self.loc_head[-2].bias.data.fill_(0.00)\n\n    # classification head\n    self.cls_head = nn.Sequential(\n        nn.Conv2d(channels[-1], head_conv,\n        kernel_size=3, padding=1, bias=True),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(\n            head_conv, self.num_classes, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            )\n        )\n\n    self.cls_head[-1].bias.data.fill_(0.00)\n</code></pre>"},{"location":"base/models/detection/herdnet/model/#PytorchWildlife.models.detection.herdnet.model.HerdNet.freeze","title":"<code>freeze(layers)</code>","text":"<p>Freeze all layers mentioned in the input list</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def freeze(self, layers: list) -&gt; None:\n    ''' Freeze all layers mentioned in the input list '''\n    for layer in layers:\n        self._freeze_layer(layer)\n</code></pre>"},{"location":"base/models/detection/herdnet/model/#PytorchWildlife.models.detection.herdnet.model.HerdNet.reshape_classes","title":"<code>reshape_classes(num_classes)</code>","text":"<p>Reshape architecture according to a new number of classes.</p> Arg <p>num_classes (int): new number of classes</p> Source code in <code>PytorchWildlife/models/detection/herdnet/model.py</code> <pre><code>def reshape_classes(self, num_classes: int) -&gt; None:\n    ''' Reshape architecture according to a new number of classes.\n\n    Arg:\n        num_classes (int): new number of classes\n    '''\n\n    self.cls_head[-1] = nn.Conv2d(\n            self.head_conv, num_classes, \n            kernel_size=1, stride=1, \n            padding=0, bias=True\n            )\n\n    self.cls_head[-1].bias.data.fill_(0.00)\n\n    self.num_classes = num_classes\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/patches/","title":"Patches","text":""},{"location":"base/models/detection/herdnet/animaloc/data/patches/#PytorchWildlife.models.detection.herdnet.animaloc.data.patches.ImageToPatches","title":"<code>ImageToPatches</code>","text":"<p>Class to make patches from a tensor image</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/patches.py</code> <pre><code>class ImageToPatches:\n    ''' Class to make patches from a tensor image '''\n\n    def __init__(\n            self, \n            image: Union[PIL.Image.Image, torch.Tensor], \n            size: Tuple[int,int], \n            overlap: int = 0\n        ) -&gt; None:\n        '''\n        Args:\n          image (PIL.Image.Image or torch.Tensor): image, if tensor: (C,H,W)\n          size (tuple): patches size (height, width), in pixels\n          overlap (int, optional): overlap between patches, in pixels. \n              Defaults to 0.\n        '''\n\n        assert isinstance(image, (PIL.Image.Image, torch.Tensor)), \\\n            'image must be a PIL.Image.Image or a torch.Tensor instance'\n\n        self.image = image\n        if isinstance(self.image, PIL.Image.Image):\n            self.image = torchvision.transforms.ToTensor()(self.image)\n\n        self.size = size\n        self.overlap = overlap\n\n    def make_patches(self) -&gt; torch.Tensor:\n        ''' Make patches from the image\n\n        When the image division is not perfect, a zero-padding is performed \n        so that the patches have the same size.\n\n        Returns:\n            torch.Tensor:\n                patches of shape (B,C,H,W)\n        '''\n        # patches' height &amp; width\n        height = min(self.image.size(1),self.size[0])\n        width = min(self.image.size(2),self.size[1])\n\n        # unfold on height \n        height_fold = self.image.unfold(1, height, height - self.overlap)\n\n        # if non-perfect division on height\n        residual = self._img_residual(self.image.size(1), height, self.overlap)\n        if residual != 0:\n            # get the residual patch and add it to the fold\n            remaining_height = torch.zeros(3, 1, self.image.size(2), height) # padding\n            remaining_height[:,:,:,:residual] = self.image[:,-residual:,:].permute(0,2,1).unsqueeze(1)\n\n            height_fold = torch.cat((height_fold,remaining_height),dim=1)\n\n        # unfold on width\n        fold = height_fold.unfold(2, width, width - self.overlap)\n\n        # if non-perfect division on width, the same\n        residual = self._img_residual(self.image.size(2), width, self.overlap)\n        if residual != 0:\n            remaining_width = torch.zeros(3, fold.shape[1], 1, height, width) # padding\n            remaining_width[:,:,:,:,:residual] = height_fold[:,:,-residual:,:].permute(0,1,3,2).unsqueeze(2)\n\n            fold = torch.cat((fold,remaining_width),dim=2)\n\n        self._nrow , self._ncol = fold.shape[2] , fold.shape[1]\n\n        # reshaping\n        patches = fold.permute(1,2,0,3,4).reshape(-1,self.image.size(0),height,width)\n\n        return patches\n\n    def get_limits(self) -&gt; dict:\n        ''' Get patches limits within the image frame\n\n        When the image division is not perfect, the zero-padding is not\n        considered here. Hence, the limits are the true limits of patches\n        within the initial image.\n\n        Returns:\n            dict:\n                a dict containing int as key and BoundingBox as value\n        '''\n\n        # patches' height &amp; width\n        height = min(self.image.size(1),self.size[0])\n        width = min(self.image.size(2),self.size[1])\n\n        # lists of pixels numbers\n        y_pixels = torch.tensor(list(range(0,self.image.size(1)+1)))\n        x_pixels = torch.tensor(list(range(0,self.image.size(2)+1)))\n\n        # cut into patches to get limits\n        y_pixels_fold = y_pixels.unfold(0, height+1, height-self.overlap)\n        y_mina = [int(patch[0]) for patch in y_pixels_fold]\n        y_maxa = [int(patch[-1]) for patch in y_pixels_fold]\n\n        x_pixels_fold = x_pixels.unfold(0, width+1, width-self.overlap)\n        x_mina = [int(patch[0]) for patch in x_pixels_fold]\n        x_maxa = [int(patch[-1]) for patch in x_pixels_fold]\n\n        # if non-perfect division on height\n        residual = self._img_residual(self.image.size(1), height, self.overlap)\n        if residual != 0:\n            remaining_y = y_pixels[-residual-1:].unsqueeze(0)[0]\n            y_mina.append(int(remaining_y[0]))\n            y_maxa.append(int(remaining_y[-1]))\n\n        # if non-perfect division on width  \n        residual = self._img_residual(self.image.size(2), width, self.overlap)\n        if residual != 0:\n            remaining_x = x_pixels[-residual-1:].unsqueeze(0)[0]\n            x_mina.append(int(remaining_x[0]))\n            x_maxa.append(int(remaining_x[-1]))\n\n        i = 0\n        patches_limits = {}\n        for y_min , y_max in zip(y_mina,y_maxa):\n            for x_min , x_max in zip(x_mina,x_maxa):\n                patches_limits[i] = BoundingBox(x_min,y_min,x_max,y_max)\n                i += 1\n\n        return patches_limits\n\n    def show(self) -&gt; None:\n        ''' Show the grid of patches '''\n\n        grid = make_grid(\n            self.make_patches(),\n            padding=50,\n            nrow=self._nrow\n            ).permute(1,2,0).numpy()\n\n        plt.imshow(grid)\n\n        plt.show()\n\n        return grid\n\n    def _img_residual(self, ims: int, ks: int, overlap: int) -&gt; int:\n\n        ims, stride = int(ims), int(ks - overlap)\n        n = ims // stride\n        end = n * stride + overlap\n\n        residual = ims % stride\n\n        if end &gt; ims:\n            n -= 1\n            residual = ims - (n * stride)\n\n        return residual\n\n    def __len__(self) -&gt; int:\n        return len(self.get_limits())\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/patches/#PytorchWildlife.models.detection.herdnet.animaloc.data.patches.ImageToPatches.__init__","title":"<code>__init__(image, size, overlap=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image or Tensor</code> <p>image, if tensor: (C,H,W)</p> required <code>size</code> <code>tuple</code> <p>patches size (height, width), in pixels</p> required <code>overlap</code> <code>int</code> <p>overlap between patches, in pixels.    Defaults to 0.</p> <code>0</code> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/patches.py</code> <pre><code>def __init__(\n        self, \n        image: Union[PIL.Image.Image, torch.Tensor], \n        size: Tuple[int,int], \n        overlap: int = 0\n    ) -&gt; None:\n    '''\n    Args:\n      image (PIL.Image.Image or torch.Tensor): image, if tensor: (C,H,W)\n      size (tuple): patches size (height, width), in pixels\n      overlap (int, optional): overlap between patches, in pixels. \n          Defaults to 0.\n    '''\n\n    assert isinstance(image, (PIL.Image.Image, torch.Tensor)), \\\n        'image must be a PIL.Image.Image or a torch.Tensor instance'\n\n    self.image = image\n    if isinstance(self.image, PIL.Image.Image):\n        self.image = torchvision.transforms.ToTensor()(self.image)\n\n    self.size = size\n    self.overlap = overlap\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/patches/#PytorchWildlife.models.detection.herdnet.animaloc.data.patches.ImageToPatches.get_limits","title":"<code>get_limits()</code>","text":"<p>Get patches limits within the image frame</p> <p>When the image division is not perfect, the zero-padding is not considered here. Hence, the limits are the true limits of patches within the initial image.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>a dict containing int as key and BoundingBox as value</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/patches.py</code> <pre><code>def get_limits(self) -&gt; dict:\n    ''' Get patches limits within the image frame\n\n    When the image division is not perfect, the zero-padding is not\n    considered here. Hence, the limits are the true limits of patches\n    within the initial image.\n\n    Returns:\n        dict:\n            a dict containing int as key and BoundingBox as value\n    '''\n\n    # patches' height &amp; width\n    height = min(self.image.size(1),self.size[0])\n    width = min(self.image.size(2),self.size[1])\n\n    # lists of pixels numbers\n    y_pixels = torch.tensor(list(range(0,self.image.size(1)+1)))\n    x_pixels = torch.tensor(list(range(0,self.image.size(2)+1)))\n\n    # cut into patches to get limits\n    y_pixels_fold = y_pixels.unfold(0, height+1, height-self.overlap)\n    y_mina = [int(patch[0]) for patch in y_pixels_fold]\n    y_maxa = [int(patch[-1]) for patch in y_pixels_fold]\n\n    x_pixels_fold = x_pixels.unfold(0, width+1, width-self.overlap)\n    x_mina = [int(patch[0]) for patch in x_pixels_fold]\n    x_maxa = [int(patch[-1]) for patch in x_pixels_fold]\n\n    # if non-perfect division on height\n    residual = self._img_residual(self.image.size(1), height, self.overlap)\n    if residual != 0:\n        remaining_y = y_pixels[-residual-1:].unsqueeze(0)[0]\n        y_mina.append(int(remaining_y[0]))\n        y_maxa.append(int(remaining_y[-1]))\n\n    # if non-perfect division on width  \n    residual = self._img_residual(self.image.size(2), width, self.overlap)\n    if residual != 0:\n        remaining_x = x_pixels[-residual-1:].unsqueeze(0)[0]\n        x_mina.append(int(remaining_x[0]))\n        x_maxa.append(int(remaining_x[-1]))\n\n    i = 0\n    patches_limits = {}\n    for y_min , y_max in zip(y_mina,y_maxa):\n        for x_min , x_max in zip(x_mina,x_maxa):\n            patches_limits[i] = BoundingBox(x_min,y_min,x_max,y_max)\n            i += 1\n\n    return patches_limits\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/patches/#PytorchWildlife.models.detection.herdnet.animaloc.data.patches.ImageToPatches.make_patches","title":"<code>make_patches()</code>","text":"<p>Make patches from the image</p> <p>When the image division is not perfect, a zero-padding is performed  so that the patches have the same size.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: patches of shape (B,C,H,W)</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/patches.py</code> <pre><code>def make_patches(self) -&gt; torch.Tensor:\n    ''' Make patches from the image\n\n    When the image division is not perfect, a zero-padding is performed \n    so that the patches have the same size.\n\n    Returns:\n        torch.Tensor:\n            patches of shape (B,C,H,W)\n    '''\n    # patches' height &amp; width\n    height = min(self.image.size(1),self.size[0])\n    width = min(self.image.size(2),self.size[1])\n\n    # unfold on height \n    height_fold = self.image.unfold(1, height, height - self.overlap)\n\n    # if non-perfect division on height\n    residual = self._img_residual(self.image.size(1), height, self.overlap)\n    if residual != 0:\n        # get the residual patch and add it to the fold\n        remaining_height = torch.zeros(3, 1, self.image.size(2), height) # padding\n        remaining_height[:,:,:,:residual] = self.image[:,-residual:,:].permute(0,2,1).unsqueeze(1)\n\n        height_fold = torch.cat((height_fold,remaining_height),dim=1)\n\n    # unfold on width\n    fold = height_fold.unfold(2, width, width - self.overlap)\n\n    # if non-perfect division on width, the same\n    residual = self._img_residual(self.image.size(2), width, self.overlap)\n    if residual != 0:\n        remaining_width = torch.zeros(3, fold.shape[1], 1, height, width) # padding\n        remaining_width[:,:,:,:,:residual] = height_fold[:,:,-residual:,:].permute(0,1,3,2).unsqueeze(2)\n\n        fold = torch.cat((fold,remaining_width),dim=2)\n\n    self._nrow , self._ncol = fold.shape[2] , fold.shape[1]\n\n    # reshaping\n    patches = fold.permute(1,2,0,3,4).reshape(-1,self.image.size(0),height,width)\n\n    return patches\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/patches/#PytorchWildlife.models.detection.herdnet.animaloc.data.patches.ImageToPatches.show","title":"<code>show()</code>","text":"<p>Show the grid of patches</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/patches.py</code> <pre><code>def show(self) -&gt; None:\n    ''' Show the grid of patches '''\n\n    grid = make_grid(\n        self.make_patches(),\n        padding=50,\n        nrow=self._nrow\n        ).permute(1,2,0).numpy()\n\n    plt.imshow(grid)\n\n    plt.show()\n\n    return grid\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/types/","title":"Types","text":""},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>Class to define a BoundingBox object in a 2D Cartesian  coordinate system.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/types.py</code> <pre><code>class BoundingBox:\n    ''' Class to define a BoundingBox object in a 2D Cartesian \n    coordinate system.\n    '''\n\n    def __init__(\n            self, \n            x_min: Union[int,float], \n            y_min: Union[int,float], \n            x_max: Union[int,float], \n            y_max: Union[int,float]\n        ) -&gt; None:\n        '''\n        Args:\n            x_min (int, float): x bbox top-left coordinate\n            y_min (int, float): y bbox top-left coordinate\n            x_max (int, float): x bbox bottom-right coordinate\n            y_max (int, float): y bbox bottom-right coordinate\n        '''\n\n        assert all([c &gt;= 0 for c in [x_min,y_min,x_max,y_max]]), \\\n            f'Coordinates must be positives, got x_min={x_min}, y_min={y_min}, ' \\\n                f'x_max={x_max} and y_max={y_max}'\n\n        assert x_max &gt;= x_min and y_max &gt;= y_min, \\\n            'Wrong bounding box coordinates.'\n\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max\n\n    @property\n    def area(self) -&gt; Union[int,float]:\n        ''' To get bbox area '''\n        return max(0, self.width) * max(0, self.height)\n\n    @property\n    def width(self) -&gt; Union[int,float]:\n        ''' To get bbox width '''\n        return max(0, self.x_max - self.x_min)\n\n    @property\n    def height(self) -&gt; Union[int,float]:\n        ''' To get bbox height '''\n        return max(0, self.y_max - self.y_min)\n\n    @property\n    def get_tuple(self) -&gt; Tuple[Union[int,float],...]:\n        ''' To get bbox coordinates in tuple type '''\n        return (self.x_min,self.y_min,self.x_max,self.y_max)\n\n    @property\n    def atype(self) -&gt; str:\n        ''' To get annotation type string '''\n        return 'BoundingBox'\n\n    def __repr__(self) -&gt; str:\n        return f'BoundingBox(x_min: {self.x_min}, y_min: {self.y_min}, x_max: {self.x_max}, y_max: {self.y_max})'\n\n    def __eq__(self, other) -&gt; bool:\n        return all([\n            self.x_min == other.x_min,\n            self.y_min == other.y_min,\n            self.x_max == other.x_max,\n            self.y_max == other.y_max\n            ])\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.area","title":"<code>area</code>  <code>property</code>","text":"<p>To get bbox area</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.atype","title":"<code>atype</code>  <code>property</code>","text":"<p>To get annotation type string</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.get_tuple","title":"<code>get_tuple</code>  <code>property</code>","text":"<p>To get bbox coordinates in tuple type</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.height","title":"<code>height</code>  <code>property</code>","text":"<p>To get bbox height</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.width","title":"<code>width</code>  <code>property</code>","text":"<p>To get bbox width</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.BoundingBox.__init__","title":"<code>__init__(x_min, y_min, x_max, y_max)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x_min</code> <code>(int, float)</code> <p>x bbox top-left coordinate</p> required <code>y_min</code> <code>(int, float)</code> <p>y bbox top-left coordinate</p> required <code>x_max</code> <code>(int, float)</code> <p>x bbox bottom-right coordinate</p> required <code>y_max</code> <code>(int, float)</code> <p>y bbox bottom-right coordinate</p> required Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/types.py</code> <pre><code>def __init__(\n        self, \n        x_min: Union[int,float], \n        y_min: Union[int,float], \n        x_max: Union[int,float], \n        y_max: Union[int,float]\n    ) -&gt; None:\n    '''\n    Args:\n        x_min (int, float): x bbox top-left coordinate\n        y_min (int, float): y bbox top-left coordinate\n        x_max (int, float): x bbox bottom-right coordinate\n        y_max (int, float): y bbox bottom-right coordinate\n    '''\n\n    assert all([c &gt;= 0 for c in [x_min,y_min,x_max,y_max]]), \\\n        f'Coordinates must be positives, got x_min={x_min}, y_min={y_min}, ' \\\n            f'x_max={x_max} and y_max={y_max}'\n\n    assert x_max &gt;= x_min and y_max &gt;= y_min, \\\n        'Wrong bounding box coordinates.'\n\n    self.x_min = x_min\n    self.y_min = y_min\n    self.x_max = x_max\n    self.y_max = y_max\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.Point","title":"<code>Point</code>","text":"<p>Class to define a Point object in a 2D Cartesian  coordinate system.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/types.py</code> <pre><code>class Point:\n    ''' Class to define a Point object in a 2D Cartesian \n    coordinate system.\n    '''\n\n    def __init__(self, x: Union[int,float], y: Union[int,float]) -&gt; None:\n        '''\n        Args:\n            x (int, float): x coordinate\n            y (int, float): y coordinate\n        '''\n\n        assert x &gt;= 0 and y &gt;= 0, f'Coordinates must be positives, got x={x} and y={y}'\n\n        self.x = x\n        self.y = y\n        self.area = 1 # always 1 pixel\n\n    # @property\n    # def area(self) -&gt; int:\n    #     ''' To get area '''\n    #     return 1 # always 1 pixel\n\n    @property\n    def get_tuple(self) -&gt; Tuple[Union[int,float],Union[int,float]]:\n        ''' To get point's coordinates in tuple '''\n        return (self.x,self.y)\n\n    @property\n    def atype(self) -&gt; str:\n        ''' To get annotation type string '''\n        return 'Point'\n\n    def __repr__(self) -&gt; str:\n        return f'Point(x: {self.x}, y: {self.y})'\n\n    def __eq__(self, other) -&gt; bool:\n        return all([\n            self.x == other.x,\n            self.y == other.y\n            ])\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.Point.atype","title":"<code>atype</code>  <code>property</code>","text":"<p>To get annotation type string</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.Point.get_tuple","title":"<code>get_tuple</code>  <code>property</code>","text":"<p>To get point's coordinates in tuple</p>"},{"location":"base/models/detection/herdnet/animaloc/data/types/#PytorchWildlife.models.detection.herdnet.animaloc.data.types.Point.__init__","title":"<code>__init__(x, y)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>(int, float)</code> <p>x coordinate</p> required <code>y</code> <code>(int, float)</code> <p>y coordinate</p> required Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/data/types.py</code> <pre><code>def __init__(self, x: Union[int,float], y: Union[int,float]) -&gt; None:\n    '''\n    Args:\n        x (int, float): x coordinate\n        y (int, float): y coordinate\n    '''\n\n    assert x &gt;= 0 and y &gt;= 0, f'Coordinates must be positives, got x={x} and y={y}'\n\n    self.x = x\n    self.y = y\n    self.area = 1 # always 1 pixel\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/","title":"LMDS","text":""},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.HerdNetLMDS","title":"<code>HerdNetLMDS</code>","text":"<p>               Bases: <code>LMDS</code></p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>class HerdNetLMDS(LMDS):\n\n    def __init__(\n        self, \n        up: bool = True, \n        kernel_size: tuple = (3,3), \n        adapt_ts: float = 0.3, \n        neg_ts: float = 0.1\n        ) -&gt; None:\n        '''\n        Args:\n            up (bool, optional): set to False to disable class maps upsampling.\n                Defaults to True.\n            kernel_size (tuple, optional): size of the kernel used to select local\n                maxima. Defaults to (3,3) (as in the paper).\n            adapt_ts (float, optional): adaptive threshold to select final points\n                from candidates. Defaults to 0.3.\n            neg_ts (float, optional): negative sample threshold used to define if \n                an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n        '''\n\n        super().__init__(kernel_size=kernel_size, adapt_ts=adapt_ts, neg_ts=neg_ts)\n\n        self.up = up\n\n    def __call__(self, outputs: List[torch.Tensor]) -&gt; Tuple[list, list, list, list, list]:\n        \"\"\"\n        Args:\n            outputs (List[torch.Tensor]): Outputs of HerdNet, i.e., 2 tensors:\n                - heatmap: [B,1,H,W],\n                - class map: [B,C,H/16,W/16].\n\n        Returns:\n            Tuple[list, list, list, list, list]:\n                Counts, locations, labels, class scores, and detection scores per batch.\n        \"\"\"\n\n        heatmap, clsmap = outputs\n\n        # upsample class map\n        if self.up:\n            scale_factor = 16\n            clsmap = F.interpolate(clsmap, scale_factor=scale_factor, mode='nearest')\n\n        # softmax\n        cls_scores = torch.softmax(clsmap, dim=1)[:,1:,:,:]\n\n        # cat to heatmap\n        outmaps = torch.cat([heatmap, cls_scores], dim=1)\n\n        # LMDS\n        batch_size, channels = outmaps.shape[:2]\n\n        b_counts, b_labels, b_scores, b_locs, b_dscores = [], [], [], [], []\n        for b in range(batch_size):\n\n            _, locs, _ = self._lmds(heatmap[b][0])\n\n            cls_idx = torch.argmax(clsmap[b,1:,:,:], dim=0)\n            classes = torch.add(cls_idx, 1)\n\n            h_idx = torch.Tensor([l[0] for l in locs]).long()\n            w_idx = torch.Tensor([l[1] for l in locs]).long()\n            labels = classes[h_idx, w_idx].long().tolist()\n\n            chan_idx = cls_idx[h_idx, w_idx].long().tolist()\n            scores = cls_scores[b, chan_idx, h_idx, w_idx].float().tolist()\n\n            dscores = heatmap[b, 0, h_idx, w_idx].float().tolist()\n\n            counts = [labels.count(i) for i in range(1, channels)]\n\n            b_labels.append(labels)\n            b_scores.append(scores)\n            b_locs.append(locs)\n            b_counts.append(counts)\n            b_dscores.append(dscores)\n\n        return b_counts, b_locs, b_labels, b_scores, b_dscores\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.HerdNetLMDS.__call__","title":"<code>__call__(outputs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>List[Tensor]</code> <p>Outputs of HerdNet, i.e., 2 tensors: - heatmap: [B,1,H,W], - class map: [B,C,H/16,W/16].</p> required <p>Returns:</p> Type Description <code>Tuple[list, list, list, list, list]</code> <p>Tuple[list, list, list, list, list]: Counts, locations, labels, class scores, and detection scores per batch.</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __call__(self, outputs: List[torch.Tensor]) -&gt; Tuple[list, list, list, list, list]:\n    \"\"\"\n    Args:\n        outputs (List[torch.Tensor]): Outputs of HerdNet, i.e., 2 tensors:\n            - heatmap: [B,1,H,W],\n            - class map: [B,C,H/16,W/16].\n\n    Returns:\n        Tuple[list, list, list, list, list]:\n            Counts, locations, labels, class scores, and detection scores per batch.\n    \"\"\"\n\n    heatmap, clsmap = outputs\n\n    # upsample class map\n    if self.up:\n        scale_factor = 16\n        clsmap = F.interpolate(clsmap, scale_factor=scale_factor, mode='nearest')\n\n    # softmax\n    cls_scores = torch.softmax(clsmap, dim=1)[:,1:,:,:]\n\n    # cat to heatmap\n    outmaps = torch.cat([heatmap, cls_scores], dim=1)\n\n    # LMDS\n    batch_size, channels = outmaps.shape[:2]\n\n    b_counts, b_labels, b_scores, b_locs, b_dscores = [], [], [], [], []\n    for b in range(batch_size):\n\n        _, locs, _ = self._lmds(heatmap[b][0])\n\n        cls_idx = torch.argmax(clsmap[b,1:,:,:], dim=0)\n        classes = torch.add(cls_idx, 1)\n\n        h_idx = torch.Tensor([l[0] for l in locs]).long()\n        w_idx = torch.Tensor([l[1] for l in locs]).long()\n        labels = classes[h_idx, w_idx].long().tolist()\n\n        chan_idx = cls_idx[h_idx, w_idx].long().tolist()\n        scores = cls_scores[b, chan_idx, h_idx, w_idx].float().tolist()\n\n        dscores = heatmap[b, 0, h_idx, w_idx].float().tolist()\n\n        counts = [labels.count(i) for i in range(1, channels)]\n\n        b_labels.append(labels)\n        b_scores.append(scores)\n        b_locs.append(locs)\n        b_counts.append(counts)\n        b_dscores.append(dscores)\n\n    return b_counts, b_locs, b_labels, b_scores, b_dscores\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.HerdNetLMDS.__init__","title":"<code>__init__(up=True, kernel_size=(3, 3), adapt_ts=0.3, neg_ts=0.1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>up</code> <code>bool</code> <p>set to False to disable class maps upsampling. Defaults to True.</p> <code>True</code> <code>kernel_size</code> <code>tuple</code> <p>size of the kernel used to select local maxima. Defaults to (3,3) (as in the paper).</p> <code>(3, 3)</code> <code>adapt_ts</code> <code>float</code> <p>adaptive threshold to select final points from candidates. Defaults to 0.3.</p> <code>0.3</code> <code>neg_ts</code> <code>float</code> <p>negative sample threshold used to define if  an image is a negative sample or not. Defaults to 0.1 (as in the paper).</p> <code>0.1</code> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __init__(\n    self, \n    up: bool = True, \n    kernel_size: tuple = (3,3), \n    adapt_ts: float = 0.3, \n    neg_ts: float = 0.1\n    ) -&gt; None:\n    '''\n    Args:\n        up (bool, optional): set to False to disable class maps upsampling.\n            Defaults to True.\n        kernel_size (tuple, optional): size of the kernel used to select local\n            maxima. Defaults to (3,3) (as in the paper).\n        adapt_ts (float, optional): adaptive threshold to select final points\n            from candidates. Defaults to 0.3.\n        neg_ts (float, optional): negative sample threshold used to define if \n            an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n    '''\n\n    super().__init__(kernel_size=kernel_size, adapt_ts=adapt_ts, neg_ts=neg_ts)\n\n    self.up = up\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.LMDS","title":"<code>LMDS</code>","text":"<p>Local Maxima Detection Strategy </p> <p>Adapted and enhanced from https://github.com/dk-liang/FIDTM (author: dklinag) available under the MIT license</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>class LMDS:\n    ''' Local Maxima Detection Strategy \n\n    Adapted and enhanced from https://github.com/dk-liang/FIDTM (author: dklinag)\n    available under the MIT license '''\n\n    def __init__(\n        self, \n        kernel_size: tuple = (3,3),\n        adapt_ts: float = 100.0/255.0, \n        neg_ts: float = 0.1\n        ) -&gt; None:\n        '''\n        Args:\n            kernel_size (tuple, optional): size of the kernel used to select local\n                maxima. Defaults to (3,3) (as in the paper).\n            adapt_ts (float, optional): adaptive threshold to select final points\n                from candidates. Defaults to 100.0/255.0 (as in the paper).\n            neg_ts (float, optional): negative sample threshold used to define if \n                an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n        '''\n\n        assert kernel_size[0] == kernel_size[1], \\\n            f'The kernel shape must be a square, got {kernel_size[0]}x{kernel_size[1]}'\n        assert not kernel_size[0] % 2 == 0, \\\n            f'The kernel size must be odd, got {kernel_size[0]}'\n\n        self.kernel_size = tuple(kernel_size)\n        self.adapt_ts = adapt_ts\n        self.neg_ts = neg_ts\n\n    def __call__(self, est_map: torch.Tensor) -&gt; Tuple[list,list,list,list]:\n        '''\n        Args:\n            est_map (torch.Tensor): the estimated FIDT map\n\n        Returns:\n            Tuple[list,list,list,list]\n                counts, labels, scores and locations per batch\n        '''\n        batch_size, classes = est_map.shape[:2]\n\n        b_counts, b_labels, b_scores, b_locs = [], [], [], []\n        for b in range(batch_size):\n            counts, labels, scores, locs = [], [], [], []\n\n            for c in range(classes):\n                count, loc, score = self._lmds(est_map[b][c])\n                counts.append(count)\n                labels = [*labels, *[c+1]*count]\n                scores = [*scores, *score]\n                locs = [*locs, *loc]\n\n            b_counts.append(counts)\n            b_labels.append(labels)\n            b_scores.append(scores)\n            b_locs.append(locs)\n\n        return b_counts, b_locs, b_labels, b_scores\n\n    def _local_max(self, est_map: torch.Tensor) -&gt; torch.Tensor:\n        ''' Shape: est_map = [B,C,H,W] '''\n\n        pad = int(self.kernel_size[0] / 2)\n        keep = torch.nn.functional.max_pool2d(est_map, kernel_size=self.kernel_size, stride=1, padding=pad)\n        keep = (keep == est_map).float()\n        est_map = keep * est_map\n\n        return est_map\n\n    def _get_locs_and_scores(\n        self, \n        locs_map: torch.Tensor, \n        scores_map: torch.Tensor\n        ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        ''' Shapes: locs_map = [H,W] and scores_map = [H,W] '''\n\n        locs_map = locs_map.data.cpu().numpy()\n        scores_map = scores_map.data.cpu().numpy()\n        locs = []\n        scores = []\n        for i, j in numpy.argwhere(locs_map ==  1):\n            locs.append((i,j))\n            scores.append(scores_map[i][j])\n\n        return torch.Tensor(locs), torch.Tensor(scores)\n\n    def _lmds(self, est_map: torch.Tensor) -&gt; Tuple[int, list, list]:\n        ''' Shape: est_map = [H,W] '''\n\n        est_map_max = torch.max(est_map).item()\n\n        # local maxima\n        est_map = self._local_max(est_map.unsqueeze(0).unsqueeze(0))\n\n        # adaptive threshold for counting\n        est_map[est_map &lt; self.adapt_ts * est_map_max] = 0\n        scores_map = torch.clone(est_map)\n        est_map[est_map &gt; 0] = 1\n\n        # negative sample\n        if est_map_max &lt; self.neg_ts:\n            est_map = est_map * 0\n\n        # count\n        count = int(torch.sum(est_map).item())\n\n        # locations and scores\n        locs, scores = self._get_locs_and_scores(\n            est_map.squeeze(0).squeeze(0), \n            scores_map.squeeze(0).squeeze(0)\n            )\n\n        return count, locs.tolist(), scores.tolist()\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.LMDS.__call__","title":"<code>__call__(est_map)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>est_map</code> <code>Tensor</code> <p>the estimated FIDT map</p> required <p>Returns:</p> Type Description <code>Tuple[list, list, list, list]</code> <p>Tuple[list,list,list,list] counts, labels, scores and locations per batch</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __call__(self, est_map: torch.Tensor) -&gt; Tuple[list,list,list,list]:\n    '''\n    Args:\n        est_map (torch.Tensor): the estimated FIDT map\n\n    Returns:\n        Tuple[list,list,list,list]\n            counts, labels, scores and locations per batch\n    '''\n    batch_size, classes = est_map.shape[:2]\n\n    b_counts, b_labels, b_scores, b_locs = [], [], [], []\n    for b in range(batch_size):\n        counts, labels, scores, locs = [], [], [], []\n\n        for c in range(classes):\n            count, loc, score = self._lmds(est_map[b][c])\n            counts.append(count)\n            labels = [*labels, *[c+1]*count]\n            scores = [*scores, *score]\n            locs = [*locs, *loc]\n\n        b_counts.append(counts)\n        b_labels.append(labels)\n        b_scores.append(scores)\n        b_locs.append(locs)\n\n    return b_counts, b_locs, b_labels, b_scores\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/lmds/#PytorchWildlife.models.detection.herdnet.animaloc.eval.lmds.LMDS.__init__","title":"<code>__init__(kernel_size=(3, 3), adapt_ts=100.0 / 255.0, neg_ts=0.1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>tuple</code> <p>size of the kernel used to select local maxima. Defaults to (3,3) (as in the paper).</p> <code>(3, 3)</code> <code>adapt_ts</code> <code>float</code> <p>adaptive threshold to select final points from candidates. Defaults to 100.0/255.0 (as in the paper).</p> <code>100.0 / 255.0</code> <code>neg_ts</code> <code>float</code> <p>negative sample threshold used to define if  an image is a negative sample or not. Defaults to 0.1 (as in the paper).</p> <code>0.1</code> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/lmds.py</code> <pre><code>def __init__(\n    self, \n    kernel_size: tuple = (3,3),\n    adapt_ts: float = 100.0/255.0, \n    neg_ts: float = 0.1\n    ) -&gt; None:\n    '''\n    Args:\n        kernel_size (tuple, optional): size of the kernel used to select local\n            maxima. Defaults to (3,3) (as in the paper).\n        adapt_ts (float, optional): adaptive threshold to select final points\n            from candidates. Defaults to 100.0/255.0 (as in the paper).\n        neg_ts (float, optional): negative sample threshold used to define if \n            an image is a negative sample or not. Defaults to 0.1 (as in the paper).\n    '''\n\n    assert kernel_size[0] == kernel_size[1], \\\n        f'The kernel shape must be a square, got {kernel_size[0]}x{kernel_size[1]}'\n    assert not kernel_size[0] % 2 == 0, \\\n        f'The kernel size must be odd, got {kernel_size[0]}'\n\n    self.kernel_size = tuple(kernel_size)\n    self.adapt_ts = adapt_ts\n    self.neg_ts = neg_ts\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/stitchers/","title":"Stitchers","text":""},{"location":"base/models/detection/herdnet/animaloc/eval/stitchers/#PytorchWildlife.models.detection.herdnet.animaloc.eval.stitchers.Stitcher","title":"<code>Stitcher</code>","text":"<p>               Bases: <code>ImageToPatches</code></p> <p>Class to stitch detections of patches into original image coordinates system </p> This algorithm works as follow <p>1) Cut original image into patches 2) Make inference on each patches and harvest the detections 3) Patch the detections maps into the coordinate system of the original image Optional: 4) Upsample the patched detection map</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/stitchers.py</code> <pre><code>class Stitcher(ImageToPatches):\n    ''' Class to stitch detections of patches into original image\n    coordinates system \n\n    This algorithm works as follow:\n        1) Cut original image into patches\n        2) Make inference on each patches and harvest the detections\n        3) Patch the detections maps into the coordinate system of the original image\n        Optional:\n        4) Upsample the patched detection map\n    '''\n\n    def __init__(\n        self,\n        model: torch.nn.Module, \n        size: Tuple[int,int], \n        overlap: int = 100,\n        batch_size: int = 1,\n        down_ratio: int = 1,\n        up: bool = False,\n        reduction: str = 'sum',\n        device_name: str = 'cuda',\n        ) -&gt; None:\n        '''\n        Args:\n            model (torch.nn.Module): CNN detection model, that takes as inputs image and returns\n                output and dict (i.e. wrapped by LossWrapper)\n            size (tuple): patches size (height, width), in pixels\n            overlap (int, optional): overlap between patches, in pixels. \n                Defaults to 100. \n            batch_size (int, optional): batch size used for inference over patches. \n                Defaults to 1.\n            down_ratio (int, optional): downsample ratio. Set to 1 to get output of the same \n                size as input (i.e. no downsample). Defaults to 1.\n            up (bool, optional): set to True to upsample the patched map. Defaults to False.\n            reduction (str, optional): specifies the reduction to apply on overlapping areas.\n                Possible values are 'sum', 'mean', 'max'. Defaults to 'sum'.\n            device_name (str, optional): the device name on which tensors will be allocated \n                ('cpu' or 'cuda'). Defaults to 'cuda'.\n        '''\n\n        assert isinstance(model, torch.nn.Module), \\\n            'model argument must be an instance of nn.Module()'\n\n        assert reduction in ['sum', 'mean', 'max'], \\\n            'reduction argument possible values are \\'sum\\', \\'mean\\' and \\'max\\' ' \\\n                f'got \\'{reduction}\\''\n\n        self.model = model\n        self.size = size\n        self.overlap = overlap\n        self.batch_size = batch_size\n        self.down_ratio = down_ratio\n        self.up = up\n        self.reduction = reduction\n        self.device = torch.device(device_name)\n\n        self.model.to(self.device)\n\n    def __call__(\n        self, \n        image: torch.Tensor\n        ) -&gt; torch.Tensor:\n        ''' Apply the stitching algorithm to the image\n\n        Args:\n            image (torch.Tensor): image of shape [C,H,W]\n\n        Returns:\n            torch.Tensor\n                the detections into the coordinate system of the original image\n        '''\n\n        super(Stitcher, self).__init__(image, self.size, self.overlap)\n\n        self.image = image.to(torch.device('cpu')) \n\n        # step 1 - get patches and limits\n        patches = self.make_patches()\n\n        # step 2 - inference to get maps\n        det_maps = self._inference(patches)\n\n        # step 3 - patch the maps into initial coordinates system\n        patched_map = self._patch_maps(det_maps)\n        patched_map = self._reduce(patched_map)\n\n        # (step 4 - upsample)\n        if self.up:\n            patched_map = F.interpolate(patched_map, scale_factor=self.down_ratio, \n                mode='bilinear', align_corners=True)\n\n        return patched_map\n\n\n    @torch.no_grad()\n    def _inference(self, patches: torch.Tensor) -&gt; List[torch.Tensor]:\n\n        self.model.eval()\n\n        dataset = TensorDataset(patches)\n        dataloader = DataLoader(\n            dataset,   \n            batch_size=self.batch_size,\n            sampler=SequentialSampler(dataset)\n            )\n\n        maps = []\n        for patch in dataloader:\n            patch = patch[0].to(self.device)\n            outputs, _ = self.model(patch)\n            maps = [*maps, *outputs.unsqueeze(0)]\n\n        return maps\n\n    def _patch_maps(self, maps: List[torch.Tensor]) -&gt; torch.Tensor:\n\n        _, h, w = self.image.shape\n        dh, dw = h // self.down_ratio, w // self.down_ratio\n        kernel_size = np.array(self.size) // self.down_ratio\n        stride = kernel_size - self.overlap // self.down_ratio\n        output_size = (\n            self._ncol * kernel_size[0] - ((self._ncol-1) * self.overlap // self.down_ratio), \n            self._nrow * kernel_size[1] - ((self._nrow-1) * self.overlap // self.down_ratio)\n            )\n\n        maps = torch.cat(maps, dim=0)\n\n        if self.reduction == 'max':\n            out_map = self._max_fold(maps, output_size=output_size,\n                kernel_size=tuple(kernel_size), stride=tuple(stride))\n        else:\n            n_patches = maps.shape[0]\n            maps = maps.permute(1,2,3,0).contiguous().view(1, -1, n_patches)\n            out_map = F.fold(maps, output_size=output_size, \n                kernel_size=tuple(kernel_size), stride=tuple(stride))\n\n        out_map = out_map[:,:, 0:dh, 0:dw]\n\n        return out_map\n\n    def _reduce(self, map: torch.Tensor) -&gt; torch.Tensor:\n\n        dh = self.image.shape[1] // self.down_ratio\n        dw = self.image.shape[2] // self.down_ratio\n        ones = torch.ones(self.image.shape[0],dh,dw)\n\n        if self.reduction == 'mean':\n            ones_patches = ImageToPatches(ones, \n                np.array(self.size)//self.down_ratio, \n                self.overlap//self.down_ratio\n                ).make_patches()\n\n            ones_patches = [p.unsqueeze(0).unsqueeze(0) for p in ones_patches[:,1,:,:]]\n            norm_map = self._patch_maps(ones_patches)\n\n        else:\n            norm_map = ones[1,:,:]\n\n        return torch.div(map.to(self.device), norm_map.to(self.device))\n\n    def _max_fold(self, maps: torch.Tensor, output_size: tuple, \n        kernel_size: tuple, stride: tuple\n        ) -&gt; torch.Tensor:\n\n        output = torch.zeros((1, maps.shape[1], *output_size))\n\n        fn = lambda x: [[i, i+kernel_size[x]] for i in range(0, output_size[x], stride[x])][:-1]\n        locs = [[*h, *w] for h in fn(0) for w in fn(1)]\n\n        for loc, m in zip(locs, maps):\n            patch = torch.zeros(output.shape)\n            patch[:,:, loc[0]:loc[1], loc[2]:loc[3]] = m\n            output = torch.max(output, patch)\n\n        return output\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/stitchers/#PytorchWildlife.models.detection.herdnet.animaloc.eval.stitchers.Stitcher.__call__","title":"<code>__call__(image)</code>","text":"<p>Apply the stitching algorithm to the image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>image of shape [C,H,W]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor the detections into the coordinate system of the original image</p> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/stitchers.py</code> <pre><code>def __call__(\n    self, \n    image: torch.Tensor\n    ) -&gt; torch.Tensor:\n    ''' Apply the stitching algorithm to the image\n\n    Args:\n        image (torch.Tensor): image of shape [C,H,W]\n\n    Returns:\n        torch.Tensor\n            the detections into the coordinate system of the original image\n    '''\n\n    super(Stitcher, self).__init__(image, self.size, self.overlap)\n\n    self.image = image.to(torch.device('cpu')) \n\n    # step 1 - get patches and limits\n    patches = self.make_patches()\n\n    # step 2 - inference to get maps\n    det_maps = self._inference(patches)\n\n    # step 3 - patch the maps into initial coordinates system\n    patched_map = self._patch_maps(det_maps)\n    patched_map = self._reduce(patched_map)\n\n    # (step 4 - upsample)\n    if self.up:\n        patched_map = F.interpolate(patched_map, scale_factor=self.down_ratio, \n            mode='bilinear', align_corners=True)\n\n    return patched_map\n</code></pre>"},{"location":"base/models/detection/herdnet/animaloc/eval/stitchers/#PytorchWildlife.models.detection.herdnet.animaloc.eval.stitchers.Stitcher.__init__","title":"<code>__init__(model, size, overlap=100, batch_size=1, down_ratio=1, up=False, reduction='sum', device_name='cuda')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>CNN detection model, that takes as inputs image and returns output and dict (i.e. wrapped by LossWrapper)</p> required <code>size</code> <code>tuple</code> <p>patches size (height, width), in pixels</p> required <code>overlap</code> <code>int</code> <p>overlap between patches, in pixels.  Defaults to 100. </p> <code>100</code> <code>batch_size</code> <code>int</code> <p>batch size used for inference over patches.  Defaults to 1.</p> <code>1</code> <code>down_ratio</code> <code>int</code> <p>downsample ratio. Set to 1 to get output of the same  size as input (i.e. no downsample). Defaults to 1.</p> <code>1</code> <code>up</code> <code>bool</code> <p>set to True to upsample the patched map. Defaults to False.</p> <code>False</code> <code>reduction</code> <code>str</code> <p>specifies the reduction to apply on overlapping areas. Possible values are 'sum', 'mean', 'max'. Defaults to 'sum'.</p> <code>'sum'</code> <code>device_name</code> <code>str</code> <p>the device name on which tensors will be allocated  ('cpu' or 'cuda'). Defaults to 'cuda'.</p> <code>'cuda'</code> Source code in <code>PytorchWildlife/models/detection/herdnet/animaloc/eval/stitchers.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module, \n    size: Tuple[int,int], \n    overlap: int = 100,\n    batch_size: int = 1,\n    down_ratio: int = 1,\n    up: bool = False,\n    reduction: str = 'sum',\n    device_name: str = 'cuda',\n    ) -&gt; None:\n    '''\n    Args:\n        model (torch.nn.Module): CNN detection model, that takes as inputs image and returns\n            output and dict (i.e. wrapped by LossWrapper)\n        size (tuple): patches size (height, width), in pixels\n        overlap (int, optional): overlap between patches, in pixels. \n            Defaults to 100. \n        batch_size (int, optional): batch size used for inference over patches. \n            Defaults to 1.\n        down_ratio (int, optional): downsample ratio. Set to 1 to get output of the same \n            size as input (i.e. no downsample). Defaults to 1.\n        up (bool, optional): set to True to upsample the patched map. Defaults to False.\n        reduction (str, optional): specifies the reduction to apply on overlapping areas.\n            Possible values are 'sum', 'mean', 'max'. Defaults to 'sum'.\n        device_name (str, optional): the device name on which tensors will be allocated \n            ('cpu' or 'cuda'). Defaults to 'cuda'.\n    '''\n\n    assert isinstance(model, torch.nn.Module), \\\n        'model argument must be an instance of nn.Module()'\n\n    assert reduction in ['sum', 'mean', 'max'], \\\n        'reduction argument possible values are \\'sum\\', \\'mean\\' and \\'max\\' ' \\\n            f'got \\'{reduction}\\''\n\n    self.model = model\n    self.size = size\n    self.overlap = overlap\n    self.batch_size = batch_size\n    self.down_ratio = down_ratio\n    self.up = up\n    self.reduction = reduction\n    self.device = torch.device(device_name)\n\n    self.model.to(self.device)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/Deepfaune/","title":"Deepfaune","text":"<p>This is a Pytorch-Wildlife loader for the Deepfaune detector. The original Deepfaune model is available at: https://www.deepfaune.cnrs.fr/en/ Licence: CC BY-SA 4.0 Copyright CNRS 2024 simon.chamaille@cefe.cnrs.fr; vincent.miele@univ-lyon1.fr</p>"},{"location":"base/models/detection/ultralytics_based/Deepfaune/#PytorchWildlife.models.detection.ultralytics_based.Deepfaune.DeepfauneDetector","title":"<code>DeepfauneDetector</code>","text":"<p>               Bases: <code>YOLOV8Base</code></p> <p>MegaDetectorV6 is a specialized class derived from the YOLOV8Base class  that is specifically designed for detecting animals, persons, and vehicles.</p> <p>Attributes:</p> Name Type Description <code>CLASS_NAMES</code> <code>dict</code> <p>Mapping of class IDs to their respective names.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/Deepfaune.py</code> <pre><code>class DeepfauneDetector(YOLOV8Base):\n    \"\"\"\n    MegaDetectorV6 is a specialized class derived from the YOLOV8Base class \n    that is specifically designed for detecting animals, persons, and vehicles.\n\n    Attributes:\n        CLASS_NAMES (dict): Mapping of class IDs to their respective names.\n    \"\"\"\n\n    CLASS_NAMES = {\n        0: \"animal\",\n        1: \"person\",\n        2: \"vehicle\"\n    }\n\n    def __init__(self, weights=None, device=\"cpu\"):\n        \"\"\"\n        Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n        Args:\n            weights (str, optional): Path to the weights file.\n            device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n        \"\"\"\n        self.IMAGE_SIZE = 960\n\n        url = \"https://pbil.univ-lyon1.fr/software/download/deepfaune/v1.3/deepfaune-yolov8s_960.pt\" \n        self.MODEL_NAME = \"deepfaune-yolov8s_960.pt\"\n\n        super(DeepfauneDetector, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/Deepfaune/#PytorchWildlife.models.detection.ultralytics_based.Deepfaune.DeepfauneDetector.__init__","title":"<code>__init__(weights=None, device='cpu')</code>","text":"<p>Initializes the MegaDetectorV5 model with the option to load pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the weights file.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".</p> <code>'cpu'</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/Deepfaune.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\"):\n    \"\"\"\n    Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n    Args:\n        weights (str, optional): Path to the weights file.\n        device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n    \"\"\"\n    self.IMAGE_SIZE = 960\n\n    url = \"https://pbil.univ-lyon1.fr/software/download/deepfaune/v1.3/deepfaune-yolov8s_960.pt\" \n    self.MODEL_NAME = \"deepfaune-yolov8s_960.pt\"\n\n    super(DeepfauneDetector, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv5/","title":"MegaDetector v5","text":""},{"location":"base/models/detection/ultralytics_based/megadetectorv5/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv5.MegaDetectorV5","title":"<code>MegaDetectorV5</code>","text":"<p>               Bases: <code>YOLOV5Base</code></p> <p>MegaDetectorV5 is a specialized class derived from the YOLOV5Base class  that is specifically designed for detecting animals, persons, and vehicles.</p> <p>Attributes:</p> Name Type Description <code>IMAGE_SIZE</code> <code>int</code> <p>The standard image size used during training.</p> <code>STRIDE</code> <code>int</code> <p>Stride value used in the detector.</p> <code>CLASS_NAMES</code> <code>dict</code> <p>Mapping of class IDs to their respective names.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv5.py</code> <pre><code>class MegaDetectorV5(YOLOV5Base):\n    \"\"\"\n    MegaDetectorV5 is a specialized class derived from the YOLOV5Base class \n    that is specifically designed for detecting animals, persons, and vehicles.\n\n    Attributes:\n        IMAGE_SIZE (int): The standard image size used during training.\n        STRIDE (int): Stride value used in the detector.\n        CLASS_NAMES (dict): Mapping of class IDs to their respective names.\n    \"\"\"\n\n    IMAGE_SIZE = 1280  # image size used in training\n    STRIDE = 64\n    CLASS_NAMES = {\n        0: \"animal\",\n        1: \"person\",\n        2: \"vehicle\"\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True, version=\"a\"):\n        \"\"\"\n        Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n        Args:\n            weights (str, optional): Path to the weights file.\n            device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n            pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n            version (str, optional): Version of the MegaDetectorV5 model to load. Default is \"a\".\n        \"\"\"\n\n        if pretrained:\n            if version == \"a\":\n                url = \"https://zenodo.org/records/13357337/files/md_v5a.0.0.pt?download=1\"\n            elif version == \"b\":\n                url = \"https://zenodo.org/records/10023414/files/MegaDetector_v5b.0.0.pt?download=1\"\n        else:\n            url = None\n\n        import site \n        import sys\n        sys.path.insert(0, site.getsitepackages()[0]+'/yolov5')\n\n        super(MegaDetectorV5, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv5/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv5.MegaDetectorV5.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True, version='a')</code>","text":"<p>Initializes the MegaDetectorV5 model with the option to load pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the weights file.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained model. Default is True.</p> <code>True</code> <code>version</code> <code>str</code> <p>Version of the MegaDetectorV5 model to load. Default is \"a\".</p> <code>'a'</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv5.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True, version=\"a\"):\n    \"\"\"\n    Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n    Args:\n        weights (str, optional): Path to the weights file.\n        device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n        pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n        version (str, optional): Version of the MegaDetectorV5 model to load. Default is \"a\".\n    \"\"\"\n\n    if pretrained:\n        if version == \"a\":\n            url = \"https://zenodo.org/records/13357337/files/md_v5a.0.0.pt?download=1\"\n        elif version == \"b\":\n            url = \"https://zenodo.org/records/10023414/files/MegaDetector_v5b.0.0.pt?download=1\"\n    else:\n        url = None\n\n    import site \n    import sys\n    sys.path.insert(0, site.getsitepackages()[0]+'/yolov5')\n\n    super(MegaDetectorV5, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv6/","title":"MegaDetector v6","text":""},{"location":"base/models/detection/ultralytics_based/megadetectorv6/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv6.MegaDetectorV6","title":"<code>MegaDetectorV6</code>","text":"<p>               Bases: <code>YOLOV8Base</code></p> <p>MegaDetectorV6 is a specialized class derived from the YOLOV8Base class  that is specifically designed for detecting animals, persons, and vehicles.</p> <p>Attributes:</p> Name Type Description <code>CLASS_NAMES</code> <code>dict</code> <p>Mapping of class IDs to their respective names.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv6.py</code> <pre><code>class MegaDetectorV6(YOLOV8Base):\n    \"\"\"\n    MegaDetectorV6 is a specialized class derived from the YOLOV8Base class \n    that is specifically designed for detecting animals, persons, and vehicles.\n\n    Attributes:\n        CLASS_NAMES (dict): Mapping of class IDs to their respective names.\n    \"\"\"\n\n    CLASS_NAMES = {\n        0: \"animal\",\n        1: \"person\",\n        2: \"vehicle\"\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True, version='yolov9c'):\n        \"\"\"\n        Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n        Args:\n            weights (str, optional): Path to the weights file.\n            device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n            pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n            version (str, optional): Version of the model to load. Default is 'yolov9c'.\n        \"\"\"\n        self.IMAGE_SIZE = 1280\n\n        if version == 'MDV6-yolov9-c':            \n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-c.pt?download=1\" \n            self.MODEL_NAME = \"MDV6b-yolov9-c.pt\"\n        elif version == 'MDV6-yolov9-e':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-e-1280.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov9-e-1280.pt\"\n        elif version == 'MDV6-yolov10-c':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-c.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov10-c.pt\"\n        elif version == 'MDV6-yolov10-e':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-e-1280.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov10-e-1280.pt\"\n        elif version == 'MDV6-rtdetr-c':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-rtdetr-c.pt?download=1\"\n            self.MODEL_NAME = \"MDV6b-rtdetr-c.pt\"\n        else:\n            raise ValueError('Select a valid model version: MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e, MDV6-rtdetr-c')\n\n        super(MegaDetectorV6, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv6/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv6.MegaDetectorV6.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True, version='yolov9c')</code>","text":"<p>Initializes the MegaDetectorV5 model with the option to load pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the weights file.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained model. Default is True.</p> <code>True</code> <code>version</code> <code>str</code> <p>Version of the model to load. Default is 'yolov9c'.</p> <code>'yolov9c'</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv6.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True, version='yolov9c'):\n    \"\"\"\n    Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n    Args:\n        weights (str, optional): Path to the weights file.\n        device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n        pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n        version (str, optional): Version of the model to load. Default is 'yolov9c'.\n    \"\"\"\n    self.IMAGE_SIZE = 1280\n\n    if version == 'MDV6-yolov9-c':            \n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-c.pt?download=1\" \n        self.MODEL_NAME = \"MDV6b-yolov9-c.pt\"\n    elif version == 'MDV6-yolov9-e':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-e-1280.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov9-e-1280.pt\"\n    elif version == 'MDV6-yolov10-c':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-c.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov10-c.pt\"\n    elif version == 'MDV6-yolov10-e':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-e-1280.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov10-e-1280.pt\"\n    elif version == 'MDV6-rtdetr-c':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-rtdetr-c.pt?download=1\"\n        self.MODEL_NAME = \"MDV6b-rtdetr-c.pt\"\n    else:\n        raise ValueError('Select a valid model version: MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e, MDV6-rtdetr-c')\n\n    super(MegaDetectorV6, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv6_distributed/","title":"MegaDetector v6 Distributed","text":""},{"location":"base/models/detection/ultralytics_based/megadetectorv6_distributed/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv6_distributed.MegaDetectorV6_Distributed","title":"<code>MegaDetectorV6_Distributed</code>","text":"<p>               Bases: <code>YOLOV8_Distributed</code></p> <p>MegaDetectorV6 is a specialized class derived from the YOLOV8Base class  that is specifically designed for detecting animals, persons, and vehicles.</p> <p>Attributes:</p> Name Type Description <code>CLASS_NAMES</code> <code>dict</code> <p>Mapping of class IDs to their respective names.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv6_distributed.py</code> <pre><code>class MegaDetectorV6_Distributed(YOLOV8_Distributed):\n    \"\"\"\n    MegaDetectorV6 is a specialized class derived from the YOLOV8Base class \n    that is specifically designed for detecting animals, persons, and vehicles.\n\n    Attributes:\n        CLASS_NAMES (dict): Mapping of class IDs to their respective names.\n    \"\"\"\n\n    CLASS_NAMES = {\n        0: \"animal\",\n        1: \"person\",\n        2: \"vehicle\"\n    }\n\n    def __init__(self, weights=None, device=\"cpu\", pretrained=True, version='yolov9c'):\n        \"\"\"\n        Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n        Args:\n            weights (str, optional): Path to the weights file.\n            device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n            pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n            version (str, optional): Version of the model to load. Default is 'yolov9c'.\n        \"\"\"\n        self.IMAGE_SIZE = 1280\n\n        if version == 'MDV6-yolov9-c':            \n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-c.pt?download=1\" \n            self.MODEL_NAME = \"MDV6b-yolov9-c.pt\"\n        elif version == 'MDV6-yolov9-e':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-e-1280.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov9-e-1280.pt\"\n        elif version == 'MDV6-yolov10-c':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-c.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov10-c.pt\"\n        elif version == 'MDV6-yolov10-e':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-e-1280.pt?download=1\"\n            self.MODEL_NAME = \"MDV6-yolov10-e-1280.pt\"\n        elif version == 'MDV6-rtdetr-c':\n            url = \"https://zenodo.org/records/15398270/files/MDV6-rtdetr-c.pt?download=1\"\n            self.MODEL_NAME = \"MDV6b-rtdetr-c.pt\"\n        else:\n            raise ValueError('Select a valid model version: MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e, MDV6-rtdetr-c')\n\n        super(MegaDetectorV6_Distributed, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/megadetectorv6_distributed/#PytorchWildlife.models.detection.ultralytics_based.megadetectorv6_distributed.MegaDetectorV6_Distributed.__init__","title":"<code>__init__(weights=None, device='cpu', pretrained=True, version='yolov9c')</code>","text":"<p>Initializes the MegaDetectorV5 model with the option to load pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the weights file.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".</p> <code>'cpu'</code> <code>pretrained</code> <code>bool</code> <p>Whether to load the pretrained model. Default is True.</p> <code>True</code> <code>version</code> <code>str</code> <p>Version of the model to load. Default is 'yolov9c'.</p> <code>'yolov9c'</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/megadetectorv6_distributed.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", pretrained=True, version='yolov9c'):\n    \"\"\"\n    Initializes the MegaDetectorV5 model with the option to load pretrained weights.\n\n    Args:\n        weights (str, optional): Path to the weights file.\n        device (str, optional): Device to load the model on (e.g., \"cpu\" or \"cuda\"). Default is \"cpu\".\n        pretrained (bool, optional): Whether to load the pretrained model. Default is True.\n        version (str, optional): Version of the model to load. Default is 'yolov9c'.\n    \"\"\"\n    self.IMAGE_SIZE = 1280\n\n    if version == 'MDV6-yolov9-c':            \n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-c.pt?download=1\" \n        self.MODEL_NAME = \"MDV6b-yolov9-c.pt\"\n    elif version == 'MDV6-yolov9-e':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov9-e-1280.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov9-e-1280.pt\"\n    elif version == 'MDV6-yolov10-c':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-c.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov10-c.pt\"\n    elif version == 'MDV6-yolov10-e':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-yolov10-e-1280.pt?download=1\"\n        self.MODEL_NAME = \"MDV6-yolov10-e-1280.pt\"\n    elif version == 'MDV6-rtdetr-c':\n        url = \"https://zenodo.org/records/15398270/files/MDV6-rtdetr-c.pt?download=1\"\n        self.MODEL_NAME = \"MDV6b-rtdetr-c.pt\"\n    else:\n        raise ValueError('Select a valid model version: MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e, MDV6-rtdetr-c')\n\n    super(MegaDetectorV6_Distributed, self).__init__(weights=weights, device=device, url=url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/","title":"YOLOv5 Base","text":"<p>YoloV5 base detector class.</p>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/#PytorchWildlife.models.detection.ultralytics_based.yolov5_base.YOLOV5Base","title":"<code>YOLOV5Base</code>","text":"<p>               Bases: <code>BaseDetector</code></p> <p>Base detector class for YOLO V5. This class provides utility methods for loading the model, generating results, and performing single and batch image detections.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov5_base.py</code> <pre><code>class YOLOV5Base(BaseDetector):\n    \"\"\"\n    Base detector class for YOLO V5. This class provides utility methods for\n    loading the model, generating results, and performing single and batch image detections.\n    \"\"\"\n    def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n        \"\"\"\n        Initialize the YOLO V5 detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n            transform (callable, optional):\n                Optional transform to be applied on the image. Defaults to None.\n        \"\"\"\n        self.transform = transform\n        super(YOLOV5Base, self).__init__(weights=weights, device=device, url=url)\n        self._load_model(weights, device, url)\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load the YOLO V5 model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n        if weights:\n            checkpoint = torch.load(weights, map_location=torch.device(device))\n        elif url:\n            checkpoint = load_state_dict_from_url(url, map_location=torch.device(self.device))\n        else:\n            raise Exception(\"Need weights for inference.\")\n        self.model = checkpoint[\"model\"].float().fuse().eval().to(self.device)\n\n        if not self.transform:\n            self.transform = pw_trans.MegaDetector_v5_Transform(target_size=self.IMAGE_SIZE,\n                                                                stride=self.STRIDE)\n\n    def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (numpy.ndarray): \n                Model predictions.\n            img_id (str): \n                Image identifier.\n            id_strip (str, optional): \n                Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        results = {\"img_id\": str(img_id).strip(id_strip)}\n        results[\"detections\"] = sv.Detections(\n            xyxy=preds[:, :4],\n            confidence=preds[:, 4],\n            class_id=preds[:, 5].astype(int)\n        )\n        results[\"labels\"] = [\n            f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"\n            for confidence, class_id in zip(results[\"detections\"].confidence, results[\"detections\"].class_id)\n        ]\n        return results\n\n    def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, id_strip=None) -&gt; dict:\n        \"\"\"\n        Perform detection on a single image.\n\n        Args:\n            img (str or ndarray): \n                Image path or ndarray of images.\n            img_path (str, optional): \n                Image path or identifier.\n            det_conf_thres (float, optional): \n                Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): \n                Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            dict: Detection results.\n        \"\"\"\n        if type(img) == str:\n            if img_path is None:\n                img_path = img\n            img = np.array(Image.open(img_path).convert(\"RGB\"))\n        img_size = img.shape\n        img = self.transform(img)\n\n        if img_size is None:\n            img_size = img.permute((1, 2, 0)).shape # We need hwc instead of chw for coord scaling\n        preds = self.model(img.unsqueeze(0).to(self.device))[0]\n        preds = torch.cat(non_max_suppression(prediction=preds, conf_thres=det_conf_thres), axis=0).cpu().numpy()\n        # preds[:, :4] = scale_coords([self.IMAGE_SIZE] * 2, preds[:, :4], img_size).round()\n        preds[:, :4] = scale_boxes([self.IMAGE_SIZE] * 2, preds[:, :4], img_size).round()\n        res = self.results_generation(preds, img_path, id_strip)\n\n        normalized_coords = [[x1 / img_size[1], y1 / img_size[0], x2 / img_size[1], y2 / img_size[0]] for x1, y1, x2, y2 in preds[:, :4]]\n        res[\"normalized_coords\"] = normalized_coords\n\n        return res\n\n    def batch_image_detection(self, data_path, batch_size: int = 16, det_conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Perform detection on a batch of images.\n\n        Args:\n            data_path (str): Path containing all images for inference.\n            batch_size (int, optional): Batch size for inference. Defaults to 16.\n            det_conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            list[dict]: List of detection results for all images.\n        \"\"\"\n\n        dataset = pw_data.DetectionImageFolder(\n            data_path,\n            transform=self.transform,\n        )\n\n        # Creating a DataLoader for batching and parallel processing of the images\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                            pin_memory=True, num_workers=0, drop_last=False)\n\n        results = []\n        with tqdm(total=len(loader)) as pbar:\n            for batch_index, (imgs, paths, sizes) in enumerate(loader):\n                imgs = imgs.to(self.device)\n                predictions = self.model(imgs)[0].detach().cpu()\n                predictions = non_max_suppression(predictions, conf_thres=det_conf_thres)\n\n                batch_results = []\n                for i, pred in enumerate(predictions):\n                    if pred.size(0) == 0:  \n                        continue\n                    pred = pred.numpy()\n                    size = sizes[i].numpy()\n                    path = paths[i]\n                    original_coords = pred[:, :4].copy()\n                    # pred[:, :4] = scale_coords([self.IMAGE_SIZE] * 2, pred[:, :4], size).round()\n                    pred[:, :4] = scale_boxes([self.IMAGE_SIZE] * 2, pred[:, :4], size).round()\n                    # Normalize the coordinates for timelapse compatibility\n                    normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in pred[:, :4]]\n                    res = self.results_generation(pred, path, id_strip)\n                    res[\"normalized_coords\"] = normalized_coords\n                    batch_results.append(res)\n                pbar.update(1)\n                results.extend(batch_results)\n            return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/#PytorchWildlife.models.detection.ultralytics_based.yolov5_base.YOLOV5Base.__init__","title":"<code>__init__(weights=None, device='cpu', url=None, transform=None)</code>","text":"<p>Initialize the YOLO V5 detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> <code>transform</code> <code>callable</code> <p>Optional transform to be applied on the image. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov5_base.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n    \"\"\"\n    Initialize the YOLO V5 detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n        transform (callable, optional):\n            Optional transform to be applied on the image. Defaults to None.\n    \"\"\"\n    self.transform = transform\n    super(YOLOV5Base, self).__init__(weights=weights, device=device, url=url)\n    self._load_model(weights, device, url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/#PytorchWildlife.models.detection.ultralytics_based.yolov5_base.YOLOV5Base.batch_image_detection","title":"<code>batch_image_detection(data_path, batch_size=16, det_conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path containing all images for inference.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 16.</p> <code>16</code> <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of detection results for all images.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov5_base.py</code> <pre><code>def batch_image_detection(self, data_path, batch_size: int = 16, det_conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Perform detection on a batch of images.\n\n    Args:\n        data_path (str): Path containing all images for inference.\n        batch_size (int, optional): Batch size for inference. Defaults to 16.\n        det_conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        list[dict]: List of detection results for all images.\n    \"\"\"\n\n    dataset = pw_data.DetectionImageFolder(\n        data_path,\n        transform=self.transform,\n    )\n\n    # Creating a DataLoader for batching and parallel processing of the images\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                        pin_memory=True, num_workers=0, drop_last=False)\n\n    results = []\n    with tqdm(total=len(loader)) as pbar:\n        for batch_index, (imgs, paths, sizes) in enumerate(loader):\n            imgs = imgs.to(self.device)\n            predictions = self.model(imgs)[0].detach().cpu()\n            predictions = non_max_suppression(predictions, conf_thres=det_conf_thres)\n\n            batch_results = []\n            for i, pred in enumerate(predictions):\n                if pred.size(0) == 0:  \n                    continue\n                pred = pred.numpy()\n                size = sizes[i].numpy()\n                path = paths[i]\n                original_coords = pred[:, :4].copy()\n                # pred[:, :4] = scale_coords([self.IMAGE_SIZE] * 2, pred[:, :4], size).round()\n                pred[:, :4] = scale_boxes([self.IMAGE_SIZE] * 2, pred[:, :4], size).round()\n                # Normalize the coordinates for timelapse compatibility\n                normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in pred[:, :4]]\n                res = self.results_generation(pred, path, id_strip)\n                res[\"normalized_coords\"] = normalized_coords\n                batch_results.append(res)\n            pbar.update(1)\n            results.extend(batch_results)\n        return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/#PytorchWildlife.models.detection.ultralytics_based.yolov5_base.YOLOV5Base.results_generation","title":"<code>results_generation(preds, img_id, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>ndarray</code> <p>Model predictions.</p> required <code>img_id</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov5_base.py</code> <pre><code>def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (numpy.ndarray): \n            Model predictions.\n        img_id (str): \n            Image identifier.\n        id_strip (str, optional): \n            Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    results = {\"img_id\": str(img_id).strip(id_strip)}\n    results[\"detections\"] = sv.Detections(\n        xyxy=preds[:, :4],\n        confidence=preds[:, 4],\n        class_id=preds[:, 5].astype(int)\n    )\n    results[\"labels\"] = [\n        f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"\n        for confidence, class_id in zip(results[\"detections\"].confidence, results[\"detections\"].class_id)\n    ]\n    return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov5_base/#PytorchWildlife.models.detection.ultralytics_based.yolov5_base.YOLOV5Base.single_image_detection","title":"<code>single_image_detection(img, img_path=None, det_conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image path or ndarray of images.</p> required <code>img_path</code> <code>str</code> <p>Image path or identifier.</p> <code>None</code> <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Detection results.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov5_base.py</code> <pre><code>def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, id_strip=None) -&gt; dict:\n    \"\"\"\n    Perform detection on a single image.\n\n    Args:\n        img (str or ndarray): \n            Image path or ndarray of images.\n        img_path (str, optional): \n            Image path or identifier.\n        det_conf_thres (float, optional): \n            Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): \n            Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        dict: Detection results.\n    \"\"\"\n    if type(img) == str:\n        if img_path is None:\n            img_path = img\n        img = np.array(Image.open(img_path).convert(\"RGB\"))\n    img_size = img.shape\n    img = self.transform(img)\n\n    if img_size is None:\n        img_size = img.permute((1, 2, 0)).shape # We need hwc instead of chw for coord scaling\n    preds = self.model(img.unsqueeze(0).to(self.device))[0]\n    preds = torch.cat(non_max_suppression(prediction=preds, conf_thres=det_conf_thres), axis=0).cpu().numpy()\n    # preds[:, :4] = scale_coords([self.IMAGE_SIZE] * 2, preds[:, :4], img_size).round()\n    preds[:, :4] = scale_boxes([self.IMAGE_SIZE] * 2, preds[:, :4], img_size).round()\n    res = self.results_generation(preds, img_path, id_strip)\n\n    normalized_coords = [[x1 / img_size[1], y1 / img_size[0], x2 / img_size[1], y2 / img_size[0]] for x1, y1, x2, y2 in preds[:, :4]]\n    res[\"normalized_coords\"] = normalized_coords\n\n    return res\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/","title":"YOLOv8 Base","text":"<p>YoloV8 base detector class.</p>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/#PytorchWildlife.models.detection.ultralytics_based.yolov8_base.YOLOV8Base","title":"<code>YOLOV8Base</code>","text":"<p>               Bases: <code>BaseDetector</code></p> <p>Base detector class for the new ultralytics YOLOV8 framework. This class provides utility methods for loading the model, generating results, and performing single and batch image detections. This base detector class is also compatible with all the new ultralytics models including YOLOV9,  RTDetr, and more.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_base.py</code> <pre><code>class YOLOV8Base(BaseDetector):\n    \"\"\"\n    Base detector class for the new ultralytics YOLOV8 framework. This class provides utility methods for\n    loading the model, generating results, and performing single and batch image detections.\n    This base detector class is also compatible with all the new ultralytics models including YOLOV9, \n    RTDetr, and more.\n    \"\"\"\n    def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n        \"\"\"\n        Initialize the YOLOV8 detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        \"\"\"\n        super(YOLOV8Base, self).__init__(weights=weights, device=device, url=url)\n        self.transform = transform\n        self._load_model(weights, self.device, url)\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load the YOLOV8 model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n\n        if self.MODEL_NAME == 'MDV6b-rtdetrl.pt':\n            self.predictor = rtdetr.RTDETRPredictor()\n        else:\n            self.predictor = yolo.detect.DetectionPredictor()\n        # self.predictor.args.device = device # Will uncomment later\n        self.predictor.args.imgsz = self.IMAGE_SIZE\n        self.predictor.args.save = False # Will see if we want to use ultralytics native inference saving functions.\n\n        if weights:\n            self.predictor.setup_model(weights)\n        elif url:\n            if not os.path.exists(os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)):\n                os.makedirs(os.path.join(torch.hub.get_dir(), \"checkpoints\"), exist_ok=True)\n                weights = wget.download(url, out=os.path.join(torch.hub.get_dir(), \"checkpoints\"))\n            else:\n                weights = os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)\n            self.predictor.setup_model(weights)\n        else:\n            raise Exception(\"Need weights for inference.\")\n\n        if not self.transform:\n            self.transform = pw_trans.MegaDetector_v5_Transform(target_size=self.IMAGE_SIZE,\n                                                                stride=self.STRIDE)\n\n    def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (ultralytics.engine.results.Results): \n                Model predictions.\n            img_id (str): \n                Image identifier.\n            id_strip (str, optional): \n                Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        xyxy = preds.boxes.xyxy.cpu().numpy()\n        confidence = preds.boxes.conf.cpu().numpy()\n        class_id = preds.boxes.cls.cpu().numpy().astype(int)\n\n        results = {\"img_id\": str(img_id).strip(id_strip)}\n        results[\"detections\"] = sv.Detections(\n            xyxy=xyxy,\n            confidence=confidence,\n            class_id=class_id\n        )\n\n        results[\"labels\"] = [\n            f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"  \n            for _, _, confidence, class_id, _, _ in results[\"detections\"] \n        ]\n\n        return results\n\n\n    def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, id_strip=None) -&gt; dict:\n        \"\"\"\n        Perform detection on a single image.\n\n        Args:\n            img (str or ndarray): \n                Image path or ndarray of images.\n            img_path (str, optional): \n                Image path or identifier.\n            det_conf_thres (float, optional): \n                Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): \n                Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            dict: Detection results.\n        \"\"\"\n\n        if type(img) == str:\n            if img_path is None:\n                img_path = img\n            img = np.array(Image.open(img_path).convert(\"RGB\"))\n        img_size = img.shape\n\n        self.predictor.args.batch = 1\n        self.predictor.args.conf = det_conf_thres\n\n        det_results = list(self.predictor.stream_inference([img]))\n\n        res = self.results_generation(det_results[0], img_path, id_strip)\n\n        normalized_coords = [[x1 / img_size[1], y1 / img_size[0], x2 / img_size[1], y2 / img_size[0]] \n                             for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n        res[\"normalized_coords\"] = normalized_coords\n\n        return res\n\n    def batch_image_detection(self, data_source, batch_size: int = 16, det_conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n        \"\"\"\n        Perform detection on a batch of images.\n\n        Args:\n            data_source (str or List[np.ndarray]): Either path containing images for inference or list of numpy arrays (RGB format, shape: H\u00d7W\u00d73).\n            batch_size (int, optional): Batch size for inference. Defaults to 16.\n            det_conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n            id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n        Returns:\n            list[dict]: List of detection results for all images.\n        \"\"\"\n        self.predictor.args.batch = batch_size\n        self.predictor.args.conf = det_conf_thres\n\n        # Handle numpy array input\n        if isinstance(data_source, (list, np.ndarray)):\n            results = []\n            num_batches = (len(data_source) + batch_size - 1) // batch_size  # Calculate total batches\n\n            with tqdm(total=num_batches) as pbar:\n                for start_idx in range(0, len(data_source), batch_size):\n                    batch_arrays = data_source[start_idx:start_idx + batch_size]\n                    det_results = self.predictor.stream_inference(batch_arrays)\n\n                    for idx, preds in enumerate(det_results):\n                        res = self.results_generation(preds, f\"{start_idx + idx}\", id_strip)\n                        # Get size directly from numpy array\n                        img_height, img_width = batch_arrays[idx].shape[:2]\n                        normalized_coords = [[x1/img_width, y1/img_height, x2/img_width, y2/img_height] \n                                        for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n                        res[\"normalized_coords\"] = normalized_coords\n                        results.append(res)\n                    pbar.update(1)\n            return results\n\n        # Handle image directory input\n        dataset = pw_data.DetectionImageFolder(\n            data_source,\n            transform=self.transform,\n        )\n\n        # Creating a DataLoader for batching and parallel processing of the images\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                            pin_memory=True, num_workers=0, drop_last=False\n                            )\n\n        results = []\n        with tqdm(total=len(loader)) as pbar:\n            for batch_index, (imgs, paths, sizes) in enumerate(loader):\n                det_results = self.predictor.stream_inference(paths)\n                batch_results = []\n                for idx, preds in enumerate(det_results):\n                    res = self.results_generation(preds, paths[idx], id_strip)\n                    size = preds.orig_shape\n                    # Normalize the coordinates for timelapse compatibility\n                    normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n                    res[\"normalized_coords\"] = normalized_coords\n                    results.append(res)\n                pbar.update(1)\n                results.extend(batch_results)\n        return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/#PytorchWildlife.models.detection.ultralytics_based.yolov8_base.YOLOV8Base.__init__","title":"<code>__init__(weights=None, device='cpu', url=None, transform=None)</code>","text":"<p>Initialize the YOLOV8 detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_base.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n    \"\"\"\n    Initialize the YOLOV8 detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n    \"\"\"\n    super(YOLOV8Base, self).__init__(weights=weights, device=device, url=url)\n    self.transform = transform\n    self._load_model(weights, self.device, url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/#PytorchWildlife.models.detection.ultralytics_based.yolov8_base.YOLOV8Base.batch_image_detection","title":"<code>batch_image_detection(data_source, batch_size=16, det_conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str or List[ndarray]</code> <p>Either path containing images for inference or list of numpy arrays (RGB format, shape: H\u00d7W\u00d73).</p> required <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 16.</p> <code>16</code> <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of detection results for all images.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_base.py</code> <pre><code>def batch_image_detection(self, data_source, batch_size: int = 16, det_conf_thres: float = 0.2, id_strip: str = None) -&gt; list[dict]:\n    \"\"\"\n    Perform detection on a batch of images.\n\n    Args:\n        data_source (str or List[np.ndarray]): Either path containing images for inference or list of numpy arrays (RGB format, shape: H\u00d7W\u00d73).\n        batch_size (int, optional): Batch size for inference. Defaults to 16.\n        det_conf_thres (float, optional): Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        list[dict]: List of detection results for all images.\n    \"\"\"\n    self.predictor.args.batch = batch_size\n    self.predictor.args.conf = det_conf_thres\n\n    # Handle numpy array input\n    if isinstance(data_source, (list, np.ndarray)):\n        results = []\n        num_batches = (len(data_source) + batch_size - 1) // batch_size  # Calculate total batches\n\n        with tqdm(total=num_batches) as pbar:\n            for start_idx in range(0, len(data_source), batch_size):\n                batch_arrays = data_source[start_idx:start_idx + batch_size]\n                det_results = self.predictor.stream_inference(batch_arrays)\n\n                for idx, preds in enumerate(det_results):\n                    res = self.results_generation(preds, f\"{start_idx + idx}\", id_strip)\n                    # Get size directly from numpy array\n                    img_height, img_width = batch_arrays[idx].shape[:2]\n                    normalized_coords = [[x1/img_width, y1/img_height, x2/img_width, y2/img_height] \n                                    for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n                    res[\"normalized_coords\"] = normalized_coords\n                    results.append(res)\n                pbar.update(1)\n        return results\n\n    # Handle image directory input\n    dataset = pw_data.DetectionImageFolder(\n        data_source,\n        transform=self.transform,\n    )\n\n    # Creating a DataLoader for batching and parallel processing of the images\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n                        pin_memory=True, num_workers=0, drop_last=False\n                        )\n\n    results = []\n    with tqdm(total=len(loader)) as pbar:\n        for batch_index, (imgs, paths, sizes) in enumerate(loader):\n            det_results = self.predictor.stream_inference(paths)\n            batch_results = []\n            for idx, preds in enumerate(det_results):\n                res = self.results_generation(preds, paths[idx], id_strip)\n                size = preds.orig_shape\n                # Normalize the coordinates for timelapse compatibility\n                normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n                res[\"normalized_coords\"] = normalized_coords\n                results.append(res)\n            pbar.update(1)\n            results.extend(batch_results)\n    return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/#PytorchWildlife.models.detection.ultralytics_based.yolov8_base.YOLOV8Base.results_generation","title":"<code>results_generation(preds, img_id, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Results</code> <p>Model predictions.</p> required <code>img_id</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_base.py</code> <pre><code>def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (ultralytics.engine.results.Results): \n            Model predictions.\n        img_id (str): \n            Image identifier.\n        id_strip (str, optional): \n            Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    xyxy = preds.boxes.xyxy.cpu().numpy()\n    confidence = preds.boxes.conf.cpu().numpy()\n    class_id = preds.boxes.cls.cpu().numpy().astype(int)\n\n    results = {\"img_id\": str(img_id).strip(id_strip)}\n    results[\"detections\"] = sv.Detections(\n        xyxy=xyxy,\n        confidence=confidence,\n        class_id=class_id\n    )\n\n    results[\"labels\"] = [\n        f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"  \n        for _, _, confidence, class_id, _, _ in results[\"detections\"] \n    ]\n\n    return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_base/#PytorchWildlife.models.detection.ultralytics_based.yolov8_base.YOLOV8Base.single_image_detection","title":"<code>single_image_detection(img, img_path=None, det_conf_thres=0.2, id_strip=None)</code>","text":"<p>Perform detection on a single image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>str or ndarray</code> <p>Image path or ndarray of images.</p> required <code>img_path</code> <code>str</code> <p>Image path or identifier.</p> <code>None</code> <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.2.</p> <code>0.2</code> <code>id_strip</code> <code>str</code> <p>Characters to strip from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Detection results.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_base.py</code> <pre><code>def single_image_detection(self, img, img_path=None, det_conf_thres=0.2, id_strip=None) -&gt; dict:\n    \"\"\"\n    Perform detection on a single image.\n\n    Args:\n        img (str or ndarray): \n            Image path or ndarray of images.\n        img_path (str, optional): \n            Image path or identifier.\n        det_conf_thres (float, optional): \n            Confidence threshold for predictions. Defaults to 0.2.\n        id_strip (str, optional): \n            Characters to strip from img_id. Defaults to None.\n\n    Returns:\n        dict: Detection results.\n    \"\"\"\n\n    if type(img) == str:\n        if img_path is None:\n            img_path = img\n        img = np.array(Image.open(img_path).convert(\"RGB\"))\n    img_size = img.shape\n\n    self.predictor.args.batch = 1\n    self.predictor.args.conf = det_conf_thres\n\n    det_results = list(self.predictor.stream_inference([img]))\n\n    res = self.results_generation(det_results[0], img_path, id_strip)\n\n    normalized_coords = [[x1 / img_size[1], y1 / img_size[0], x2 / img_size[1], y2 / img_size[0]] \n                         for x1, y1, x2, y2 in res[\"detections\"].xyxy]\n    res[\"normalized_coords\"] = normalized_coords\n\n    return res\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_distributed/","title":"YOLOv8 Distributed","text":"<p>\" YoloV8 base detector class. Modified to support PyTorch DDP framework</p>"},{"location":"base/models/detection/ultralytics_based/yolov8_distributed/#PytorchWildlife.models.detection.ultralytics_based.yolov8_distributed.YOLOV8_Distributed","title":"<code>YOLOV8_Distributed</code>","text":"<p>               Bases: <code>BaseDetector</code></p> <p>Distributed YoloV8 detector class. This class provides utility methods for loading the model, generating results, and performing batch image detections.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_distributed.py</code> <pre><code>class YOLOV8_Distributed(BaseDetector):\n    \"\"\"\n    Distributed YoloV8 detector class.\n    This class provides utility methods for loading the model, generating results,\n    and performing batch image detections.\n    \"\"\"\n\n    def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n        \"\"\"\n        Initialize the YOLOV8 detector.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        \"\"\"\n        self.transform = transform\n        super(YOLOV8_Distributed, self).__init__(weights=weights, device=device, url=url)\n        self._load_model(weights, self.device, url)\n\n    def _load_model(self, weights=None, device=\"cpu\", url=None):\n        \"\"\"\n        Load the YOLOV8 model weights.\n\n        Args:\n            weights (str, optional): \n                Path to the model weights. Defaults to None.\n            device (str, optional): \n                Device for model inference. Defaults to \"cpu\".\n            url (str, optional): \n                URL to fetch the model weights. Defaults to None.\n        Raises:\n            Exception: If weights are not provided.\n        \"\"\"\n\n        if self.MODEL_NAME == 'MDV6b-rtdetrl.pt':\n            self.predictor = rtdetr.RTDETRPredictor()\n        else:\n            self.predictor = yolo.detect.DetectionPredictor()\n        # self.predictor.args.device = device # Will uncomment later\n        self.predictor.args.imgsz = self.IMAGE_SIZE\n        self.predictor.args.save = False # Will see if we want to use ultralytics native inference saving functions.\n\n        if weights:\n            self.predictor.setup_model(weights)\n        elif url:\n            if not os.path.exists(os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)):\n                os.makedirs(os.path.join(torch.hub.get_dir(), \"checkpoints\"), exist_ok=True)\n                weights = wget.download(url, out=os.path.join(torch.hub.get_dir(), \"checkpoints\"))\n            else:\n                weights = os.path.join(torch.hub.get_dir(), \"checkpoints\", self.MODEL_NAME)\n            self.predictor.setup_model(weights)\n        else:\n            raise Exception(\"Need weights for inference.\")\n\n        if not self.transform:\n            self.transform = pw_trans.MegaDetector_v5_Transform(target_size=self.IMAGE_SIZE,\n                                                                stride=self.STRIDE)\n\n    def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n        \"\"\"\n        Generate results for detection based on model predictions.\n\n        Args:\n            preds (ultralytics.engine.results.Results): \n                Model predictions.\n            img_id (str): \n                Image identifier.\n            id_strip (str, optional): \n                Strip specific characters from img_id. Defaults to None.\n\n        Returns:\n            dict: Dictionary containing image ID, detections, and labels.\n        \"\"\"\n        xyxy = preds.boxes.xyxy.cpu().numpy()\n        confidence = preds.boxes.conf.cpu().numpy()\n        class_id = preds.boxes.cls.cpu().numpy().astype(int)\n\n        results = {\"img_id\": str(img_id).strip(id_strip)}\n        # results[\"detections\"] = sv.Detections(\n        #     xyxy=xyxy,\n        #     confidence=confidence,\n        #     class_id=class_id\n        # )\n        results[\"detections_xyxy\"] = xyxy\n        results[\"detections_confidence\"] = confidence\n        results[\"detections_class_id\"] = class_id\n\n        # results[\"labels\"] = [\n        #     f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"  \n        #     for _, _, confidence, class_id, _, _ in results[\"detections\"] \n        # ]\n\n        results[\"labels\"] = [\n            f\"{self.CLASS_NAMES[cls_id]} {conf:0.2f}\"  \n            for cls_id, conf in zip(class_id, confidence)\n        ]\n\n        results[\"n_animal_detected\"] = np.sum(class_id == 0)\n\n        return results\n\n    def batch_image_detection(self, loader, batch_size, global_rank, local_rank, output_dir, det_conf_thres=0.2, checkpoint_frequency = 1000):\n\n        \"\"\"\n        Perform batch image detection using the YOLOV8 model.\n\n        Args:\n            loader (torch.utils.data.DataLoader): \n                DataLoader for input images.\n            batch_size (int):\n                Size of the batch for detection.\n            global_rank (int): \n                Global rank of the process.\n            local_rank (int): \n                Local rank of the process.\n            output_dir (str): \n                Directory to save detection results.\n            det_conf_thres (float, optional): \n                Confidence threshold for detections. Defaults to 0.2.\n            checkpoint_frequency (int, optional): \n                Frequency of saving intermediate results. Defaults to 1000.\n        \"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        self.predictor.args.batch = batch_size\n        self.predictor.args.conf = det_conf_thres\n        self.predictor.args.device = local_rank\n\n\n        # Create checkpoint directory\n        # Track batches and processed items\n        results = {\n            \"img_id\": [],\n            \"detections_xyxy\": [],\n            \"detections_confidence\": [],\n            \"detections_class_id\": [],\n            \"labels\": [],\n            \"n_animal_detected\": [],\n            \"normalized_coords\": []\n        }\n\n        checkpoint_dir = os.path.join(output_dir, f\"checkpoints_rank{global_rank}\")\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        batch_counter = 0\n        processed_count = 0\n        start_time = time.time()\n\n        for uuids, images in loader:\n            batch_counter += 1\n            processed_count += len(images)\n            # images: tensor of shape [batch_size, 3, H, W]\n            # Assuming images are transformed &amp; Standardized\n            det_results = self.predictor.stream_inference(images)\n\n            for idx, preds in enumerate(det_results):\n                res = self.results_generation(preds, uuids[idx])\n\n                size = preds.orig_shape\n                normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in res[\"detections_xyxy\"]]\n                res[\"normalized_coords\"] = normalized_coords\n\n                #results.append(res)\n                results[\"img_id\"].append(res[\"img_id\"])\n                results[\"detections_xyxy\"].append(res[\"detections_xyxy\"].tolist())\n                results[\"detections_confidence\"].append(res[\"detections_confidence\"].tolist())\n                results[\"detections_class_id\"].append(res[\"detections_class_id\"].tolist())\n                results[\"labels\"].append(res[\"labels\"])\n                results[\"n_animal_detected\"].append(int(res[\"n_animal_detected\"]))\n                results[\"normalized_coords\"].append(res[\"normalized_coords\"])\n\n            if batch_counter % checkpoint_frequency == 0:\n                elapsed = time.time() - start_time\n                print(f\"[Rank {global_rank}] Processed {processed_count} images in {elapsed}\")\n\n                # Save intermediate results\n                checkpoint_path = os.path.join(\n                    checkpoint_dir, \n                    f\"checkpoint_{batch_counter:06d}.parquet\"\n                )\n\n                df = pd.DataFrame({\n                    \"img_id\": results[\"img_id\"],\n                    \"n_animal_detected\": results[\"n_animal_detected\"]\n                })\n                df.to_parquet(checkpoint_path, index=False)\n                print(f\"[Rank {global_rank}] Saved checkpoint to {checkpoint_path}\")\n\n        # Save results to disk\n        os.makedirs(output_dir, exist_ok=True)\n        df = pd.DataFrame({\n                    \"img_id\": results[\"img_id\"],\n                    \"n_animal_detected\": results[\"n_animal_detected\"]\n                })\n        out_path = os.path.join(output_dir, f\"predictions_rank{global_rank}.parquet\")\n        df.to_parquet(out_path, index=False)\n        print(f\"[rank {global_rank}] Saved predictions to {out_path}\")\n\n        return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_distributed/#PytorchWildlife.models.detection.ultralytics_based.yolov8_distributed.YOLOV8_Distributed.__init__","title":"<code>__init__(weights=None, device='cpu', url=None, transform=None)</code>","text":"<p>Initialize the YOLOV8 detector.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>str</code> <p>Path to the model weights. Defaults to None.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for model inference. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>url</code> <code>str</code> <p>URL to fetch the model weights. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_distributed.py</code> <pre><code>def __init__(self, weights=None, device=\"cpu\", url=None, transform=None):\n    \"\"\"\n    Initialize the YOLOV8 detector.\n\n    Args:\n        weights (str, optional): \n            Path to the model weights. Defaults to None.\n        device (str, optional): \n            Device for model inference. Defaults to \"cpu\".\n        url (str, optional): \n            URL to fetch the model weights. Defaults to None.\n    \"\"\"\n    self.transform = transform\n    super(YOLOV8_Distributed, self).__init__(weights=weights, device=device, url=url)\n    self._load_model(weights, self.device, url)\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_distributed/#PytorchWildlife.models.detection.ultralytics_based.yolov8_distributed.YOLOV8_Distributed.batch_image_detection","title":"<code>batch_image_detection(loader, batch_size, global_rank, local_rank, output_dir, det_conf_thres=0.2, checkpoint_frequency=1000)</code>","text":"<p>Perform batch image detection using the YOLOV8 model.</p> <p>Parameters:</p> Name Type Description Default <code>loader</code> <code>DataLoader</code> <p>DataLoader for input images.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batch for detection.</p> required <code>global_rank</code> <code>int</code> <p>Global rank of the process.</p> required <code>local_rank</code> <code>int</code> <p>Local rank of the process.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save detection results.</p> required <code>det_conf_thres</code> <code>float</code> <p>Confidence threshold for detections. Defaults to 0.2.</p> <code>0.2</code> <code>checkpoint_frequency</code> <code>int</code> <p>Frequency of saving intermediate results. Defaults to 1000.</p> <code>1000</code> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_distributed.py</code> <pre><code>def batch_image_detection(self, loader, batch_size, global_rank, local_rank, output_dir, det_conf_thres=0.2, checkpoint_frequency = 1000):\n\n    \"\"\"\n    Perform batch image detection using the YOLOV8 model.\n\n    Args:\n        loader (torch.utils.data.DataLoader): \n            DataLoader for input images.\n        batch_size (int):\n            Size of the batch for detection.\n        global_rank (int): \n            Global rank of the process.\n        local_rank (int): \n            Local rank of the process.\n        output_dir (str): \n            Directory to save detection results.\n        det_conf_thres (float, optional): \n            Confidence threshold for detections. Defaults to 0.2.\n        checkpoint_frequency (int, optional): \n            Frequency of saving intermediate results. Defaults to 1000.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    self.predictor.args.batch = batch_size\n    self.predictor.args.conf = det_conf_thres\n    self.predictor.args.device = local_rank\n\n\n    # Create checkpoint directory\n    # Track batches and processed items\n    results = {\n        \"img_id\": [],\n        \"detections_xyxy\": [],\n        \"detections_confidence\": [],\n        \"detections_class_id\": [],\n        \"labels\": [],\n        \"n_animal_detected\": [],\n        \"normalized_coords\": []\n    }\n\n    checkpoint_dir = os.path.join(output_dir, f\"checkpoints_rank{global_rank}\")\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    batch_counter = 0\n    processed_count = 0\n    start_time = time.time()\n\n    for uuids, images in loader:\n        batch_counter += 1\n        processed_count += len(images)\n        # images: tensor of shape [batch_size, 3, H, W]\n        # Assuming images are transformed &amp; Standardized\n        det_results = self.predictor.stream_inference(images)\n\n        for idx, preds in enumerate(det_results):\n            res = self.results_generation(preds, uuids[idx])\n\n            size = preds.orig_shape\n            normalized_coords = [[x1 / size[1], y1 / size[0], x2 / size[1], y2 / size[0]] for x1, y1, x2, y2 in res[\"detections_xyxy\"]]\n            res[\"normalized_coords\"] = normalized_coords\n\n            #results.append(res)\n            results[\"img_id\"].append(res[\"img_id\"])\n            results[\"detections_xyxy\"].append(res[\"detections_xyxy\"].tolist())\n            results[\"detections_confidence\"].append(res[\"detections_confidence\"].tolist())\n            results[\"detections_class_id\"].append(res[\"detections_class_id\"].tolist())\n            results[\"labels\"].append(res[\"labels\"])\n            results[\"n_animal_detected\"].append(int(res[\"n_animal_detected\"]))\n            results[\"normalized_coords\"].append(res[\"normalized_coords\"])\n\n        if batch_counter % checkpoint_frequency == 0:\n            elapsed = time.time() - start_time\n            print(f\"[Rank {global_rank}] Processed {processed_count} images in {elapsed}\")\n\n            # Save intermediate results\n            checkpoint_path = os.path.join(\n                checkpoint_dir, \n                f\"checkpoint_{batch_counter:06d}.parquet\"\n            )\n\n            df = pd.DataFrame({\n                \"img_id\": results[\"img_id\"],\n                \"n_animal_detected\": results[\"n_animal_detected\"]\n            })\n            df.to_parquet(checkpoint_path, index=False)\n            print(f\"[Rank {global_rank}] Saved checkpoint to {checkpoint_path}\")\n\n    # Save results to disk\n    os.makedirs(output_dir, exist_ok=True)\n    df = pd.DataFrame({\n                \"img_id\": results[\"img_id\"],\n                \"n_animal_detected\": results[\"n_animal_detected\"]\n            })\n    out_path = os.path.join(output_dir, f\"predictions_rank{global_rank}.parquet\")\n    df.to_parquet(out_path, index=False)\n    print(f\"[rank {global_rank}] Saved predictions to {out_path}\")\n\n    return results\n</code></pre>"},{"location":"base/models/detection/ultralytics_based/yolov8_distributed/#PytorchWildlife.models.detection.ultralytics_based.yolov8_distributed.YOLOV8_Distributed.results_generation","title":"<code>results_generation(preds, img_id, id_strip=None)</code>","text":"<p>Generate results for detection based on model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Results</code> <p>Model predictions.</p> required <code>img_id</code> <code>str</code> <p>Image identifier.</p> required <code>id_strip</code> <code>str</code> <p>Strip specific characters from img_id. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing image ID, detections, and labels.</p> Source code in <code>PytorchWildlife/models/detection/ultralytics_based/yolov8_distributed.py</code> <pre><code>def results_generation(self, preds, img_id, id_strip=None) -&gt; dict:\n    \"\"\"\n    Generate results for detection based on model predictions.\n\n    Args:\n        preds (ultralytics.engine.results.Results): \n            Model predictions.\n        img_id (str): \n            Image identifier.\n        id_strip (str, optional): \n            Strip specific characters from img_id. Defaults to None.\n\n    Returns:\n        dict: Dictionary containing image ID, detections, and labels.\n    \"\"\"\n    xyxy = preds.boxes.xyxy.cpu().numpy()\n    confidence = preds.boxes.conf.cpu().numpy()\n    class_id = preds.boxes.cls.cpu().numpy().astype(int)\n\n    results = {\"img_id\": str(img_id).strip(id_strip)}\n    # results[\"detections\"] = sv.Detections(\n    #     xyxy=xyxy,\n    #     confidence=confidence,\n    #     class_id=class_id\n    # )\n    results[\"detections_xyxy\"] = xyxy\n    results[\"detections_confidence\"] = confidence\n    results[\"detections_class_id\"] = class_id\n\n    # results[\"labels\"] = [\n    #     f\"{self.CLASS_NAMES[class_id]} {confidence:0.2f}\"  \n    #     for _, _, confidence, class_id, _, _ in results[\"detections\"] \n    # ]\n\n    results[\"labels\"] = [\n        f\"{self.CLASS_NAMES[cls_id]} {conf:0.2f}\"  \n        for cls_id, conf in zip(class_id, confidence)\n    ]\n\n    results[\"n_animal_detected\"] = np.sum(class_id == 0)\n\n    return results\n</code></pre>"},{"location":"base/utils/misc/","title":"Misc","text":"<p>Miscellaneous functions.</p>"},{"location":"base/utils/misc/#PytorchWildlife.utils.misc.process_video","title":"<code>process_video(source_path, target_path, callback, target_fps=1, codec='mp4v')</code>","text":"<p>Process a video frame-by-frame, applying a callback function to each frame and saving the results  to a new video. This version includes a progress bar and allows codec selection.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Path to the source video file.</p> required <code>target_path</code> <code>str</code> <p>Path to save the processed video.</p> required <code>callback</code> <code>Callable[[ndarray, int], ndarray]</code> <p>A function that takes a video frame and its index as input and returns the processed frame.</p> required <code>codec</code> <code>str</code> <p>Codec used to encode the processed video. Default is \"avc1\".</p> <code>'mp4v'</code> Source code in <code>PytorchWildlife/utils/misc.py</code> <pre><code>def process_video(\n    source_path: str,\n    target_path: str,\n    callback: Callable[[np.ndarray, int], np.ndarray],\n    target_fps: int = 1,\n    codec: str = \"mp4v\"\n) -&gt; None:\n    \"\"\"\n    Process a video frame-by-frame, applying a callback function to each frame and saving the results \n    to a new video. This version includes a progress bar and allows codec selection.\n\n    Args:\n        source_path (str): \n            Path to the source video file.\n        target_path (str): \n            Path to save the processed video.\n        callback (Callable[[np.ndarray, int], np.ndarray]): \n            A function that takes a video frame and its index as input and returns the processed frame.\n        codec (str, optional): \n            Codec used to encode the processed video. Default is \"avc1\".\n    \"\"\"\n    source_video_info = VideoInfo.from_video_path(video_path=source_path)\n\n    if source_video_info.fps &gt; target_fps:\n        stride = int(source_video_info.fps / target_fps)\n        source_video_info.fps = target_fps\n    else:\n        stride = 1\n\n    with VideoSink(target_path=target_path, video_info=source_video_info, codec=codec) as sink:\n        with tqdm(total=int(source_video_info.total_frames / stride)) as pbar: \n            for index, frame in enumerate(\n                get_video_frames_generator(source_path=source_path, stride=stride)\n            ):\n                result_frame = callback(frame, index)\n                sink.write_frame(frame=cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR))\n                pbar.update(1)\n</code></pre>"},{"location":"base/utils/post_process/","title":"Post process","text":"<p>Post-processing functions.</p>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.detection_folder_separation","title":"<code>detection_folder_separation(json_file, img_path, destination_path, confidence_threshold)</code>","text":"<p>Processes detection data from a JSON file to sort images into 'Animal' or 'No_animal' directories based on detection categories and confidence levels.</p> <p>This function reads a JSON formatted file containing annotations of image detections. Each image is checked for detections with category '0' and a confidence level above the specified threshold. If such detections are found, the image is categorized under 'Animal'. Images without any category '0' detections above the threshold, including those with no detections at all, are  categorized under 'No_animal'.</p> <p>Parameters: - json_file (str): Path to the JSON file containing detection data. - destination_path (str): Base path where 'Animal' and 'No_animal' folders will be created                           and into which images will be sorted and copied. - source_images_directory (str): Path to the directory containing the source images to be processed. - confidence_threshold (float): The confidence threshold to consider a detection as valid.</p> <p>Effects: - Reads from the specified <code>json_file</code>. - Copies files from <code>source_images_directory</code> to either <code>destination_path/Animal</code> or   <code>destination_path/No_animal</code> based on the detection data and confidence level.</p> <p>Note: - The function assumes that the JSON file structure includes keys 'annotations', each containing   'img_id', 'bbox', 'category', and 'confidence'. It does not handle missing keys or unexpected   JSON structures and may raise an exception in such cases. - Directories <code>Animal</code> and <code>No_animal</code> are created if they do not already exist. - Images are copied, not moved; original images remain in the source directory.</p> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def detection_folder_separation(json_file, img_path, destination_path, confidence_threshold):\n    \"\"\"\n    Processes detection data from a JSON file to sort images into 'Animal' or 'No_animal' directories\n    based on detection categories and confidence levels.\n\n    This function reads a JSON formatted file containing annotations of image detections.\n    Each image is checked for detections with category '0' and a confidence level above the specified\n    threshold. If such detections are found, the image is categorized under 'Animal'. Images without\n    any category '0' detections above the threshold, including those with no detections at all, are \n    categorized under 'No_animal'.\n\n    Parameters:\n    - json_file (str): Path to the JSON file containing detection data.\n    - destination_path (str): Base path where 'Animal' and 'No_animal' folders will be created\n                              and into which images will be sorted and copied.\n    - source_images_directory (str): Path to the directory containing the source images to be processed.\n    - confidence_threshold (float): The confidence threshold to consider a detection as valid.\n\n    Effects:\n    - Reads from the specified `json_file`.\n    - Copies files from `source_images_directory` to either `destination_path/Animal` or\n      `destination_path/No_animal` based on the detection data and confidence level.\n\n    Note:\n    - The function assumes that the JSON file structure includes keys 'annotations', each containing\n      'img_id', 'bbox', 'category', and 'confidence'. It does not handle missing keys or unexpected\n      JSON structures and may raise an exception in such cases.\n    - Directories `Animal` and `No_animal` are created if they do not already exist.\n    - Images are copied, not moved; original images remain in the source directory.\n    \"\"\"\n\n    # Load JSON data from the file\n    with open(json_file, 'r') as file:\n        data = json.load(file)\n\n    # Ensure the destination directories exist\n    os.makedirs(destination_path, exist_ok=True)\n    animal_path = os.path.join(destination_path, \"Animal\")\n    no_animal_path = os.path.join(destination_path, \"No_animal\")\n    os.makedirs(animal_path, exist_ok=True)\n    os.makedirs(no_animal_path, exist_ok=True)\n\n    # Process each image detection\n    i = 0\n    for item in data['annotations']:\n        i+=1\n        img_id = item['img_id']\n        categories = item['category']\n        confidences = item['confidence']\n\n        # Check if there is any category '0' with confidence above the threshold\n        file_targeted_for_animal = False\n        for category, confidence in zip(categories, confidences):\n            if category == 0 and confidence &gt; confidence_threshold:\n                file_targeted_for_animal = True\n                break\n\n        if file_targeted_for_animal:\n            target_folder = animal_path\n        else:\n            target_folder = no_animal_path\n\n        # Construct the source and destination file paths\n        src_file_path = os.path.join(img_path, img_id)\n        dest_file_path = os.path.join(target_folder, os.path.dirname(img_id))\n        os.makedirs(dest_file_path, exist_ok=True)\n\n        # Copy the file to the appropriate directory\n        shutil.copy(src_file_path, dest_file_path)\n\n    return \"{} files were successfully separated\".format(i)\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_crop_images","title":"<code>save_crop_images(results, output_dir, input_dir=None, overwrite=False)</code>","text":"<p>Save cropped images based on the detection bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>Detection results containing image ID and detections.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the cropped images.</p> required <code>input_dir</code> <code>str</code> <p>Directory containing the input images. Default to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether overwriting existing image folders. Default to False.</p> <code>False</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_crop_images(results, output_dir, input_dir=None, overwrite=False):\n    \"\"\"\n    Save cropped images based on the detection bounding boxes.\n\n    Args:\n        results (list):\n            Detection results containing image ID and detections.\n        output_dir (str):\n            Directory to save the cropped images.\n        input_dir (str):\n            Directory containing the input images. Default to None.\n        overwrite (bool):\n            Whether overwriting existing image folders. Default to False.\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    with sv.ImageSink(target_dir_path=output_dir, overwrite=overwrite) as sink:\n        if isinstance(results, list):\n            for entry in results:\n                for i, (xyxy, cat) in enumerate(zip(entry[\"detections\"].xyxy, entry[\"detections\"].class_id)):\n                    cropped_img = sv.crop_image(\n                        image=np.array(Image.open(entry[\"img_id\"]).convert(\"RGB\")), xyxy=xyxy\n                    )\n                    if input_dir:\n                        relative_path = os.path.relpath(entry[\"img_id\"], input_dir)\n                        save_path = os.path.join(output_dir, relative_path)\n                        os.makedirs(os.path.dirname(save_path), exist_ok=True) \n                        image_name = os.path.join(os.path.dirname(relative_path), \"{}_{}_{}\".format(int(cat), i, os.path.basename(entry[\"img_id\"])))\n                    else:\n                        image_name = \"{}_{}_{}\".format(int(cat), i, os.path.basename(entry[\"img_id\"]))\n                    sink.save_image(\n                        image=cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR),\n                        image_name=image_name,\n                    )\n        else:\n            for i, (xyxy, cat) in enumerate(zip(results[\"detections\"].xyxy, results[\"detections\"].class_id)):\n                cropped_img = sv.crop_image(\n                    image=np.array(Image.open(results[\"img_id\"]).convert(\"RGB\")), xyxy=xyxy\n                )\n                sink.save_image(\n                    image=cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR),\n                    image_name=\"{}_{}_{}\".format(int(cat), i, os.path.basename(results[\"img_id\"]),\n                ))\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_classification_json","title":"<code>save_detection_classification_json(det_results, clf_results, output_path, det_categories=None, clf_categories=None, exclude_file_path=None)</code>","text":"<p>Save classification results to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>det_results</code> <code>list</code> <p>Detection results containing image ID, bounding boxes, detection category, and confidence.</p> required <code>clf_results</code> <code>list</code> <p>classification results containing image ID, classification category, and confidence.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output JSON file.</p> required <code>det_categories</code> <code>list</code> <p>List of categories for detected objects. Defaults to None.</p> <code>None</code> <code>clf_categories</code> <code>list</code> <p>List of categories for classified objects. Defaults to None.</p> <code>None</code> <code>exclude_file_path</code> <code>str</code> <p>We can exclude the some path sections from the image ID. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_classification_json(\n    det_results, clf_results, output_path, det_categories=None, clf_categories=None, exclude_file_path=None\n):\n    \"\"\"\n    Save classification results to a JSON file.\n\n    Args:\n        det_results (list):\n            Detection results containing image ID, bounding boxes, detection category, and confidence.\n        clf_results (list):\n            classification results containing image ID, classification category, and confidence.\n        output_path (str):\n            Path to save the output JSON file.\n        det_categories (list, optional):\n            List of categories for detected objects. Defaults to None.\n        clf_categories (list, optional):\n            List of categories for classified objects. Defaults to None.\n        exclude_file_path (str, optional):\n            We can exclude the some path sections from the image ID. Defaults to None.\n    \"\"\"\n\n    json_results = {\n        \"annotations\": [],\n        \"det_categories\": det_categories,\n        \"clf_categories\": clf_categories,\n    }\n\n    with open(output_path, \"w\") as f:\n        counter = 0\n        for det_r in det_results:\n            clf_categories = []\n            clf_confidence = []\n            for i in range(counter, len(clf_results)):\n                clf_r = clf_results[i]\n                if clf_r[\"img_id\"] == det_r[\"img_id\"]:\n                    clf_categories.append(clf_r[\"class_id\"])\n                    clf_confidence.append(clf_r[\"confidence\"])\n                    counter += 1\n                else:\n                    break\n\n            json_results[\"annotations\"].append(\n                {\n                    \"img_id\": str(det_r[\"img_id\"]).replace(exclude_file_path + os.sep, '') if exclude_file_path else str(det_r[\"img_id\"]),\n                    \"bbox\": [\n                        [int(x) for x in sublist]\n                        for sublist in det_r[\"detections\"].xyxy.astype(int).tolist()\n                    ],\n                    \"det_category\": [\n                        int(x) for x in det_r[\"detections\"].class_id.tolist()\n                    ],\n                    \"det_confidence\": [\n                        float(x) for x in det_r[\"detections\"].confidence.tolist()\n                    ],\n                    \"clf_category\": [int(x) for x in clf_categories],\n                    \"clf_confidence\": [float(x) for x in clf_confidence],\n                }\n            )\n        json.dump(json_results, f, indent=4)\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_classification_timelapse_json","title":"<code>save_detection_classification_timelapse_json(det_results, clf_results, output_path, det_categories=None, clf_categories=None, exclude_file_path=None, info={'detector': 'megadetector_v5'})</code>","text":"<p>Save detection and classification results to a JSON file in the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>det_results</code> <code>list</code> <p>Detection results containing image ID, bounding boxes, detection category, and confidence.</p> required <code>clf_results</code> <code>list</code> <p>Classification results containing image ID, classification category, and confidence.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output JSON file.</p> required <code>det_categories</code> <code>dict</code> <p>Dictionary of categories for detected objects. Defaults to None.</p> <code>None</code> <code>clf_categories</code> <code>dict</code> <p>Dictionary of categories for classified objects. Defaults to None.</p> <code>None</code> <code>exclude_file_path</code> <code>str</code> <p>We can exclude the some path sections from the image ID. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_classification_timelapse_json(\n    det_results, clf_results, output_path, det_categories=None, clf_categories=None,\n    exclude_file_path=None, info={\"detector\": \"megadetector_v5\"}\n):\n    \"\"\"\n    Save detection and classification results to a JSON file in the specified format.\n\n    Args:\n        det_results (list):\n            Detection results containing image ID, bounding boxes, detection category, and confidence.\n        clf_results (list):\n            Classification results containing image ID, classification category, and confidence.\n        output_path (str):\n            Path to save the output JSON file.\n        det_categories (dict, optional):\n            Dictionary of categories for detected objects. Defaults to None.\n        clf_categories (dict, optional):\n            Dictionary of categories for classified objects. Defaults to None.\n        exclude_file_path (str, optional):\n            We can exclude the some path sections from the image ID. Defaults to None.\n    \"\"\"\n    json_results = {\n        \"info\": info,\n        \"detection_categories\": det_categories,\n        \"classification_categories\": clf_categories,\n        \"images\": []\n    }\n\n    for det_r in det_results:\n        image_annotations = {\n            \"file\": str(det_r[\"img_id\"]).replace(exclude_file_path + os.sep, '') if exclude_file_path else str(det_r[\"img_id\"]),\n            \"max_detection_conf\": float(max(det_r[\"detections\"].confidence)) if len(det_r[\"detections\"].confidence) &gt; 0 else '',\n            \"detections\": []\n        }\n\n        for i in range(len(det_r[\"detections\"])):\n            det = det_r[\"detections\"][i]\n            normalized_bbox = [float(y) for y in det_r[\"normalized_coords\"][i]]\n            detection = {\n                \"category\": str(det.class_id[0]),\n                \"conf\": float(det.confidence[0]),\n                \"bbox\": [normalized_bbox[0], normalized_bbox[1], normalized_bbox[2]-normalized_bbox[0], normalized_bbox[3]-normalized_bbox[1]],\n                \"classifications\": []\n            }\n\n            # Find classifications for this detection\n            for clf_r in clf_results:\n                if clf_r[\"img_id\"] == det_r[\"img_id\"]:\n                    detection[\"classifications\"].append([str(clf_r[\"class_id\"]), float(clf_r[\"confidence\"])])\n\n            image_annotations[\"detections\"].append(detection)\n\n        json_results[\"images\"].append(image_annotations)\n\n    with open(output_path, \"w\") as f:\n        json.dump(json_results, f, indent=4)\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_images","title":"<code>save_detection_images(results, output_dir, input_dir=None, overwrite=False)</code>","text":"<p>Save detected images with bounding boxes and labels annotated.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list or dict</code> <p>Detection results containing image ID, detections, and labels.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the annotated images.</p> required <code>input_dir</code> <code>str</code> <p>Directory containing the input images. Default to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether overwriting existing image folders. Default to False.</p> <code>False</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_images(results, output_dir, input_dir=None, overwrite=False):\n    \"\"\"\n    Save detected images with bounding boxes and labels annotated.\n\n    Args:\n        results (list or dict):\n            Detection results containing image ID, detections, and labels.\n        output_dir (str):\n            Directory to save the annotated images.\n        input_dir (str):\n            Directory containing the input images. Default to None.\n        overwrite (bool):\n            Whether overwriting existing image folders. Default to False.\n    \"\"\"\n    box_annotator = sv.BoxAnnotator(thickness=4)\n    lab_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK, text_thickness=4, text_scale=2)\n    os.makedirs(output_dir, exist_ok=True)\n\n    with sv.ImageSink(target_dir_path=output_dir, overwrite=overwrite) as sink: \n        if isinstance(results, list):\n            for entry in results:\n                annotated_img = lab_annotator.annotate(\n                    scene=box_annotator.annotate(\n                        scene=np.array(Image.open(entry[\"img_id\"]).convert(\"RGB\")),\n                        detections=entry[\"detections\"],\n                    ),\n                    detections=entry[\"detections\"],\n                    labels=entry[\"labels\"],\n                )\n                if input_dir:\n                    relative_path = os.path.relpath(entry[\"img_id\"], input_dir)\n                    save_path = os.path.join(output_dir, relative_path)\n                    os.makedirs(os.path.dirname(save_path), exist_ok=True) \n                    image_name = relative_path \n                else:\n                    image_name = os.path.basename(entry[\"img_id\"])\n                sink.save_image(\n                    image=cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR), image_name=image_name\n                )\n        else:\n            annotated_img = lab_annotator.annotate(\n                scene=box_annotator.annotate(\n                    scene=np.array(Image.open(results[\"img_id\"]).convert(\"RGB\")),\n                    detections=results[\"detections\"],\n                ),\n                detections=results[\"detections\"],\n                labels=results[\"labels\"],\n            )\n\n            sink.save_image(\n                image=cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR), image_name=os.path.basename(results[\"img_id\"])\n            )\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_images_dots","title":"<code>save_detection_images_dots(results, output_dir, input_dir=None, overwrite=False)</code>","text":"<p>Save detected images with bounding boxes and labels annotated.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list or dict</code> <p>Detection results containing image ID, detections, and labels.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the annotated images.</p> required <code>input_dir</code> <code>str</code> <p>Directory containing the input images. Default to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether overwriting existing image folders. Default to False.</p> <code>False</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_images_dots(results, output_dir, input_dir=None, overwrite=False):\n    \"\"\"\n    Save detected images with bounding boxes and labels annotated.\n\n    Args:\n        results (list or dict):\n            Detection results containing image ID, detections, and labels.\n        output_dir (str):\n            Directory to save the annotated images.\n        input_dir (str):\n            Directory containing the input images. Default to None.\n        overwrite (bool):\n            Whether overwriting existing image folders. Default to False.\n    \"\"\"\n    dot_annotator = sv.DotAnnotator(radius=6)  \n    lab_annotator = sv.LabelAnnotator(text_position=sv.Position.BOTTOM_RIGHT)   \n    os.makedirs(output_dir, exist_ok=True)\n\n    with sv.ImageSink(target_dir_path=output_dir, overwrite=overwrite) as sink:\n        if isinstance(results, list):\n            for i, entry in enumerate(results):\n                if \"img_id\" in entry:\n                    scene = np.array(Image.open(entry[\"img_id\"]).convert(\"RGB\"))\n                    image_name = os.path.basename(entry[\"img_id\"])\n                else:\n                    scene = entry[\"img\"]\n                    image_name = f\"output_image_{i}.jpg\" # default name if no image id is provided\n\n                annotated_img = lab_annotator.annotate(\n                    scene=dot_annotator.annotate(\n                        scene=scene,\n                        detections=entry[\"detections\"],\n                    ),\n                    detections=entry[\"detections\"],\n                    labels=entry[\"labels\"],\n                )\n                if input_dir:\n                    relative_path = os.path.relpath(entry[\"img_id\"], input_dir)\n                    save_path = os.path.join(output_dir, relative_path)\n                    os.makedirs(os.path.dirname(save_path), exist_ok=True) \n                    image_name = relative_path\n                sink.save_image(\n                    image=cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR), image_name=image_name\n                )\n        else:\n            if \"img_id\" in results:\n                scene = np.array(Image.open(results[\"img_id\"]).convert(\"RGB\"))\n                image_name = os.path.basename(results[\"img_id\"])\n            else:\n                scene = results[\"img\"]\n                image_name = \"output_image.jpg\" # default name if no image id is provided\n\n            annotated_img = lab_annotator.annotate(\n                scene=dot_annotator.annotate(\n                    scene=scene,\n                    detections=results[\"detections\"],\n                ),\n                detections=results[\"detections\"],\n                labels=results[\"labels\"],\n            )   \n            sink.save_image(\n                image=cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR), image_name=image_name\n            )\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_json","title":"<code>save_detection_json(det_results, output_dir, categories=None, exclude_category_ids=[], exclude_file_path=None)</code>","text":"<p>Save detection results to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>det_results</code> <code>list</code> <p>Detection results containing image ID, bounding boxes, category, and confidence.</p> required <code>output_dir</code> <code>str</code> <p>Path to save the output JSON file.</p> required <code>categories</code> <code>list</code> <p>List of categories for detected objects. Defaults to None.</p> <code>None</code> <code>exclude_category_ids</code> <code>list</code> <p>List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.</p> <code>[]</code> <code>exclude_file_path</code> <code>str</code> <p>We can exclude the some path sections from the image ID. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_json(det_results, output_dir, categories=None, exclude_category_ids=[], exclude_file_path=None):\n    \"\"\"\n    Save detection results to a JSON file.\n\n    Args:\n        det_results (list):\n            Detection results containing image ID, bounding boxes, category, and confidence.\n        output_dir (str):\n            Path to save the output JSON file.\n        categories (list, optional):\n            List of categories for detected objects. Defaults to None.\n        exclude_category_ids (list, optional):\n            List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.\n        exclude_file_path (str, optional):\n            We can exclude the some path sections from the image ID. Defaults to None.\n    \"\"\"\n    json_results = {\"annotations\": [], \"categories\": categories}\n\n    for det_r in det_results:\n\n        # Category filtering\n        img_id = det_r[\"img_id\"]\n        category = det_r[\"detections\"].class_id\n\n        bbox = det_r[\"detections\"].xyxy.astype(int)[~np.isin(category, exclude_category_ids)]\n        confidence =  det_r[\"detections\"].confidence[~np.isin(category, exclude_category_ids)]\n        category = category[~np.isin(category, exclude_category_ids)]\n\n        # if not all([x in exclude_category_ids for x in category]):\n        json_results[\"annotations\"].append(\n            {\n                \"img_id\": img_id.replace(exclude_file_path + os.sep, '') if exclude_file_path else img_id,\n                \"bbox\": bbox.tolist(),\n                \"category\": category.tolist(),\n                \"confidence\": confidence.tolist(),\n            }\n        )\n\n    with open(output_dir, \"w\") as f:\n        json.dump(json_results, f, indent=4)\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_json_as_dots","title":"<code>save_detection_json_as_dots(det_results, output_dir, categories=None, exclude_category_ids=[], exclude_file_path=None)</code>","text":"<p>Save detection results to a JSON file in dots format.</p> <p>Parameters:</p> Name Type Description Default <code>det_results</code> <code>list</code> <p>Detection results containing image ID, bounding boxes, category, and confidence.</p> required <code>output_dir</code> <code>str</code> <p>Path to save the output JSON file.</p> required <code>categories</code> <code>list</code> <p>List of categories for detected objects. Defaults to None.</p> <code>None</code> <code>exclude_category_ids</code> <code>list</code> <p>List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.</p> <code>[]</code> <code>exclude_file_path</code> <code>str</code> <p>We can exclude the some path sections from the image ID. Defaults to None.</p> <code>None</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_json_as_dots(det_results, output_dir, categories=None, exclude_category_ids=[], exclude_file_path=None):\n    \"\"\"\n    Save detection results to a JSON file in dots format.\n\n    Args:\n        det_results (list):\n            Detection results containing image ID, bounding boxes, category, and confidence.\n        output_dir (str):\n            Path to save the output JSON file.\n        categories (list, optional):\n            List of categories for detected objects. Defaults to None.\n        exclude_category_ids (list, optional):\n            List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.\n        exclude_file_path (str, optional):\n            We can exclude the some path sections from the image ID. Defaults to None.\n    \"\"\"\n    json_results = {\"annotations\": [], \"categories\": categories}\n\n    for det_r in det_results:\n\n        # Category filtering\n        img_id = det_r[\"img_id\"]\n        category = det_r[\"detections\"].class_id\n\n        bbox = det_r[\"detections\"].xyxy.astype(int)[~np.isin(category, exclude_category_ids)]\n        dot = np.array([[np.mean(row[::2]), np.mean(row[1::2])] for row in bbox])\n        confidence =  det_r[\"detections\"].confidence[~np.isin(category, exclude_category_ids)]\n        category = category[~np.isin(category, exclude_category_ids)]\n\n        # if not all([x in exclude_category_ids for x in category]):\n        json_results[\"annotations\"].append(\n            {\n                \"img_id\": img_id.replace(exclude_file_path + os.sep, '') if exclude_file_path else img_id,\n                \"dot\": dot.tolist(),\n                \"category\": category.tolist(),\n                \"confidence\": confidence.tolist(),\n            }\n        )\n\n    with open(output_dir, \"w\") as f:\n        json.dump(json_results, f, indent=4)\n</code></pre>"},{"location":"base/utils/post_process/#PytorchWildlife.utils.post_process.save_detection_timelapse_json","title":"<code>save_detection_timelapse_json(det_results, output_dir, categories=None, exclude_category_ids=[], exclude_file_path=None, info={'detector': 'megadetector_v5'})</code>","text":"<p>Save detection results to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>det_results</code> <code>list</code> <p>Detection results containing image ID, bounding boxes, category, and confidence.</p> required <code>output_dir</code> <code>str</code> <p>Path to save the output JSON file.</p> required <code>categories</code> <code>list</code> <p>List of categories for detected objects. Defaults to None.</p> <code>None</code> <code>exclude_category_ids</code> <code>list</code> <p>List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.</p> <code>[]</code> <code>exclude_file_path</code> <code>str</code> <p>Some time, Timelapse has path issues. We can exclude the some path sections from the image ID. Defaults to None.</p> <code>None</code> <code>info</code> <code>dict</code> <p>Default Timelapse info. Defaults to {\"detector\": \"megadetector_v5}.</p> <code>{'detector': 'megadetector_v5'}</code> Source code in <code>PytorchWildlife/utils/post_process.py</code> <pre><code>def save_detection_timelapse_json(\n    det_results, output_dir, categories=None,\n    exclude_category_ids=[], exclude_file_path=None, info={\"detector\": \"megadetector_v5\"}\n    ):\n    \"\"\"\n    Save detection results to a JSON file.\n\n    Args:\n        det_results (list):\n            Detection results containing image ID, bounding boxes, category, and confidence.\n        output_dir (str):\n            Path to save the output JSON file.\n        categories (list, optional):\n            List of categories for detected objects. Defaults to None.\n        exclude_category_ids (list, optional):\n            List of category IDs to exclude from the output. Defaults to []. Category IDs can be found in the definition of each models.\n        exclude_file_path (str, optional):\n            Some time, Timelapse has path issues. We can exclude the some path sections from the image ID. Defaults to None.\n        info (dict, optional):\n            Default Timelapse info. Defaults to {\"detector\": \"megadetector_v5}.\n    \"\"\"\n\n    json_results = {\n        \"info\": info,\n        \"detection_categories\": categories,\n        \"images\": []\n    }\n\n    for det_r in det_results:\n\n        img_id = det_r[\"img_id\"]\n        category_id_list = det_r[\"detections\"].class_id\n\n        bbox_list = det_r[\"detections\"].xyxy.astype(int)[~np.isin(category_id_list, exclude_category_ids)]\n        confidence_list =  det_r[\"detections\"].confidence[~np.isin(category_id_list, exclude_category_ids)]\n        normalized_bbox_list = np.array(det_r[\"normalized_coords\"])[~np.isin(category_id_list, exclude_category_ids)]\n        category_id_list = category_id_list[~np.isin(category_id_list, exclude_category_ids)]\n\n        # if not all([x in exclude_category_ids for x in category_id_list]):\n        image_annotations = {\n            \"file\": img_id.replace(exclude_file_path + os.sep, '') if exclude_file_path else img_id,\n            \"max_detection_conf\": float(max(confidence_list)) if len(confidence_list) &gt; 0 else '',\n            \"detections\": []\n        }\n        for i in range(len(bbox_list)):\n            normalized_bbox = [float(y) for y in normalized_bbox_list[i]]\n            detection = {\n                \"category\": str(category_id_list[i]),\n                \"conf\": float(confidence_list[i]),\n                \"bbox\": [normalized_bbox[0], normalized_bbox[1], normalized_bbox[2]-normalized_bbox[0], normalized_bbox[3]-normalized_bbox[1]],\n                \"classifications\": []\n            }\n\n            image_annotations[\"detections\"].append(detection)\n\n        json_results[\"images\"].append(image_annotations)\n\n    with open(output_dir, \"w\") as f:\n        json.dump(json_results, f, indent=4)\n</code></pre>"},{"location":"demo_and_ui/demo_data/","title":"Prepare data for demos","text":"<p>Before we run any demos, please download some demo data for our demo notebooks and webapps from this link and decompress the data in the demo folder. </p>"},{"location":"demo_and_ui/ecoassist/","title":"Pytorch-Wildlife modelsa are available with AddaxAI (formerly EcoAssist)!","text":"<p>We are thrilled to announce our collaboration with AddaxAI---a powerful user interface software that enables users to directly load models from the PyTorch-Wildlife model zoo for image analysis on local computers. With AddaxAI, you can now utilize MegaDetectorV5 and the classification models---AI4GAmazonRainforest and AI4GOpossum---for automatic animal detection and identification, alongside a comprehensive suite of pre- and post-processing tools. This partnership aims to enhance the overall user experience with PyTorch-Wildlife models for a general audience. We will work closely to bring more features together for more efficient and effective wildlife analysis in the future. Please refer to their tutorials on how to use Pytorch-Wildlife models with AddaxAI. </p>"},{"location":"demo_and_ui/gradio/","title":"Gradio App","text":""},{"location":"demo_and_ui/gradio/#explore-pytorch-wildlife-and-megadetector-with-our-demo-user-interface","title":"\ud83d\udd75\ufe0f Explore Pytorch-Wildlife and MegaDetector with our Demo User Interface","text":"<p>If you want to directly try Pytorch-Wildlife and the models in our model zoo, you can use our Gradio interface. </p> <p>To start using the app locally:</p> <p><pre><code>python gradio_demo.py\n</code></pre> The <code>gradio_demo.py</code> will launch a Gradio interface where you can: - Perform Single Image Detection: Upload an image and set a confidence threshold to get detections. - Perform Batch Image Detection: Upload a zip file containing multiple images to get detections in a JSON format. - Perform Video Detection: Upload a video and get a processed video with detected animals. </p> <p>Or, you can also go to our HuggingFace Page for some quick testing.</p> <p>As a showcase platform, the gradio demo offers a hands-on experience with all the available features. However, it's important to note that this interface is primarily for demonstration purposes. While it is fully equipped to run all the features effectively, it may not be optimized for scenarios involving excessive data loads. We advise users to be mindful of this limitation when experimenting with large datasets.</p> <p>Some browsers may not render processed videos due to unsupported codec. If that happens, please either use a newer version of browser or run the following for a <code>conda</code> version of <code>opencv</code> and choose <code>avc1</code> in the Video encoder drop down menu in the webapp (this might not work for MacOS):</p> <pre><code>pip uninstall opencv-python\nconda install -c conda-forge opencv\n</code></pre> <p></p>"},{"location":"demo_and_ui/notebook/","title":"Demo Scripts and Notebooks","text":""},{"location":"demo_and_ui/notebook/#local-notebooks","title":"Local notebooks","text":"<p>To run our demo notebooks and scripts, please go to the <code>demo</code> folder and follow the instructions in our installation page on how to use jupyter notebooks.</p>"},{"location":"demo_and_ui/notebook/#online-notebooks","title":"Online notebooks","text":"<p>We are currently migrating our demo jupyter notebooks to this document page so you do not have to always run jupyter on your local machines. Please keep updates! </p>"},{"location":"demo_and_ui/timelapse/","title":"Pytorch-Wildlife and TimeLapse","text":"<p>Pytorch-Wildlife offers native output formats that are directly compatible with TimeLapse. We will provide more details on this in future releases! We will keep you posted! </p>"},{"location":"fine_tuning_modules/overview/","title":"In construction","text":""},{"location":"fine_tuning_modules/classification/overview/","title":"In Construction","text":""},{"location":"fine_tuning_modules/detection/overview/","title":"In Construction","text":""},{"location":"model_zoo/classifiers/","title":"Classifiers","text":"Models Version Names Licence Release AI4G-Oppossum - MIT Released AI4G-Amazon-V1 v1 MIT Released AI4G-Amazon-V2 v2 MIT Released AI4G-Serengeti - MIT Released Deepfaune-classification v1.3 CC BY-SA 4.0 Released Deepfaune-New-England v1.0 CC0 1.0 Universal Released <p>[!TIP] Some models, such as MegaDetectorV6, HerdNet, and AI4G-Amazon, have different versions, and they are loaded by their corresponding version names. Here is an example: <code>detection_model = pw_detection.MegaDetectorV6(version=\"MDV6-yolov10-e\")</code>.</p>"},{"location":"model_zoo/megadetector/","title":"Model Zoo and Release Schedules","text":"Models Version Names Licence Release Parameters (M) mAR (Animal Class) mAP50 (All Classes) MegaDetectorV5a a AGPL-3.0 Released 139.9 81.7 92.0 MegaDetectorV5b b AGPL-3.0 Released 139.9 80.9 90.1 MegaDetectorV6-Ultralytics-YoloV9-Compact MDV6-yolov9-c AGPL-3.0 Released 25.5 78.4 87.9 MegaDetectorV6-Ultralytics-YoloV9-Extra MDV6-yolov9-e AGPL-3.0 Released 58.1 82.1 88.6 MegaDetectorV6-Ultralytics-YoloV10-Compact (even smaller and no NMS) MDV6-yolov10-c AGPL-3.0 Released 2.3 76.8 87.2 MegaDetectorV6-Ultralytics-YoloV10-Extra (extra large model and no NMS) MDV6-yolov10-e AGPL-3.0 Released 29.5 82.8 92.8 MegaDetectorV6-Ultralytics-RtDetr-Compact MDV6-rtdetr-c AGPL-3.0 Released 31.9 81.6 89.9 MegaDetectorV6-Ultralytics-YoloV11-Compact - AGPL-3.0 Will Not Release 2.6 76.0 87.6 MegaDetectorV6-Ultralytics-YoloV11-Extra - AGPL-3.0 Will Not Release 56.9 81.2 92.3 MegaDetectorV6-MIT-YoloV9-Compact MDV6-mit-yolov9-c MIT Released 9.7 74.8 87.6 MegaDetectorV6-MIT-YoloV9-Extra MDV6-mit-yolov9-e MIT Released 51 76.1 71.5 MegaDetectorV6-Apache-RTDetr-Compact MDV6-apa-rtdetr-c Apache Released 20 81.1 91.0 MegaDetectorV6-Apache-RTDetr-Extra MDV6-apa-rtdetr-e Apache Released 76 82.9 94.1 MegaDetector-Overhead - MIT Mid 2025 - MegaDetector-Bioacoustics - MIT Late 2025 - <p>[!TIP] We are specifically reporting <code>Animal Recall</code> as our primary performance metric, even though it is not commonly used in traditional object detection studies, which typically focus on balancing overall model performance. For MegaDetector, our goal is to optimize for animal recall\u2014in other words, minimizing false negative detections of animals or, more simply, ensuring our model misses as few animals as possible. While this may result in a higher false positive rate, we rely on downstream classification models to further filter the detected objects. We believe this approach is more practical for real-world animal monitoring scenarios.</p> <p>[!TIP] Some models, such as MegaDetectorV6, HerdNet, and AI4G-Amazon, have different versions, and they are loaded by their corresponding version names. Here is an example: <code>detection_model = pw_detection.MegaDetectorV6(version=\"MDV6-yolov10-e\")</code>.</p>"},{"location":"model_zoo/other_detectors/","title":"Other detection models","text":"Models Version Names Licence Release Reference Deepfaune-detection - CC BY-SA 4.0 Released Deepfaune HerdNet-general general CC BY-NC-SA-4.0 Released Alexandre et. al. 2023 HerdNet-ennedi ennedi CC BY-NC-SA-4.0 Released Alexandre et. al. 2023 <p>[!TIP] Some models, such as MegaDetectorV6, HerdNet, and AI4G-Amazon, have different versions, and they are loaded by their corresponding version names. Here is an example: <code>detection_model = pw_detection.MegaDetectorV6(version=\"MDV6-yolov10-e\")</code>.</p>"},{"location":"releases/past_releases/","title":"Past Releases","text":""},{"location":"releases/past_releases/#pytorch-wildlife-version-120","title":"Pytorch-Wildlife Version 1.2.0","text":"<ul> <li>In this version of Pytorch-Wildlife, we are happy to release our detection fine-tuning module, with which users can fine-tune their own detection model from any released pre-trained MegaDetectorV6 models. Besides, this module also has functionalities that help users to prepare their datasets for the fine-tuning, just as our classification fine-tuning modules. For more details, please check the readme. Currently the fine-tuning is based on Ultralytics with AGPL. We will release MIT versions in the future. Here is the release page.</li> <li>We have also released additional MegaDetectorV6 models based on Yolo-v10 and RtDetr. We have skipped Yolo-v11 models because of limited performance and architectural gains. Most of the MIT and Apache versions have also finished training but are waiting for internal review before they can be released.</li> <li>We have also updated our AI4G-Amazon model with bigger datasets and it has a better performance compared to previous iterations. Please feel free to test it or fine-tune on it. </li> <li>We will also make a new roadmap for 2025 in the next couple of updates.</li> <li>Special thanks to Jos\u00e9 D\u00edaz for his great cross-platform app, BoquilaHUB, that is even working on ios and android! Please check his repo out! In the future, we will create a project gallery showcasing projects that use or are build upon Pytorch-Wildlife. If you want your projects to be included, please feel free to reach out to us on </li> </ul>"},{"location":"releases/release_notes/","title":"Main changes and additions","text":""},{"location":"releases/release_notes/#pytorch-wildlife-version-121","title":"Pytorch-Wildlife Version 1.2.1","text":""},{"location":"releases/release_notes/#speciesnet-is-available-in-pytorch-wildlife-for-testing","title":"SpeciesNet is available in Pytorch-Wildlife for testing!","text":"<ul> <li>We have added SpeciesNet into our model zoo, which is compatible with all detection models provided by Pytorch-Wildlife. Please refer to this document for more details!</li> </ul>"},{"location":"releases/release_notes/#deepfaune-in-our-model-zoo","title":"Deepfaune in Our Model Zoo!!","text":"<ul> <li>We are excited to announce the release of the Deepfaune models\u2014both the detector and classifier\u2014in PyTorch-Wildlife, adding to our growing model zoo. A huge thank you to the Deepfaune team for your support! Deepfaune is one of the most comprehensive models focused on the European ecosystem for both detection and classification. It serves as a great complement to MegaDetector, which has primarily been trained on datasets from North America, South America, and Africa. The Deepfaune detector is also our first third-party camera trap detection model integrated into PyTorch-Wildlife!</li> <li>To use the model, you just need to load them as any other Pytorch-Wildife models:  <pre><code>detection_model = pw_detection.DeepfauneDetector(device=DEVICE)\nclassification_model = pw_classification.DeepfauneClassifier(device=DEVICE)\n</code></pre></li> <li>You can also use the <code>detection_classification_pipeline_demo.py</code> script in the demo folder to test the whole detection + classification pipeline. </li> <li>Please also take a look at the original Deepfaune website and give them a star! </li> </ul>"},{"location":"releases/release_notes/#deepfaune-new-england-in-our-model-zoo-too","title":"Deepfaune-New-England in Our Model Zoo Too!!","text":"<ul> <li>Besides the original Deepfaune mode, there is another fine-tuned Deepfaune model developed by USGS for the Northeastern NA area called Deepfaune-New-England (DFNE). It can also be loaded with <code>classification_model = pw_classification.DFNE(device=DEVICE)</code></li> <li>Please take a look at the orignal DFNE repo and give them a star! </li> </ul>"}]}