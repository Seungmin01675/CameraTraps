{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4210d6e4",
   "metadata": {},
   "source": [
    "\n",
    "# 🐾 Wildlife Speed Tracking\n",
    "\n",
    "Welcome! This notebook helps you **detect, classify, and estimate 2D speeds** of animals in videos using **PyTorchWildlife**.  \n",
    "\n",
    "> **What you’ll get**: Annotated videos saved to an output folder, plus a `speed.csv` summarizing speeds per tracked object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe19d7",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Requirements\n",
    "\n",
    "- Python 3.9+ recommended\n",
    "- GPU optional (CUDA speeds things up, but CPU works too)\n",
    "- Videos placed in a folder (default: `./demo_data/speed_tracking_videos`)\n",
    "\n",
    "### 📦 Install dependencies\n",
    "\n",
    "> If you already have the packages, you can **skip** this cell. If needed, uncomment to install (recommended to run one-by-one if you hit errors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30510b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install --upgrade pip\n",
    "# %pip install supervision\n",
    "# %pip install ipywidgets tqdm pandas matplotlib\n",
    "# %pip install PytorchWildlife\n",
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a116ed",
   "metadata": {},
   "source": [
    "\n",
    "## 📥 Imports\n",
    "This cell imports everything we need and validates the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b1b6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import supervision as sv\n",
    "\n",
    "from PytorchWildlife.models import detection as pw_detection\n",
    "from PytorchWildlife.models import classification as pw_classification\n",
    "from PytorchWildlife import utils as pw_utils\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b066ee",
   "metadata": {},
   "source": [
    "\n",
    "## 🎛️ Configure Inputs\n",
    "\n",
    "- **Height (optional):** Use the toggle below to set `animal_height_m` in meters to convert speeds from **px/s → m/s** (conversion: `m/s = (px/s * animal_height_m) / image_width_px`).  \n",
    "  Leave it unset to keep speeds in **px/s**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed0a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d364957784493b901fd5ba9037e2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(description='Scale by:', options=(('None (px/s)', 'none'), ('Use height (m)', 'cu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "animal_height_m = None\n",
    "\n",
    "mode = widgets.ToggleButtons(\n",
    "    options=[(\"None (px/s)\", \"none\"), (\"Use height (m)\", \"custom\")],\n",
    "    value=\"none\",\n",
    "    description=\"Scale by:\",\n",
    ")\n",
    "\n",
    "height_input = widgets.BoundedFloatText(\n",
    "    value=1.00, min=0.01, max=100.0, step=0.01,\n",
    "    description=\"Height (m):\",\n",
    "    layout=widgets.Layout(width=\"220px\"),\n",
    ")\n",
    "\n",
    "status = widgets.HTML()\n",
    "\n",
    "def _update(*_):\n",
    "    global animal_height_m\n",
    "    if mode.value == \"custom\":\n",
    "        height_input.layout.display = \"flex\"\n",
    "        animal_height_m = float(height_input.value) if (height_input.value and height_input.value > 0) else None\n",
    "        status.value = (\n",
    "            f\"<span style='color:#0a0;'>Using height → {animal_height_m:.3f} m</span>\"\n",
    "            if animal_height_m else\n",
    "            \"<span style='color:#a00;'>Enter a positive height in meters.</span>\"\n",
    "        )\n",
    "    else:\n",
    "        height_input.layout.display = \"none\"\n",
    "        animal_height_m = None\n",
    "        status.value = \"<span style='color:#555;'>No height selected → speeds will be in px/s.</span>\"\n",
    "\n",
    "mode.observe(_update, names=\"value\")\n",
    "height_input.observe(_update, names=\"value\")\n",
    "\n",
    "_update()\n",
    "display(widgets.VBox([mode, height_input, status]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14285a8",
   "metadata": {},
   "source": [
    "- **Video content:** Use the toggle below to specify what your videos contain. This choice controls how tracks are selected:\n",
    "\n",
    "    - *One individual* → keep only the single longest track (best when a single animal is present).\n",
    "    - *Group of animals* → keep all tracks (best when multiple animals may appear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a2d846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b6ad1be8d5414185807398496c43fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(description='Video content:', options=(('One individual', True), ('Group of anima…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "assume_single_individual = True\n",
    "\n",
    "mode = widgets.ToggleButtons(\n",
    "    options=[(\"One individual\", True), (\"Group of animals\", False)],\n",
    "    value=True,\n",
    "    description=\"Video content:\",\n",
    ")\n",
    "\n",
    "status = widgets.HTML()\n",
    "\n",
    "def _update(change=None):\n",
    "    global assume_single_individual\n",
    "    assume_single_individual = mode.value\n",
    "    if assume_single_individual:\n",
    "        status.value = \"<span style='color:#0a0;'>Assuming ONE individual → will keep only the longest track.</span>\"\n",
    "    else:\n",
    "        status.value = \"<span style='color:#555;'>Assuming a GROUP → will keep ALL tracks.</span>\"\n",
    "\n",
    "mode.observe(_update, names=\"value\")\n",
    "_update()\n",
    "display(widgets.VBox([mode, status]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87768843",
   "metadata": {},
   "source": [
    "\n",
    "- **Folders**: where videos live and where outputs should go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfe637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE_FOLDER_PATH = ./demo_data/speed_tracking_videos\n",
      "OUTPUT_FOLDER      = ./speed_tracking_output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SOURCE_FOLDER_PATH = os.path.join(\".\", \"demo_data\", \"speed_tracking_videos\")\n",
    "OUTPUT_FOLDER = os.path.join(\".\", \"speed_tracking_output\")\n",
    "\n",
    "Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"SOURCE_FOLDER_PATH = {SOURCE_FOLDER_PATH}\")\n",
    "print(f\"OUTPUT_FOLDER      = {OUTPUT_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b07fa2",
   "metadata": {},
   "source": [
    "\n",
    "## 🖥️ Device\n",
    "Selects GPU (CUDA) if available, otherwise CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cad197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    try:\n",
    "        dev_name = torch.cuda.get_device_name(0)\n",
    "    except Exception:\n",
    "        dev_name = \"CUDA device\"\n",
    "    print(f\"Using GPU: {dev_name}\")\n",
    "else:\n",
    "    print(\"Using CPU (this may be slower).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc63596",
   "metadata": {},
   "source": [
    "\n",
    "## 🧠 Load Models\n",
    "- **Detector**: MegaDetector V6 (YOLOv9-c backbone)\n",
    "- **Classifier**: AI4G Amazon Rainforest (v2)\n",
    "\n",
    "> First run may download weights. If downloads fail, check internet/firewall settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e910260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.55 🚀 Python-3.10.16 torch-2.5.1+cu124 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "YOLOv9c summary (fused): 384 layers, 25,321,561 parameters, 0 gradients, 102.3 GFLOPs\n",
      "✅ Models loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can switch versions here if needed.\n",
    "DETECTION_VERSION = \"MDV6-yolov9-c\"\n",
    "CLASSIFICATION_VERSION = \"v2\"\n",
    "\n",
    "try:\n",
    "    detection_model = pw_detection.MegaDetectorV6(device=DEVICE, pretrained=True, version=DETECTION_VERSION)\n",
    "    classification_model = pw_classification.AI4GAmazonRainforest(device=DEVICE, version=CLASSIFICATION_VERSION)\n",
    "    print(\"✅ Models loaded\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to load models. Verify your PyTorchWildlife install and network access for weights.\"\n",
    "    ) from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde0c14",
   "metadata": {},
   "source": [
    "\n",
    "## 🖍️ Annotators\n",
    "Configure bounding boxes and labels for the output videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4964cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotators ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "box_annotator = sv.BoxAnnotator(thickness=4)\n",
    "lab_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK, text_thickness=4, text_scale=2)\n",
    "print(\"Annotators ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f5cf9",
   "metadata": {},
   "source": [
    "\n",
    "## 🔁 Detection & Classification Callback\n",
    "\n",
    "This function:\n",
    "1. Detects animals in the frame  \n",
    "2. Classifies each detection (cropped region)  \n",
    "3. Draws boxes & labels for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ce9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> Tuple[np.ndarray, sv.Detections, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frame: Current video frame (H,W,3)\n",
    "        index: Frame index or identifier (passed to detector for metadata)\n",
    "    Returns:\n",
    "        annotated_frame: Frame with boxes+labels\n",
    "        detections: Supervision Detections object\n",
    "        clf_labels: List of (prediction, confidence) per detection in the same order\n",
    "    \"\"\"\n",
    "    results_det: Dict = detection_model.single_image_detection(frame, img_path=index)\n",
    "\n",
    "    clf_labels: List[Tuple[str, float]] = []\n",
    "    for xyxy in results_det[\"detections\"].xyxy:\n",
    "        cropped_image = sv.crop_image(image=frame, xyxy=xyxy)\n",
    "        results_clf = classification_model.single_image_classification(cropped_image)\n",
    "        clf_labels.append((results_clf[\"prediction\"], results_clf[\"confidence\"]))\n",
    "\n",
    "    annotated_frame = lab_annotator.annotate(\n",
    "        scene=box_annotator.annotate(scene=frame, detections=results_det[\"detections\"]),\n",
    "        detections=results_det[\"detections\"],\n",
    "        labels=results_det[\"labels\"]\n",
    "    )\n",
    "\n",
    "    return annotated_frame, results_det[\"detections\"], clf_labels\n",
    "\n",
    "print(\"Callback ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58919afd",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 Prepare Speed Table\n",
    "We’ll create a DataFrame that stores **t1/x1/y1 → t2/x2/y2** and a computed speed column:\n",
    "- If a **species** is chosen, speeds convert to **m/s**\n",
    "- Otherwise, speeds remain in **px/s**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ce2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No animal height specified. Speed will be in pixels/second.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def init_speed_df(species_name: str):\n",
    "    if animal_height_m:\n",
    "        cols = [\"Video\", \"Image Width (px)\", \"t1 (s)\", \"x1 (px)\", \"y1 (px)\", \"t2 (s)\", \"x2 (px)\", \"y2 (px)\", \"speed (m/s)\"]\n",
    "        print(f\"Using height ~ {animal_height_m} m for conversion.\")\n",
    "        return pd.DataFrame(columns=cols), animal_height_m, True\n",
    "    else:\n",
    "        cols = [\"Video\", \"Image Width (px)\", \"t1 (s)\", \"x1 (px)\", \"y1 (px)\", \"t2 (s)\", \"x2 (px)\", \"y2 (px)\", \"speed (px/s)\"]\n",
    "        print(\"No animal height specified. Speed will be in pixels/second.\")\n",
    "        return pd.DataFrame(columns=cols), None, False\n",
    "\n",
    "df, animal_height_m, using_meters = init_speed_df(animal_height_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b140cc0",
   "metadata": {},
   "source": [
    "\n",
    "## ▶️ Run Tracking on Your Videos\n",
    "\n",
    "- Place videos in `SOURCE_FOLDER_PATH` (e.g., `.mp4, .avi, .mov`).\n",
    "- Annotated videos will be written into `OUTPUT_FOLDER`.\n",
    "- A running **speed table** is built and saved as `speed.csv` at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 04280240.MOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  20%|██        | 1/5 [00:42<02:51, 42.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 04230105.MOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  40%|████      | 2/5 [01:18<01:55, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 03090004.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  60%|██████    | 3/5 [01:33<00:55, 27.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 03210076.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  80%|████████  | 4/5 [01:39<00:19, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 05100016.MOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 5/5 [02:27<00:00, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.CRITICAL)\n",
    "\n",
    "if not os.path.exists(SOURCE_FOLDER_PATH):\n",
    "    raise FileNotFoundError(f\"Source video folder not found at {SOURCE_FOLDER_PATH}. Please create it and add videos.\")\n",
    "\n",
    "tracks = 0\n",
    "video_files = [f for f in os.listdir(SOURCE_FOLDER_PATH) if f.lower().endswith((\".mp4\", \".avi\", \".mov\"))]\n",
    "\n",
    "if not video_files:\n",
    "    print(\"No video files found. Add videos to the source folder and re-run this cell.\")\n",
    "else:\n",
    "    iterator = video_files\n",
    "    if tqdm is not None:\n",
    "        iterator = tqdm(video_files, desc=\"Processing videos\")\n",
    "\n",
    "    for video_name in iterator:\n",
    "        SOURCE_VIDEO_PATH = os.path.join(SOURCE_FOLDER_PATH, video_name)\n",
    "        TARGET_VIDEO_PATH = os.path.join(OUTPUT_FOLDER, f\"{os.path.splitext(video_name)[0]}_tracked.mp4\")\n",
    "        print(f\"\\nProcessing: {video_name}\")\n",
    "\n",
    "        try:\n",
    "            image_width_px, track_summaries = pw_utils.speed_in_video(\n",
    "                source_path=SOURCE_VIDEO_PATH,\n",
    "                target_path=TARGET_VIDEO_PATH,\n",
    "                callback=callback,\n",
    "                target_fps=10,\n",
    "                codec=\"mp4v\",\n",
    "                longest=assume_single_individual,\n",
    "                min_points=6,\n",
    "                min_duration_s=0.5,\n",
    "                min_displacement_px=20,\n",
    "                suppress_subtracks=True,\n",
    "                subtrack_radius_px=50,\n",
    "            )\n",
    "\n",
    "            # Each 'track' has two points (t1,x1,y1) and (t2,x2,y2) and a speed in px/s\n",
    "            for i, key in enumerate(track_summaries):\n",
    "                t1, x1, y1 = track_summaries[key]['points'][0]\n",
    "                t2, x2, y2 = track_summaries[key]['points'][1]\n",
    "                speed_px_s = track_summaries[key]['speed']\n",
    "\n",
    "                if using_meters and animal_height_m:\n",
    "                    # Convert px/s to m/s using width-scale (height_m / image_width_px)\n",
    "                    speed_val = (speed_px_s * animal_height_m) / image_width_px\n",
    "                else:\n",
    "                    speed_val = speed_px_s\n",
    "\n",
    "                df.loc[tracks] = [video_name, image_width_px, t1, x1, y1, t2, x2, y2, speed_val]\n",
    "                tracks += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {video_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f2ed6",
   "metadata": {},
   "source": [
    "\n",
    "## 💾 Save Results\n",
    "This writes a `speed.csv` into your output folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = os.path.join(OUTPUT_FOLDER, \"speed.csv\")\n",
    "if len(df) > 0:\n",
    "    df.to_csv(csv_path, index=False, float_format=\"%.3f\")\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"Speed table is empty—nothing to save yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-wildlife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
